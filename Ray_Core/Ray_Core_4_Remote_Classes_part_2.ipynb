{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Core: Remote Classes as Actors, part 2\n",
    "\n",
    "© 2022, Anyscale Inc. All Rights Reserved.\n",
    "\n",
    "## Overview\n",
    "Ray has a myriad of design patterns for [tasks](https://docs.ray.io/en/latest/ray-core/tasks/patterns/index.html#task-patterns) and [actors](https://docs.ray.io/en/latest/ray-core/actors/patterns/index.html). These patterns allows you to write distributed applications. \n",
    "\n",
    "### Learning objectives\n",
    "In this this tutorial, we revisit Ray Actors and learn more about:\n",
    " * Common Ray Actors patterns used in Ray native libraries for writing distributed Actors\n",
    "   * Tree of Actors \n",
    "   * Same Data Different Functions (SDDF)\n",
    " * How to use Actors for Batch Inference\n",
    " * How to pass Ray Actors to remote tasks for distributed computing\n",
    "\n",
    "Let's implement a simple example to illustrate this pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree of Actors Pattern\n",
    "A common pattern used in Ray libraries [Ray Tune](https://docs.ray.io/en/latest/tune/index.html), [Ray Train](https://docs.ray.io/en/latest/train/train.html), and [RLlib](https://docs.ray.io/en/latest/rllib/index.html) to train models in a parallel or conduct distributed HPO.\n",
    "\n",
    "In this common pattern, tree of actors, a collection of workers as actors, are managed by a supervisor actor. For example, you want to train multiple models, each of a different type, at the same time, while being able to inspect its state during its training.\n",
    "\n",
    "<img src=\"../_static/assets/Ray_Core/Ray_Core_4_Remote_Classes_part_2/tree_of_actors.svg\" width=\"25%\" height=\"25%\">\n",
    "\n",
    "This pattern is no different from Same Data Different Function/Model (SDDF). Popular in AutoML scenarios, where you may want to train different models at the same time using the same dataset.\n",
    "\n",
    "<img src=\"../_static/assets/Ray_Core/Ray_Core_4_Remote_Classes_part_2/same_data_different_model_architecture.png\" width=\"35%\" height=\"25%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's start Ray…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "import ray\n",
    "import random\n",
    "from random import randint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import ray\n",
    "from ray.util.actor_pool import ActorPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ray.is_initialized:\n",
    "    ray.shutdown()\n",
    "ray.init(logging_level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Supervisor and worker actor pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generic model factory utility  \n",
    "\n",
    "This factory generates a few specify type of models (they are fake 😏): regression, classification, or neural network, and will have its respective training function. Each model will be in a particular state  during training. The final state is `DONE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factory function to return an instance of a model type\n",
    "def model_factory(m: str, func: object):\n",
    "    return Model(m, func)\n",
    "\n",
    "\n",
    "# states to inspect or checkpoint\n",
    "STATES = [\"RUNNING\", \"PENDING\", \"DONE\"]\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, m: str, func: object):\n",
    "        self._model = m\n",
    "        self._func = func\n",
    "\n",
    "    def train(self):\n",
    "        # do some training work here for the respective model type\n",
    "        self._func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Worker Actor\n",
    "This worker actor will train each model. When the model's state reaches `DONE`, we stop training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class Worker(object):\n",
    "    def __init__(self, m: str, func: object):\n",
    "        # type of a model: lr, cl, or nn\n",
    "        self._model = m\n",
    "        self._func = func\n",
    "\n",
    "    # inspect its current state and return it. For now\n",
    "    # it could be in one of the states\n",
    "    def state(self) -> str:\n",
    "        return random.choice(STATES)\n",
    "\n",
    "    # Create the model from the factory for this worker and\n",
    "    # do the training by invoking its respective objective function\n",
    "    # for that model\n",
    "    def work(self) -> None:\n",
    "        model_factory(self._model, self._func).train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Supervisor Actor \n",
    "The supervisor creates three actors, each with its own respective training model type and its training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define respective model training functions\n",
    "\n",
    "\n",
    "def lf_func():\n",
    "    # do some training work for linear regression\n",
    "    time.sleep(1)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def cl_func():\n",
    "    # do some training work for classification\n",
    "    time.sleep(1)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def nn_func():\n",
    "    # do some training work for neural networks\n",
    "    time.sleep(1)\n",
    "    return 0\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "class Supervisor:\n",
    "    def __init__(self):\n",
    "        # Create three Actor Workers, each by its unique model type and\n",
    "        # their respective training function\n",
    "        self.workers = [\n",
    "            Worker.remote(name, func)\n",
    "            for (name, func) in [(\"lr\", lf_func), (\"cl\", cl_func), (\"nn\", nn_func)]\n",
    "        ]\n",
    "\n",
    "    def work(self):\n",
    "        # do the work\n",
    "        [worker.work.remote() for worker in self.workers]\n",
    "\n",
    "    def terminate(self):\n",
    "        [ray.kill(worker) for worker in self.workers]\n",
    "\n",
    "    def state(self):\n",
    "        return ray.get([worker.state.remote() for worker in self.workers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create a Actor instance for supervisor and launch its workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sup = Supervisor.remote()\n",
    "\n",
    "# Launch remote actors as workers\n",
    "sup.work.remote()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at the Ray Dashboard\n",
    "\n",
    "You should see Actors running as process on the workders nodes\n",
    " * Supervisor\n",
    " * Workers\n",
    " \n",
    "Also, click on the `Actors` to view more metrics and data on individual Ray Actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check their status\n",
    "while True:\n",
    "    # Fetch the states of all its workers\n",
    "    states = ray.get(sup.state.remote())\n",
    "    print(states)\n",
    "    # check if all are DONE\n",
    "    result = all(\"DONE\" == e for e in states)\n",
    "    if result:\n",
    "        # Note: Actor processes will be terminated automatically when the initial actor handle goes out of scope in Python.\n",
    "        # If we create an actor with actor_handle = ActorClass.remote(), then when actor_handle goes out of scope and is destroyed,\n",
    "        # the actor process will be terminated. Note that this only applies to the original actor handle created for the actor\n",
    "        # and not to subsequent actor handles created by passing the actor handle to other tasks.\n",
    "\n",
    "        # kill supervisors' all workers manually, only for illustrtation and demo\n",
    "        sup.terminate.remote()\n",
    "\n",
    "        # kill the supervisor manually, only for illustration and demo\n",
    "        ray.kill(sup)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Actor-based batch inference\n",
    "In our first tutorial on [Ray Tasks](./ex_01_remote_funcs.ipynb) we covered a use case to parallelize batch inference. In short, we\n",
    "used the Different Data Same Function(DDSF) pattern. Here we will the same pattern but with Ray Actors, which are state-based and \n",
    "preferred method to do batch inference. Primarily, the elements remain the same except for few modificaitons.\n",
    "\n",
    "**Input dataset**: This is a large collection of observations to generate predictions for. The data is usually stored in an external storage system like S3, HDFS or database, across\n",
    "many, files.\n",
    "\n",
    "**ML model**: This is a trained ML model that is usually also stored in an external storage system or in a model store.\n",
    "\n",
    "**Predictions**: These are the outputs when applying the ML model on observations. Normally, predictions are usually written back to the storage system. Unlike tasks\n",
    "doing the predictions, we employ a pool of Actors.\n",
    "\n",
    "For purpose of this tutorial, we make the following provisions:\n",
    " * create a dummy model that returns some fake prediction\n",
    " * use real-world NYC taxi data to provide large data set for batch inference\n",
    " * create a pool of actors and submit each shard to the pool.\n",
    " * return the predictions instead of writing it back to the disk\n",
    "\n",
    "As an example of scaling pattern called Different Data Same Function (DDSF), also known as Distributed Data Parallel (DDP) paradigm, our function in this digaram is the \n",
    "pretrained **model** and the data is split and disributed as **shards**.\n",
    "\n",
    "<img src=\"../_static/assets/Ray_Core/Ray_Core_1_Remote_Functions/batch_inference_architecture.png\" width=\"25%\" height=\"25%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ACTORS = 5\n",
    "NUM_SHARD_FILES = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our load model closure remains the same\n",
    "def load_trained_model():\n",
    "    # A fake model that predicts whether tips were given based on number of passengers in the taxi cab.\n",
    "    def model(batch: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Some model payload so Ray copies the model in the shared plasma store to tasks scheduled across nodes.\n",
    "        model.payload = np.arange(100, 100_000_000, dtype=float)\n",
    "        model.cls = \"regression\"\n",
    "\n",
    "        # give a tip if 2 or more passengers\n",
    "        predict = batch[\"passenger_count\"] >= 2\n",
    "        return pd.DataFrame({\"score\": predict})\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Ray Actor that stores a model reference and does the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class NYCBatchPredictor:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def predict(self, shard_path):\n",
    "        # read each shard and convert to pandas\n",
    "        df = pq.read_table(shard_path).to_pandas()\n",
    "\n",
    "        # do the inference with our model and return the result\n",
    "        result = self.model(df)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get our trained model instance\n",
    "2. Store it into the plasma object store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_trained_model()\n",
    "model_ref = ray.put(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch our NYC taxi shard files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate thorough our NYC files\n",
    "input_shard_files = [\n",
    "    f\"s3://anonymous@air-example-data/ursa-labs-taxi-data/downsampled_2009_full_year_data.parquet\"\n",
    "    f\"/fe41422b01c04169af2a65a83b753e0f_{i:06d}.parquet\"\n",
    "    for i in range(NUM_SHARD_FILES)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create five Actor instances, each initialized with the same model reference\n",
    "2. Create a pool of five actors\n",
    "\n",
    "We use the Ray actor pool utility [ActorPool](https://docs.ray.io/en/latest/ray-core/actors/actor-utils.html?highlight=ActorPool#actor-pool).\n",
    "\n",
    "[Actool Pool API](https://docs.ray.io/en/latest/ray-core/package-ref.html?highlight=ActorPool#ray-util-actorpool) reference package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors = [NYCBatchPredictor.remote(model_ref) for _ in range(NUM_ACTORS)]\n",
    "actors_pool = ActorPool(actors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit each shard to the pool of actors for batch reference\n",
    "# The API syntax is not dissimilar to Python or Ray Multiprocessor pool APIs\n",
    "\n",
    "for shard_path in input_shard_files:\n",
    "    # Submit file shard for prediction to the pool\n",
    "    actors_pool.submit(lambda actor, shard: actor.predict.remote(shard), shard_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over finised actor's predictions\n",
    "while actors_pool.has_next():\n",
    "    r = actors_pool.get_next()\n",
    "    print(\n",
    "        f\"Predictions dataframe size: {len(r)} | Total score for tips: {r['score'].sum()}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recap\n",
    "\n",
    "What we have demonstrated above is an Actor tree design pattern, commonly used in Ray for writing distributed applications. In particular, Ray's native libraries such as Train, Tune, Serve, and RLib and [Ray AIR's](https://docs.ray.io/en/latest/ray-air/getting-started.html) components use it for distributed training and tuning trials. \n",
    "\n",
    "Additionally, we implemented a DDSF scaling design pattern using an Actor-based predictor model function, using an `ActorPool` utility class instead of task.  \n",
    "Task-based batch inferene has an overhead cost that can be significant if the model size is large, since it has to fetch the model from the driver's plasma store. We can optimize it by using Ray actors, \n",
    "which will fetch the model just once and reuse it for all predictions assigned to the same actor in the pool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Any questions?\n",
    "\n",
    "Let's look at another example in a similar tree of actors pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Passing Actor handles to Ray Tasks\n",
    "\n",
    "Actors are versatile: they can instantiated and passed to remote Ray tasks or even other actors. \n",
    "\n",
    "Let's consider writing a distributed messaging service, where workers may post messages to update the state of the messaging service. This could be a logging or monitoring service. For example, [WhyLabs](https://www.anyscale.com/blog/running-and-monitoring-distributed-ml-with-ray-and-whylogs) implemented a variation of this usage pattern to monitor Ray Serve deployments. Since tasks and actors are accessiible as Python\n",
    "objects, they can be passed around to other Python classes or functions.\n",
    "\n",
    "You can pass actor handle instances to remote Ray tasks, which can change the actor's \n",
    "state. The `MessageActor` keeps or clears messages, depending on the method\n",
    "invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class MessageActor(object):\n",
    "    def __init__(self):\n",
    "        # Keep the state of all the messages received\n",
    "        self.messages = []\n",
    "\n",
    "    def add_message(self, message):\n",
    "        self.messages.append(message)\n",
    "\n",
    "    # reset and clear all messages\n",
    "    def get_and_clear_messages(self):\n",
    "        messages = self.messages\n",
    "        self.messages = []\n",
    "        return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a worker doing some specific work, such as updating a DB or posting a tweet or\n",
    "checking a status of a process and then sends a message to the actor.\n",
    "\n",
    "**NOTE**: _Question: What does this remind of you from the previous lessons?_ \n",
    "\n",
    "(PS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def worker(message_actor, j):\n",
    "    for i in range(10):\n",
    "        time.sleep(1)\n",
    "        message_actor.add_message.remote(f\"Message {i} from worker {j}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_actor = MessageActor.remote()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start three worker tasks that update the `MessageActor` service since each Ray task gets the handle to the `MessageActor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[worker.remote(message_actor, j) for j in range(3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the workers are already launched, let's get actor's state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    new_messages = ray.get(message_actor.get_and_clear_messages.remote())\n",
    "    print(\"New messages\\n:\", new_messages)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. Add a remote class, such as a logging actor, that keeps states by logging info (may be only in memory) about N experiments, where (N <=3).\n",
    "2. Implement actor methods that alters the state. That is, it tracks results of 9 separate runs per each experiment.\n",
    "3. Write a separate Ray task that executes 9 runs per each experiment.\n",
    "4. Instantiate the actor and call its methods from within the remote Ray task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution hints\n",
    "\n",
    "This solution is just a structural hint. There are few missing bits:\n",
    " * instantiation of `LoggingActor`\n",
    " * Need to use `ray.get()` to fetch the values from the object store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "class LoggingActor(object):\n",
    "    def __init__(self):\n",
    "        self.logs = defaultdict(list)\n",
    "\n",
    "    def log(self, index, message):\n",
    "        self.logs[index].append(message)\n",
    "\n",
    "    def get_logs(self):\n",
    "        return dict(self.logs)\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def run_experiment(experiment_index, logging_actor):\n",
    "    for i in range(9):\n",
    "        time.sleep(1)\n",
    "        # Push a logging message to the actor.\n",
    "        logging_actor.log.remote(experiment_index, \"On iteration {}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging_actor = # TODO Instantiate Actor here\n",
    "\n",
    "# experiment_ids = []\n",
    "# for i in range(3):\n",
    "# TODO\n",
    "# invoke task and append results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logs = logging_actor.get_logs.remote()\n",
    "# TODO use ray.get() to fetch the logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "1. Read references below.\n",
    "2. Can you implement calculating `pi` as a combination of actor (which keeps the state of the progress of calculating `pi` as it approaches its final value) and a task (which  computes candidates for `pi`)? \n",
    "\n",
    "**solution hint**: Check the Ray core quickstart docs only if you need to... :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "* [Writing your First Distributed Python Application with Ray](https://www.anyscale.com/blog/writing-your-first-distributed-python-application-with-ray)\n",
    "* [Using and Programming with Actors](https://docs.ray.io/en/latest/actors.html)\n",
    "* [Ray Asynchronous and Threaded Actors: A way to achieve concurrency](https://medium.com/@2twitme/ray-asynchronous-and-threaded-actors-a-way-to-achieve-concurrency-ad9f86145f72)\n",
    "* [Model Batch Inference in Ray: Actors, ActorPool, and Datasets](https://www.anyscale.com/blog/model-batch-inference-in-ray-actors-actorpool-and-datasets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
