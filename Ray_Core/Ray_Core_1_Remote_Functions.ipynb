{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a14cab54",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Ray Core: Remote Functions as Tasks\n",
    "\n",
    "© 2022, Anyscale Inc. All Rights Reserved.\n",
    "\n",
    "## Introduction\n",
    "Ray enables arbitrary functions to be executed asynchronously on separate Python workers. These asynchronous Ray functions are called “tasks.” You can specify task's resource requirements in terms of CPUs, GPUs, and custom resources. These resource requests are used by the cluster scheduler to distribute tasks across the cluster for parallelized execution.  \n",
    "\n",
    "<img src=\"../_static/assets/Ray_Core/Ray_Core_1_Remote_Functions/python_to_ray_concept_map.png\" height=\"55%\" width=\"50%\">\n",
    "\n",
    "### Learning objectives\n",
    "In this this tutorial, we learn about:\n",
    " * Remote Task Parallel Pattern\n",
    " * Stateless remote functions as distributed tasks\n",
    " * Serial vs Parallel execution \n",
    " * Understand the concept of a Ray task \n",
    " * Easy API to convert an existing Python function into a Ray remote task\n",
    " * Walk through a map-reduce and distribute batch inference use cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee29917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from numpy import loadtxt\n",
    "import ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b98b39",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tasks Parallel Pattern\n",
    "\n",
    "Ray converts decorated functions with `@ray.remote` into stateless tasks, scheduled anywhere on a Ray node's worker in the cluster. \n",
    "\n",
    "Where they will be executed (and by whom), you don't have to worry about its details. All that is taken care for you. Nor do \n",
    "you have to reason about it — all that burden is Ray's job. You simply take your existing Python functions and covert them into \n",
    "distributed stateless *Ray Tasks*: **as simple as that!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893ec22c",
   "metadata": {},
   "source": [
    "### Example 1: Serial vs Parallelism\n",
    "\n",
    "Let's look at simple tasks running serially and then in parallel. For illustration, we'll use a simple task, but this could be a compute-intensive task as part of your workload.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a44dfe",
   "metadata": {},
   "source": [
    "There are a few key differences between the original function and the decorated one:\n",
    "\n",
    "**Invocation**: The regular version is called with `regular_function()`, whereas the remote version is called with `remote_function.remote()`. Keep this pattern in mind for all Ray remote execution methods.\n",
    "\n",
    "**Mode of execution and return values**: `regular_function` executes synchronously and returns the result of the function as the value `1` (in our case), whereas `remote_function` immediately returns an `ObjectID` (a future) and then executes the task in the background on a remote worker process. The result of the future is obtained by calling `ray.get` on the `ObjectID`. This is a blocking function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1982539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A regular Python function.\n",
    "def regular_function():\n",
    "    time.sleep(1)\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94597c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Ray remote function.\n",
    "@ray.remote\n",
    "def remote_function():\n",
    "    time.sleep(1)\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b04aec4",
   "metadata": {},
   "source": [
    "Let's launch a Ray cluster on our local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aad47d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ray.is_initialized:\n",
    "    ray.shutdown()\n",
    "ray.init(logging_level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6cea5d-09fb-4fb8-a7bc-8839804c375d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's invoke the regular function\n",
    "assert regular_function() == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0895cda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's invoke the remote function.\n",
    "obj_ref = remote_function.remote()\n",
    "obj_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111ac990",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ray.get(obj_ref) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c8f5fe",
   "metadata": {},
   "source": [
    "#### Serial execution in Python with no parallelism\n",
    "Invocations of `regular_function` in a comprehension loop happens `serially`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d090c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are executed one at a time, back-to-back, in a list comprehension\n",
    "results = [regular_function() for _ in range(10)]\n",
    "assert sum(results) == 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5601c",
   "metadata": {},
   "source": [
    "#### Parallel execution in Python with Ray\n",
    "\n",
    "Invocations of `remote_function` in a loop happen `asynchronously` and in parallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06dc2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executing these functions, in comprehension list, happens at the same time in the background,\n",
    "# and we get the results using ray.get.\n",
    "\n",
    "results = [remote_function.remote() for _ in range(10)]\n",
    "assert sum(ray.get(results)) == 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf7bb91",
   "metadata": {},
   "source": [
    "### Example 2: Adding two np arrays\n",
    "\n",
    "<img src=\"../_static/assets/Ray_Core/Ray_Core_1_Remote_Functions/remote_task_api_add_array.png\" width=\"50%\" height=\"25%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e573d5",
   "metadata": {},
   "source": [
    "Define a function as a Ray task to read an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b86a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def read_array(fn: str) -> np.array:\n",
    "    arr = loadtxt(fn, comments=\"#\", delimiter=\",\", unpack=False)\n",
    "    return arr.astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a7cebe",
   "metadata": {},
   "source": [
    "Define a function as a Ray task to add two np arrays return the sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeb063d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def add_array(arr1: np.array, arr2: np.array) -> np.array:\n",
    "    return np.add(arr1, arr1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553e7cd7",
   "metadata": {},
   "source": [
    "Define a function as a Ray task to sum the contents of an np array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b01bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def sum_array(arr1: np.array) -> int:\n",
    "    return np.sum(arr1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779d86e8",
   "metadata": {},
   "source": [
    "Now let's execute our tasks. For now we will run Ray locally on our laptop or on a single node, with potential access to utilize all the available cores when necessary.\n",
    "\n",
    "Ray executes immediately and returns an object reference `ObjectRef` as a future. This enables Ray to parallelize tasks and execute them asynchronously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1853f1f",
   "metadata": {},
   "source": [
    "#### Read both arrays\n",
    "\n",
    "Use the `func_name.remote(args)` extention to invoke a remote Ray Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e32a37-723a-4c6e-9b2a-1a51377c6db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.getcwd().endswith(\"Ray_Core\"):\n",
    "    path_root = os.getcwd()\n",
    "else:\n",
    "    path_root = os.path.join(os.getcwd(), \"Ray_Core\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e63c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_ref_arr1 = read_array.remote(os.path.join(path_root, \"data/file_1.txt\"))\n",
    "print(f\"array 1: {obj_ref_arr1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be601ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_ref_arr2 = read_array.remote(os.path.join(path_root, \"data/file_2.txt\"))\n",
    "print(f\"array 2: {obj_ref_arr2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c37ac17",
   "metadata": {},
   "source": [
    "#### Add both arrays\n",
    "\n",
    "Let's add our two arrays by calling the remote method. *Note*: We are sending Ray `ObjectRef` references as arguments. Those arguments will be resolved inline and fetched from owner's object store. That is, the cluster node that creates the `ObjectRef` owns the meta data associated and stores it in its object store. \n",
    "\n",
    "Ray scheduler is aware of where these object references reside or who owns them, so it will schedule this remote task on node on the worker process for data locality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ec1898",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_obj_ref = add_array.remote(obj_ref_arr1, obj_ref_arr2)\n",
    "result_obj_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70b38ce",
   "metadata": {},
   "source": [
    "#### Fetch the result\n",
    "\n",
    "This will task if not finished will block during `.get(object_ref`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f5e031",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ray.get(result_obj_ref)\n",
    "print(f\"Result: add arr1 + arr2: \\n {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4051a7",
   "metadata": {},
   "source": [
    "Add the array elements within an `np.array` and get the sum. \n",
    "**Note** that we are sending `ObjRefs` as arguments to the function. Ray will resolve or fetch the value of these arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f27dc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_1 = ray.get(sum_array.remote(obj_ref_arr1))\n",
    "sum_2 = ray.get(sum_array.remote(obj_ref_arr2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d5e2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Sum of arr1: {sum_1}\")\n",
    "print(f\"Sum of arr2: {sum_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fdb316-83d9-4599-b335-f6dbfb9485ef",
   "metadata": {},
   "source": [
    "#### Any questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27eeba1a",
   "metadata": {},
   "source": [
    "### Example 3: Generating Fibonnaci series\n",
    "\n",
    "Let's define two functions: one runs locally or serially, the other runs on a Ray cluster (local or remote). This example is borrowed and refactored from our \n",
    "blog: [Writing your First Distributed Python Application with Ray](https://www.anyscale.com/blog/writing-your-first-distributed-python-application-with-ray). \n",
    "(This is an excellent tutorial to get started with the concept of why and when to use Ray tasks and Ray Actors. Highly recommended read!)\n",
    "\n",
    "Another similar blog of interest is how to compute the value of **pi**: [How to scale Python multiprocessing to a cluster with one line of code](https://medium.com/distributed-computing-with-ray/how-to-scale-python-multiprocessing-to-a-cluster-with-one-line-of-code-d19f242f60ff)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49994253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for local execution\n",
    "def generate_fibonacci(sequence_size):\n",
    "    fibonacci = []\n",
    "    for i in range(0, sequence_size):\n",
    "        if i < 2:\n",
    "            fibonacci.append(i)\n",
    "            continue\n",
    "        fibonacci.append(fibonacci[i - 1] + fibonacci[i - 2])\n",
    "    return len(fibonacci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92777c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for remote Ray task with just a wrapper\n",
    "@ray.remote\n",
    "def generate_fibonacci_distributed(sequence_size):\n",
    "    return generate_fibonacci(sequence_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8b6ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of cores\n",
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675f4fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal Python in a single process\n",
    "def run_local(sequence_size):\n",
    "    results = [generate_fibonacci(sequence_size) for _ in range(os.cpu_count())]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199e8e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "run_local(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc510e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributed on a Ray cluster\n",
    "def run_remote(sequence_size):\n",
    "    results = ray.get(\n",
    "        [\n",
    "            generate_fibonacci_distributed.remote(sequence_size)\n",
    "            for _ in range(os.cpu_count())\n",
    "        ]\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ebb951",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "run_remote(100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9d115e-9ee3-43a8-bf4b-3b42e762c1e5",
   "metadata": {},
   "source": [
    "#### Any questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea4b7d2-2ceb-4a87-9a7b-3945f15f7d0a",
   "metadata": {},
   "source": [
    "### Example 4: Use case of `tasks` for map-reduce\n",
    "\n",
    "The `map-reduce` pattern is a good use case for writing distributed applications with Ray core APIs. For _map_, this example uses Ray tasks to execute a \n",
    "given function multiple times in parallel (on a separate process on a node).  \n",
    "\n",
    "We then use `ray.get`, as part of the `reduce` process, to fetch the results of each of these functions.\n",
    "\n",
    "<img src=\"../_static/assets/Ray_Core/Ray_Core_1_Remote_Functions/map_reduce_architecture.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691c697f-7827-446e-9772-c41052ac6485",
   "metadata": {},
   "source": [
    "#### Single-threaded map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4332c96-92b3-4dab-8e19-6f42275bffc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = list(range(100))\n",
    "map_func = lambda i: i * 2\n",
    "output = [map_func(i) for i in items]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e307c179-6309-4b1b-855b-2a5569b0ab08",
   "metadata": {},
   "source": [
    "#### Ray parallel map\n",
    "Use the `@ray.remote` decorator to convert this `map`function into a Ray task. It takes an object and func argument and invokes the function to process the object.\n",
    "Simple and elegant!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297fb8e5-5977-4253-9fca-0350a18e539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def map(obj, f):\n",
    "    return f(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff668e0-ad2d-4331-815b-a120caa4a201",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = list(range(100))\n",
    "map_func = lambda i: i * 2\n",
    "\n",
    "# map.remote() will return an objRef to the computed value. We fetch\n",
    "# that value using ray.get\n",
    "output = ray.get([map.remote(i, map_func) for i in items])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b957d3-32df-414e-baa9-ff4a318b25a6",
   "metadata": {},
   "source": [
    "#### Single-threaded reduce\n",
    "For reduce, let's imagine that we want to sum up the numbers computed from\n",
    "our map function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5b2fed-add8-4221-b9dd-5872cb3fc2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = list(range(100))\n",
    "map_func = lambda i: i * 2\n",
    "output = sum([map_func(i) for i in items])\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f41d52-a295-4626-984c-7a3c7d21a461",
   "metadata": {},
   "source": [
    "#### Ray parallel map and reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36d2ebc-0757-4e42-94d1-79936a187ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def map(obj, f):\n",
    "    return f(obj)\n",
    "\n",
    "\n",
    "# Our reduce operation is expecting multipe arguments.\n",
    "# It sums up all arguments using np.sum(elements)\n",
    "@ray.remote\n",
    "def sum_results(*elements):\n",
    "    return np.sum(elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5042ad71-675a-476b-bc47-2af794a5df2c",
   "metadata": {},
   "source": [
    "Let's do our Ray parallel map. Note that comprehension list is a collection of `ObjRefs`, each element returned by `map.remote(i, map_func)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc1581b-2657-4907-b2e8-c2336ea59ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = list(range(100))\n",
    "map_func = lambda i: i * 2\n",
    "remote_elements = [map.remote(i, map_func) for i in items]\n",
    "remote_elements[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66231521-daa8-464f-b565-57a9e59e7af6",
   "metadata": {},
   "source": [
    "##### Simple reduce\n",
    "The `sum_results.remote()` as a reduce step returns the `ObjectRef` to results\n",
    "of all the values in the elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263e3a55-da3a-4102-9d59-d66ebf522e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple reduce\n",
    "remote_final_sum = sum_results.remote(*remote_elements)\n",
    "# fetch the reduce sumed result\n",
    "result = ray.get(remote_final_sum)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b56f2c2-8625-475d-905c-a0609ef3df6f",
   "metadata": {},
   "source": [
    "##### Tree reduce\n",
    "Simply break into intermediate results, followed by the final reduce. \n",
    "In short break into five groups of 20 object references, and then final\n",
    "reduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3cf3df-8c37-4d93-92f1-7f7350cc94a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree reduce using comprehension list.\n",
    "# Split in five groups of 20 ObjecRefs for intermediate reduce,\n",
    "# followed by final reduce, for all 100 elements\n",
    "intermediate_results = [\n",
    "    sum_results.remote(*remote_elements[i * 20 : (i + 1) * 20]) for i in range(5)\n",
    "]\n",
    "\n",
    "# Get the reduce results of these groups\n",
    "remote_final_sum = sum_results.remote(*intermediate_results)\n",
    "result = ray.get(remote_final_sum)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b6bfc7-21df-4865-9d69-37d9b5e5b2a6",
   "metadata": {},
   "source": [
    "### Example 5:  How to use Tasks for distributed batch inference \n",
    "\n",
    "Batch inference is a common distributed application workload in machine learning. It's a process of using a trained model to generate predictions for a collection of observations. \n",
    "Primarily, it has the following elements:\n",
    "\n",
    "**Input dataset**: This is a large collection of observations to generate predictions for. The data is usually stored in an external storage system like S3, HDFS or database, across\n",
    "many files.\n",
    "**ML model**: This is a trained ML model that is usually also stored in an external storage system or in a model store.\n",
    "**Predictions**: These are the outputs when applying the ML model on observations. Normally, predictions are usually written back to the storage system.\n",
    "\n",
    "For purpose of this tutorial, we make the following provisions:\n",
    " * create a dummy model that returns some fake prediction\n",
    " * use real-world NYC taxi data to provide large data set for batch inference\n",
    " * return the predictions instead of writing it back to the disk\n",
    "\n",
    "As an example of scaling pattern called Different Data Same Function (DDSF), also known as Distributed Data Parallel (DDP) paradigm, our function in this digaram is the \n",
    "pretrained **model** and the data is split and disributed as **shards**.\n",
    "\n",
    "<img src=\"../_static/assets/Ray_Core/Ray_Core_1_Remote_Functions/batch_inference_architecture.png\" width=\"25%\" height=\"25%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe2fc8d-4a9b-495e-9c74-2ac15f0ddf92",
   "metadata": {},
   "source": [
    "Define a Python closure to load our pretrained model. This model is just a fake model that predicts whether a \n",
    "tip is warranted continent on the number of fares (2 or more) on collective rides.\n",
    "\n",
    "**Note**: This prediction is fake. The real model will invoke model's `model.predict(input_data)`. Yet\n",
    "it suffices for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cc1f1d-13b1-48ca-bf6c-7e55584bb0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model():\n",
    "    # A fake model that predicts whether tips were given based on number of passengers in the taxi cab.\n",
    "    def model(batch: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Some model payload so Ray copies the model in the shared plasma store to tasks scheduled across nodes.\n",
    "        model.payload = np.arange(100, 100_000_000, dtype=float)\n",
    "        model.cls = \"regression\"\n",
    "\n",
    "        # give a tip if 2 or more passengers\n",
    "        predict = batch[\"passenger_count\"] >= 2\n",
    "        return pd.DataFrame({\"score\": predict})\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c347d93-6ed8-4d01-acb9-39309c8d6b79",
   "metadata": {},
   "source": [
    "Let's define a Ray task that will handle each shard of the NYC taxt data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794a075a-01e0-4789-9734-6a29e9df79aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def make_model_batch_predictions(model, shard_path):\n",
    "    print(f\"Batch inference for shard file: {shard_path}\")\n",
    "    df = pq.read_table(shard_path).to_pandas()\n",
    "    result = model(df)\n",
    "\n",
    "    # Return our prediction data frame\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba78dfe6-f17f-4c99-93a2-1b87bd7b474a",
   "metadata": {},
   "source": [
    "Get the 8 files consisting of NYC data per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044acf37-a2bc-41dc-9347-4631aacfef4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12 files, one for each remote task.\n",
    "input_files = [\n",
    "    f\"s3://anonymous@air-example-data/ursa-labs-taxi-data/downsampled_2009_full_year_data.parquet\"\n",
    "    f\"/fe41422b01c04169af2a65a83b753e0f_{i:06d}.parquet\"\n",
    "    for i in range(8)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473ae826-61c5-4212-9374-98b53a0ef78b",
   "metadata": {},
   "source": [
    "`ray.put()` the model just once to local object store, and then pass the reference to the remote tasks.\n",
    "This is Ray core API for putting objects into the Ray Plasma store. We discuss these APIs and Plasma store\n",
    "in the next tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb61e5c-d190-4b10-bf4f-251a1c138235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model\n",
    "model = load_trained_model()\n",
    "\n",
    "# Put the model object into the shared object store.\n",
    "model_ref = ray.put(model)\n",
    "model_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4946c974-64ee-4e85-982e-8f5dff3bad7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List for holding all object references from the model's predictions\n",
    "result_refs = []\n",
    "\n",
    "# Launch all prediction tasks. For each file create a Ray remote task\n",
    "# to do a batch inference\n",
    "for file in input_files:\n",
    "\n",
    "    # Launch a prediction task by passing model reference and shard file to it.\n",
    "    # NOTE: it would be highly inefficient if you are passing the model itself\n",
    "    # like  make_model_prediction.remote(model, file), which in order to pass the model\n",
    "    # to remote node will ray.put(model) for each task, potentially overwhelming\n",
    "    # the local object store and causing out-of-memory or out-of-disk error.\n",
    "    result_refs.append(make_model_batch_predictions.remote(model_ref, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bd8ce7-9b9b-45e3-a24a-882736445eef",
   "metadata": {},
   "source": [
    "Fetch the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2c5618-d45f-407f-8535-8a7626b66d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ray.get(result_refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169cd4aa-08e4-4234-9799-d3266355b1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check predictions and output size.\n",
    "for r in results:\n",
    "    print(\n",
    "        f\"Predictions dataframe size: {len(r)} | Total score for tips: {r['score'].sum()}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6626f9-2887-46ca-8c2c-db7a0173fb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517f5781-4ddc-4bcd-83b8-9bdbdbbf9f4d",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0658478a",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. Increase the fibonacci with 200K, 300K\n",
    "1. Add a compute intensive function; pick some function from your repo and convert to a remote task.\n",
    "1. (Optional) Run how to [compute PI](extra/highly_parallel.ipynb). **Note**: You can tweak with the `FULL_SAMPLE_COUNT`, to adjust the accuracy to the value of `math.pi`. `100 billion samples` may take too long. Play with this number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4fa0e7",
   "metadata": {},
   "source": [
    "### Homework\n",
    "1. Try writing a map-reduce app for word or character count in a list. Try first with simple case of a few lines, then extend it to a large file.\n",
    "1. Try using local [bubble sort](https://www.geeksforgeeks.org/python-program-for-bubble-sort/) and remote bubble sort\n",
    "1. Do you see the difference for small and large numbers?\n",
    "1. Read this [blog](https://www.anyscale.com/blog/parallelizing-python-code) and try some examples.\n",
    "1. Take an existing regression model, save it as in a model format, use this scaling technique to do batch inference at scale and in parallel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
