{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Guided Tour of Ray Core: Remote Objects\n",
    "\n",
    "© 2019-2023, Anyscale. All Rights Reserved\n",
    "\n",
    "## Overview\n",
    "\n",
    "In Ray, tasks and actors create and compute on objects. We refer to these objects as remote objects because they can be stored anywhere in a Ray cluster, and we use object refs to refer to them. Remote objects are cached in Ray’s distributed shared-memory object store, and there is one object store per node in the cluster. In the cluster setting, a remote object can live on one or many nodes, independent of who holds the object ref(s). Collectively, these individual object store makes a shared object store across the the Ray Cluster, as shown in the diagram below.\n",
    "\n",
    "[Remote Objects](https://docs.ray.io/en/latest/walkthrough.html#objects-in-ray)\n",
    "reside in a distributed [shared-memory object store](https://en.wikipedia.org/wiki/Shared_memory).\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Core/ray_system_architecture.png\" width=\"70%\" height=\"30%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Ray archictecture with Ray nodes, each with its own object store. Collectively, it's a shared object store across the cluster.|\n",
    "\n",
    "Objects are immutable and can be accessed from anywhere on the cluster, as they are stored in the cluster shared memory. An object ref is essentially a pointer or a unique ID that can be used to refer to a remote object without seeing its value. If you’re familiar with futures in Python, Java or Scala, Ray object refs are conceptually similar.\n",
    "\n",
    "In general, small objects are stored in their owner’s **in-process store** (**<=100KB**), while large objects are stored in the **distributed object store**. This decision is meant to reduce the memory footprint and resolution time for each object. Note that in the latter case, a placeholder object is stored in the in-process store to indicate the object has been promoted to shared memory.\n",
    "\n",
    "In the case if there is no space in the shared-memory, objects are spilled over to disk. But the main point here is that\n",
    "shared-memory allows _zero-copy_ access to processes on the same worker node.\n",
    "\n",
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Core/shared_memory_plasma_store.png\" height=\"60%\" width=\"65%\">\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "In this tutorial, you learn about:\n",
    "\n",
    "* Ray Futures as one of the patterns\n",
    "* Ray's distributed Plasma object store\n",
    "* How obejcts are stored and fetched from the distributed shared object store\n",
    "    * Use `ray.get` and `ray.put` examples\n",
    "* How to use Ray tasks and object store to do inference batching at scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object references as futures pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's start Ray…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import torch\n",
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ray.is_initialized:\n",
    "    ray.shutdown()\n",
    "ray.init(logging_level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Remote Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we'll create some python objects and put them in shared memory using the [Ray Core APIs](https://docs.ray.io/en/latest/ray-core/package-ref.html)\n",
    "\n",
    "* `ray.put()` - put an object in the in-memory object store and return its `RefObjectID`. Use this `RefObjectID` to pass object to any remote task or an Actor method call\n",
    "* `ray.get()` - get the values from a remote object or a list of remote objects from the object store\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/object_store.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Diagram of workers in worker nodes using `ray.put()` to place values and using `ray.get()` to retrieve them from each node's object store. If the workder node's does not have the value of the ObjectRefID, it'll fetched or copied from the worker's node that created it.|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to return an random tensor shape. We will use this\n",
    "tensor to store in our object store and retrieve it later for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rand_tensor(size: Tuple[int, int]) -> torch.tensor:\n",
    "    return torch.randn(size=(size), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def transform_rand_tensor(tensor: torch.tensor) -> torch.tensor:\n",
    "    return torch.mul(tensor, random.randint(2, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create random tensors and store them in object store\n",
    "1. create a random tensor\n",
    "2. put it in the object store\n",
    "3. the final list returned from the comprehension is list of `ObjectRefIDs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "#\n",
    "# Create a tensor of shape (X, 50)\n",
    "#\n",
    "tensor_list_obj_ref = [ray.put(create_rand_tensor(((i+1)*25, 50))) for i in range(0, 100)]\n",
    "tensor_list_obj_ref[:2], len(tensor_list_obj_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view the object store in the [Ray Dashboard](https://docs.ray.io/en/latest/ray-core/ray-dashboard.html)\n",
    "**Note**: Use the link above for the actual dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetch the random tensors from the object store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the value of this object reference. \n",
    "\n",
    "Small objects are resolved by copying them directly from the _owner’s_ **in-process store**. For example, if the owner calls `ray.get`, the system looks up and deserializes the value from the local **in-process store**. For larger objects greater than 100KB, they will be stored in the distributed object store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we got back a list of ObjectRefIDs, index into the first value of the tensor from \n",
    "# the list of ObectRefIDs\n",
    "val = ray.get(tensor_list_obj_ref[0])\n",
    "val.size(), val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can fetch all the values of multiple object references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ray.get(tensor_list_obj_ref)\n",
    "results[:1], results[:1][0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform tensors stored in the object store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform our tensors stored in the object store, put the transformed tensors in the object store (the ray remote task will implicity store it as a returned value), and then fetch the values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform each tensor in the object store with a remote task in our Python comprehension list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_object_list = [transform_rand_tensor.remote(t_obj_ref) for t_obj_ref in tensor_list_obj_ref]\n",
    "transformed_object_list[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch all the transformed tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_tensor_values = ray.get(transformed_object_list)\n",
    "transformed_tensor_values[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap\n",
    "Ray's object store is a shared memory store spanning a Ray cluster. Workers on each Ray node have their own object store, and they can use simple Ray APIs,`ray.put()` and `ray.get()`, to insert values and fetch values of Ray objects created by Ray tasks or Actor methods. Collectively, these individual object stores per node comprise a shared and distributed object store.  \n",
    "\n",
    "In the above exercise, we created random tensors, inserted them into our object store, transformed them, by iterating over each `ObjectRefID`, sending this `ObjectRefID` to a Ray task, and then fetching the transformed tensor returned by each Ray remote task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "See if you can write your own transformation to modify the tensor. Consider any of the following\n",
    "tensor transformations:\n",
    " 1. [torch.transpose](https://pytorch.org/docs/stable/generated/torch.transpose.html)\n",
    " 2. [torch.dot](https://pytorch.org/docs/stable/generated/torch.dot.html)\n",
    " 3. [torch.reshape](https://pytorch.org/docs/stable/generated/torch.reshape.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def transform_rand_tensor_2(tensor: torch.tensor) -> torch.tensor:\n",
    "    return torch.transpose(tensor, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing Objects by Reference\n",
    "\n",
    "Ray object references can be freely passed around a Ray application. This means that they can be passed as arguments to tasks, actor methods, and even stored in other objects. Objects are tracked via distributed reference counting, and their data is automatically freed once all references to the object are deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Task\n",
    "@ray.remote\n",
    "def echo(x):\n",
    "    print(f\"current value of argument x: {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some variables\n",
    "x = list(range(10))\n",
    "obj_ref_x = ray.put(x)\n",
    "y = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass-by-value\n",
    "\n",
    "Send the object to a task as a top-level argument.\n",
    "The object will be *de-referenced* automatically, so the task only sees its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send y as value argument\n",
    "echo.remote(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send a an object reference\n",
    "# note that the echo function deferences it\n",
    "echo.remote(obj_ref_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass-by-reference\n",
    "\n",
    "When a parameter is passed inside a Python list or as any other data structure,\n",
    "the *object ref is preserved*, meaning it's not *de-referenced*. The object data is not transferred to the worker when it is passed by reference, until `ray.get()` is called on the reference.\n",
    "\n",
    "You can pass by reference in two ways:\n",
    " 1. as a dictionary `.remote({\"obj\": obj_ref_x})`\n",
    " 2. as list of objRefs `.remote([obj_ref_x])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(range(20))\n",
    "obj_ref_x = ray.put(x)\n",
    "# Echo will not automaticall de-reference it\n",
    "echo.remote({\"obj\": obj_ref_x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo.remote([obj_ref_x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about long running tasks?\n",
    "\n",
    "Sometimes, you may have tasks that are long running, past their expected times due to some problem, maybe blocked on accessing a variable in the object store. How do you exit or terminate it? Use a timeout!\n",
    "\n",
    "Now let's set a timeout to return early from an attempted access of a remote object that is blocking for too long..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "@ray.remote\n",
    "def long_running_function ():\n",
    "    time.sleep(10)\n",
    "    return 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can control how long you want to wait for the task to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from ray.exceptions import GetTimeoutError\n",
    "\n",
    "obj_ref = long_running_function.remote()\n",
    "\n",
    "try:\n",
    "    ray.get(obj_ref, timeout=6)\n",
    "except GetTimeoutError:\n",
    "    print(\"`get` timed out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2:  How to use Tasks and object store for distributed batch inference \n",
    "\n",
    "Batch inference is a common distributed application workload in machine learning. It's a process of using a trained model to generate predictions for a collection of observations. \n",
    "Primarily, it has the following elements:\n",
    "\n",
    "**Input dataset**: This is a large collection of observations to generate predictions for. The data is usually stored in an external storage system like S3, HDFS or database, across\n",
    "many files.\n",
    "\n",
    "**ML model**: This is a trained ML model that is usually also stored in an external storage system or in a model store.\n",
    "\n",
    "**Predictions**: These are the outputs when applying the ML model on observations. Normally, predictions are usually written back to the storage system.\n",
    "\n",
    "For purpose of this exercise, we make the following provisions:\n",
    " * create a dummy model that returns some fake prediction\n",
    " * use real-world NYC taxi data to provide large data set for batch inference\n",
    " * return the predictions instead of writing it back to the disk\n",
    "\n",
    "As an example of scaling pattern called **Different Data Same Function** (DDSF), also known as **Distributed Data Parallel** (DDP), our function in this diagram is the \n",
    "pretrained **model**, and the data is split and disributed as **shards**.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Core/different_data_same_function.png\" width=\"65%\" height=\"35%\">|\n",
    "|:--|\n",
    "|Distributed batch inference: Different Data Same Function (DDSF).|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a Python closure to load our pretrained model. This model is just a fake model that predicts whether a \n",
    "tip is warranted contigent on the number of fares (2 or more) on collective rides.\n",
    "\n",
    "**Note**: This prediction is fake. The real model will invoke model's `model.predict(input_data)`. Yet\n",
    "it suffices for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model():\n",
    "    # A fake model that predicts whether tips were given based on number of passengers in the taxi cab.\n",
    "    def model(batch: pd.DataFrame) -> pd.DataFrame:\n",
    "        \n",
    "        # Some model weights and payload so Ray copies the model in the \n",
    "        # shared plasma store for tasks scheduled across nodes.\n",
    "        model.payload = np.arange(10, 100_000, dtype=float)\n",
    "        \n",
    "        # Try with a larger model on a larger machine\n",
    "        #model.payload = np.arange(100, 100_000_000, dtype=float)\n",
    "        model.cls = \"regression\"\n",
    "        \n",
    "        # give a tip if 2 or more passengers\n",
    "        predict = batch[\"passenger_count\"] >= 2 \n",
    "        return pd.DataFrame({\"score\": predict})\n",
    "    \n",
    "    return model    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a Ray task that will handle each shard of the NYC taxt data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def make_model_batch_predictions(model, shard_path, verbose=False):\n",
    "    if verbose:\n",
    "        print(f\"Batch inference for shard file: {shard_path}\")\n",
    "    df = pq.read_table(shard_path).to_pandas()\n",
    "    result = model(df)\n",
    "\n",
    "    # Return our prediction data frame\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the 12 files consisting of NYC data per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12 files, one for each remote task.\n",
    "input_files = [\n",
    "    f\"s3://anonymous@air-example-data/ursa-labs-taxi-data/downsampled_2009_full_year_data.parquet\"\n",
    "    f\"/fe41422b01c04169af2a65a83b753e0f_{i:06d}.parquet\" for i in range(12)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert model into the object store\n",
    "\n",
    "`ray.put()` the model just once to local object store, and then pass the reference to the remote tasks.\n",
    "\n",
    "It would be highly inefficient if you are passing the model itself like `make_model_prediction.remote(model, file)`,\n",
    "which in order to pass the model to remote node will implicitly do a `ray.put(model)` for each task, potentially overwhelming\n",
    "the local object store and causing out-of-memory.\n",
    "\n",
    "Instead, we will just pass a reference, and the node where the task is scheduled deference it.\n",
    "\n",
    "This is [Ray core API](https://docs.ray.io/en/latest/ray-core/package-ref.html) for putting objects into the Ray Plasma store. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model \n",
    "model = load_trained_model()\n",
    "\n",
    "# Put the model object into the shared object store.\n",
    "model_ref = ray.put(model)\n",
    "model_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List for holding all object references returned from the model's predictions\n",
    "result_refs = []\n",
    "\n",
    "# Launch all prediction tasks. For each file create a Ray remote task to do a batch inference\n",
    "for file in input_files:\n",
    "    \n",
    "    # Launch a prediction task by passing model reference and shard file to it.\n",
    "    result_refs.append(make_model_batch_predictions.remote(model_ref, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ray.get(result_refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check predictions and output size.\n",
    "for r in results:\n",
    "    print(f\"Predictions dataframe size: {len(r)} | Total score for tips: {r['score'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap\n",
    "\n",
    "We covered how to \n",
    " * use Ray `tasks`, `ray.get()` and `ray.put`, \n",
    " * understand distributed remote object store\n",
    " * how you to access objects from object store for transformation\n",
    "\n",
    "Let's move on to the [Ray Actors lesson](ex_03_remote_classes.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "1. Read references to get advanced deep dives and more about Ray objects\n",
    "2. [Serialization](https://docs.ray.io/en/latest/ray-core/objects/serialization.html)\n",
    "3. [Memory Management](https://docs.ray.io/en/latest/ray-core/objects/memory-management.html)\n",
    "4. [Object Spilling](https://docs.ray.io/en/latest/ray-core/objects/object-spilling.html)\n",
    "5. [Fault Tolerance](https://docs.ray.io/en/latest/ray-core/objects/fault-tolerance.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    " * [Ray Architecture Reference](https://docs.google.com/document/d/1tBw9A4j62ruI5omIJbMxly-la5w4q_TjyJgJL_jN2fI/preview)\n",
    " * [Ray Internals: A peek at ray,get](https://www.youtube.com/watch?v=a1kNnQu6vGw)\n",
    " * [Ray Internals: Object management with Ownership Model](https://www.youtube.com/watch?v=1oSBxTayfJc)\n",
    " * [Deep Dive into Ray scheduling Policies](https://www.youtube.com/watch?v=EJUYKXWGzfI)\n",
    " * [Redis in Ray: Past and future](https://www.anyscale.com/blog/redis-in-ray-past-and-future)\n",
    " * [StackOverFlow: How Ray Shares Data](https://stackoverflow.com/questions/58082023/how-exactly-does-ray-share-data-to-workers/71500979#71500979)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
