{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Finetuning and Batch Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Generic/ray_logo.png\" width=\"20%\" loading=\"lazy\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up imports and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import transformers\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "transformers.set_seed(42)\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Ray runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the Ray Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data ingest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from utils import get_random_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\").train_test_split(\n",
    "    test_size=0.2, seed=57\n",
    ")\n",
    "hf_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_random_elements(dataset=hf_dataset[\"train\"], num_examples=3)\n",
    "display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Ray Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_dataset = ray.data.from_huggingface(hf_dataset)\n",
    "ray_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up train and validation Ray datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_DATA = True\n",
    "\n",
    "if SMALL_DATA:\n",
    "    train_dataset = ray_dataset[\"train\"].limit(100)\n",
    "    validation_dataset = ray_dataset[\"test\"].limit(100)\n",
    "else:\n",
    "    train_dataset = ray_dataset[\"train\"]\n",
    "    validation_dataset = ray_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.preprocessors import BatchMapper\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(batch: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Tokenizes the input and instruction pairs in a batch using the T5 tokenizer\n",
    "    from the Google/Flan-T5-Base model, and returns a dictionary containing the\n",
    "    encoded inputs and labels.\n",
    "\n",
    "    Args:\n",
    "        batch: A dictionary containing at least two keys, \"instruction\" and\n",
    "        \"input\", whose values are lists of strings.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the encoded inputs and labels, as returned by\n",
    "        the T5 tokenizer.\n",
    "    \"\"\"\n",
    "    model_name = \"google/flan-t5-base\"\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "    encoded_inputs = tokenizer(\n",
    "        list(batch[\"instruction\"]),\n",
    "        list(batch[\"input\"]),\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"np\",\n",
    "    )\n",
    "\n",
    "    encoded_inputs[\"labels\"] = encoded_inputs[\"input_ids\"].copy()\n",
    "\n",
    "    return dict(encoded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_preprocessor = BatchMapper(preprocess_function, batch_format=\"pandas\", batch_size=4096)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize training logic for each worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "use_gpu = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_init_per_worker(\n",
    "    train_dataset: ray.data.Dataset,\n",
    "    eval_dataset: Optional[ray.data.Dataset] = None,\n",
    "    **config,\n",
    ") -> Trainer:\n",
    "    \"\"\"\n",
    "    Initializes a Hugging Face Trainer for training a T5 text generation model.\n",
    "\n",
    "    Args:\n",
    "        train_dataset (ray.data.Dataset): The dataset for training the model.\n",
    "        eval_dataset (ray.data.Dataset, optional): The dataset for evaluating\n",
    "        the model.\n",
    "            Defaults to None.\n",
    "        config: Additional arguments to configure the Trainer.\n",
    "\n",
    "    Returns:\n",
    "        Trainer: A Hugging Face Trainer for training the T5 model.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    model_name = \"google/flan-t5-base\"\n",
    "\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        \"flan-t5-base-finetuned-alpaca\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        learning_rate=config.get(\"learning_rate\", 2e-5),\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=config.get(\"epochs\", 4),\n",
    "        weight_decay=config.get(\"weight_decay\", 0.01),\n",
    "        push_to_hub=False,\n",
    "        disable_tqdm=True,\n",
    "    )\n",
    "\n",
    "    hf_trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    return hf_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.air.config import RunConfig, ScalingConfig, CheckpointConfig\n",
    "from ray.train.huggingface import HuggingFaceTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = HuggingFaceTrainer(\n",
    "    trainer_init_per_worker=trainer_init_per_worker,\n",
    "    scaling_config=ScalingConfig(num_workers=num_workers, use_gpu=use_gpu),\n",
    "    datasets={\n",
    "        \"train\": train_dataset,\n",
    "        \"evaluation\": validation_dataset,\n",
    "    },\n",
    "    run_config=RunConfig(\n",
    "        checkpoint_config=CheckpointConfig(\n",
    "            num_to_keep=1,\n",
    "            checkpoint_score_attribute=\"eval_loss\",\n",
    "            checkpoint_score_order=\"min\",\n",
    "        ),\n",
    "    ),\n",
    "    preprocessor=batch_preprocessor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/flan-t5-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = result.checkpoint\n",
    "finetuned_model = checkpoint.get_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"How many bees do I have?\"  # Enter your own instruction here.\n",
    "input_query = (\n",
    "    \"I don't have enough bees.\"  # Write additional context for the model here.\n",
    ")\n",
    "\n",
    "inputs = tokenizer(instruction, input_query, return_tensors=\"pt\")\n",
    "outputs = finetuned_model.generate(**inputs)\n",
    "\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Optional] Distributed hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from ray.tune import Tuner\n",
    "from ray.tune.schedulers.async_hyperband import ASHAScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num_trials = 4\n",
    "max_tune_epochs = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 1\n",
    "use_gpus = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = HuggingFaceTrainer(\n",
    "    trainer_init_per_worker=trainer_init_per_worker,\n",
    "    scaling_config=ScalingConfig(num_workers=num_workers, use_gpu=use_gpu),\n",
    "    datasets={\n",
    "        \"train\": train_dataset,\n",
    "        \"evaluation\": validation_dataset,\n",
    "    },\n",
    "    run_config=RunConfig(\n",
    "        checkpoint_config=CheckpointConfig(\n",
    "            num_to_keep=1,\n",
    "            checkpoint_score_attribute=\"eval_loss\",\n",
    "            checkpoint_score_order=\"min\",\n",
    "        ),\n",
    "    ),\n",
    "    preprocessor=batch_preprocessor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = Tuner(\n",
    "    trainer,\n",
    "    param_space={\n",
    "        \"trainer_init_config\": {\n",
    "            \"learning_rate\": tune.choice([2e-5, 2e-4, 2e-3, 2e-2]),\n",
    "            \"epochs\": tune.choice([2, 4, 8, max_tune_epochs]),\n",
    "            \"weight_decay\": tune.choice([0.01, 0.1, 1.0, 10.0]),\n",
    "        }\n",
    "    },\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"eval_loss\",\n",
    "        mode=\"min\",\n",
    "        num_samples=total_num_trials,\n",
    "        scheduler=ASHAScheduler(\n",
    "            max_t=max_tune_epochs,\n",
    "        ),\n",
    "    ),\n",
    "    run_config=RunConfig(\n",
    "        checkpoint_config=CheckpointConfig(\n",
    "            num_to_keep=1,\n",
    "            checkpoint_score_attribute=\"eval_loss\",\n",
    "            checkpoint_score_order=\"min\",\n",
    "        )\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_grid = tuner.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed batch inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.predictor import Predictor\n",
    "from ray.train.batch_predictor import BatchPredictor\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuggingFaceModelPredictor(Predictor):\n",
    "    \"\"\"\n",
    "    A Ray Predictor for HuggingFace models that generates text given input data.\n",
    "\n",
    "    Args:\n",
    "        model (transformers.PreTrainedModel): A trained HuggingFace model.\n",
    "        tokenizer (Optional[transformers.PreTrainedTokenizerBase]): A tokenizer\n",
    "        that can tokenize input text.\n",
    "        preprocessor (Optional[Callable]): A function that takes raw input data\n",
    "        and returns tokenized input data.\n",
    "        use_gpu (bool): Whether to use a GPU or CPU for prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Any,\n",
    "        tokenizer: Optional[Any] = None,\n",
    "        preprocessor: Optional[Any] = None,\n",
    "        use_gpu: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(preprocessor)\n",
    "        self.model = model\n",
    "        self.use_gpu = use_gpu\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    @classmethod\n",
    "    def from_checkpoint(\n",
    "        cls,\n",
    "        checkpoint: Any,\n",
    "        model_cls: Any,\n",
    "        *,\n",
    "        tokenizer: Optional[Any] = None,\n",
    "        use_gpu: bool = False,\n",
    "        **get_model_kwargs: Any,\n",
    "    ) -> \"HuggingFaceModelPredictor\":\n",
    "        \"\"\"\n",
    "        Create a HuggingFaceModelPredictor from a checkpoint.\n",
    "\n",
    "        Args:\n",
    "            checkpoint (Any): A checkpoint containing a trained HuggingFace model.\n",
    "            model_cls (Any): The type of HuggingFace model to load from the checkpoint.\n",
    "            tokenizer (Optional[Any]): A tokenizer that can tokenize input text.\n",
    "            use_gpu (bool): Whether to use a GPU or CPU for prediction.\n",
    "            **get_model_kwargs (Any): Additional keyword arguments for loading\n",
    "            the HuggingFace model.\n",
    "\n",
    "        Returns:\n",
    "            HuggingFaceModelPredictor: A Ray Predictor for the HuggingFace model.\n",
    "        \"\"\"\n",
    "        if not tokenizer:\n",
    "            tokenizer = AutoTokenizer\n",
    "        if isinstance(tokenizer, type):\n",
    "            tokenizer = checkpoint.get_tokenizer(tokenizer)\n",
    "        return cls(\n",
    "            checkpoint.get_model(model_cls, **get_model_kwargs),\n",
    "            tokenizer=tokenizer,\n",
    "            preprocessor=checkpoint.get_preprocessor(),\n",
    "            use_gpu=use_gpu,\n",
    "        )\n",
    "\n",
    "    def _predict_numpy(\n",
    "        self,\n",
    "        data: Dict[str, Any],\n",
    "        feature_columns: Optional[List[str]] = None,\n",
    "        **generate_kwargs: Any,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generates text given input data.\n",
    "\n",
    "        Args:\n",
    "            data (Dict[str, Any]): A dictionary of input data.\n",
    "            feature_columns (Optional[List[str]]): A list of feature column names\n",
    "            to use for prediction.\n",
    "            **generate_kwargs (Any): Additional keyword arguments for generating text.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A Pandas DataFrame with a single column \"generated_output\"\n",
    "            containing the generated text.\n",
    "        \"\"\"\n",
    "        # we get already tokenized text here because we have the tokenizer as an AIR preprocessor\n",
    "        if feature_columns:\n",
    "            data = {k: v for k, v in data.items() if k in feature_columns}\n",
    "\n",
    "        data = {\n",
    "            k: torch.from_numpy(v).to(device=self.model.device) for k, v in data.items()\n",
    "        }\n",
    "        generate_kwargs = {**data, **generate_kwargs}\n",
    "\n",
    "        outputs = self.model.generate(**generate_kwargs)\n",
    "        return pd.DataFrame(\n",
    "            self.tokenizer.batch_decode(outputs, skip_special_tokens=True),\n",
    "            columns=[\"generated_output\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = BatchPredictor.from_checkpoint(\n",
    "    checkpoint=result.checkpoint,\n",
    "    predictor_cls=HuggingFaceModelPredictor,\n",
    "    model_cls=T5ForConditionalGeneration,\n",
    "    tokenizer=T5Tokenizer,\n",
    "    use_gpu=use_gpu,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predictor.predict(\n",
    "    validation_dataset,\n",
    "    num_gpus_per_worker=int(use_gpu),\n",
    "    batch_size=256,\n",
    "    max_new_tokens=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect with the Ray community\n",
    "\n",
    "You can learn and get more involved with the Ray community of developers and researchers:\n",
    "\n",
    "* [**Ray documentation**](https://docs.ray.io/en/latest)\n",
    "\n",
    "* [**Official Ray site**](https://www.ray.io/)  \n",
    "Browse the ecosystem and use this site as a hub to get the information that you need to get going and building with Ray.\n",
    "\n",
    "* [**Join the community on Slack**](https://forms.gle/9TSdDYUgxYs8SA9e8)  \n",
    "Find friends to discuss your new learnings in our Slack space.\n",
    "\n",
    "* [**Use the discussion board**](https://discuss.ray.io/)  \n",
    "Ask questions, follow topics, and view announcements on this community forum.\n",
    "\n",
    "* [**Join a meetup group**](https://www.meetup.com/Bay-Area-Ray-Meetup/)  \n",
    "Tune in on meet-ups to listen to compelling talks, get to know other users, and meet the team behind Ray.\n",
    "\n",
    "* [**Open an issue**](https://github.com/ray-project/ray/issues/new/choose)  \n",
    "Ray is constantly evolving to improve developer experience. Submit feature requests, bug-reports, and get help via GitHub issues.\n",
    "\n",
    "* [**Become a Ray contributor**](https://docs.ray.io/en/latest/ray-contribute/getting-involved.html)  \n",
    "We welcome community contributions to improve our documentation and Ray framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Generic/ray_logo.png\" width=\"20%\" loading=\"lazy\">"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
