{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import transformers\n",
    "import warnings\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "transformers.set_seed(42)\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Ray runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open the Ray Dashboard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data ingest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\").train_test_split(\n",
    "    test_size=0.2\n",
    ")\n",
    "hf_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_elements(dataset: List, num_examples: int = 2) -> None:\n",
    "    \"\"\"\n",
    "    Picks a random subset of elements from the given dataset and displays them\n",
    "    as a Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        dataset: A list of elements to choose from.\n",
    "        num_examples: The number of elements to choose. Defaults to 2.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If `num_examples` is greater than the length of `dataset`.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    if num_examples > len(dataset):\n",
    "        raise ValueError(\"Can't pick more elements than there are in the dataset.\")\n",
    "\n",
    "    picks = random.sample(range(len(dataset)), k=num_examples)\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    display(HTML(df.to_html()))\n",
    "\n",
    "\n",
    "show_random_elements(hf_dataset[\"train\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Ray Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_dataset = ray.data.from_huggingface(hf_dataset)\n",
    "ray_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.preprocessors import BatchMapper\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(batch: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Tokenizes the input and instruction pairs in a batch using the T5 tokenizer\n",
    "    from the Google/Flan-T5-Base model, and returns a dictionary containing the\n",
    "    encoded inputs and labels.\n",
    "\n",
    "    Args:\n",
    "        batch: A dictionary containing at least two keys, \"instruction\" and \n",
    "        \"input\", whose values are lists of strings.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the encoded inputs and labels, as returned by \n",
    "        the T5 tokenizer.\n",
    "    \"\"\"\n",
    "    model_name = \"google/flan-t5-base\"\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "    encoded_inputs = tokenizer(\n",
    "        list(batch[\"instruction\"]),\n",
    "        list(batch[\"input\"]),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"np\",\n",
    "    )\n",
    "\n",
    "    encoded_inputs[\"labels\"] = encoded_inputs[\"input_ids\"].copy()\n",
    "\n",
    "    return dict(encoded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_preprocessor = BatchMapper(preprocess_function, batch_format=\"pandas\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed finetuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize training logic for each worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "use_gpu = True\n",
    "\n",
    "\n",
    "def trainer_init_per_worker(\n",
    "    train_dataset: ray.data.Dataset,\n",
    "    eval_dataset: Optional[ray.data.Dataset] = None,\n",
    "    **config,\n",
    ") -> Trainer:\n",
    "    \"\"\"\n",
    "    Initializes a Hugging Face Trainer for training a T5 text generation model.\n",
    "\n",
    "    Args:\n",
    "        train_dataset (ray.data.Dataset): The dataset for training the model.\n",
    "        eval_dataset (ray.data.Dataset, optional): The dataset for evaluating \n",
    "        the model.\n",
    "            Defaults to None.\n",
    "        **config: Additional arguments to configure the Trainer.\n",
    "\n",
    "    Returns:\n",
    "        Trainer: A Hugging Face Trainer for training the T5 model.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    model_name = \"google/flan-t5-base\"\n",
    "\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        \"flan-t5-base-finetuned-alpaca\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        learning_rate=config.get(\"learning_rate\", 2e-5),\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=config.get(\"epochs\", 2),\n",
    "        weight_decay=config.get(\"weight_decay\", 0.01),\n",
    "        push_to_hub=False,\n",
    "        disable_tqdm=True,\n",
    "        no_cuda=not use_gpu,\n",
    "    )\n",
    "\n",
    "    hf_trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    print(\"Starting training\")\n",
    "    return hf_trainer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.huggingface import HuggingFaceTrainer\n",
    "from ray.air.config import RunConfig, ScalingConfig, CheckpointConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_DATA = True\n",
    "\n",
    "if SMALL_DATA:\n",
    "    train_dataset = ray_dataset[\"train\"].limit(5)\n",
    "    validation_dataset = ray_dataset[\"test\"].limit(5)\n",
    "else:\n",
    "    train_dataset = ray_dataset[\"train\"]\n",
    "    validation_dataset = ray_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 2\n",
    "\n",
    "trainer = HuggingFaceTrainer(\n",
    "    trainer_init_per_worker=trainer_init_per_worker,\n",
    "    scaling_config=ScalingConfig(num_workers=num_workers, use_gpu=use_gpu),\n",
    "    datasets={\n",
    "        \"train\": train_dataset,\n",
    "        \"evaluation\": validation_dataset,\n",
    "    },\n",
    "    run_config=RunConfig(\n",
    "        checkpoint_config=CheckpointConfig(\n",
    "            num_to_keep=1,\n",
    "            checkpoint_score_attribute=\"eval_loss\",\n",
    "            checkpoint_score_order=\"min\",\n",
    "        ),\n",
    "    ),\n",
    "    preprocessor=batch_preprocessor,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = trainer.fit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = result.checkpoint\n",
    "finetuned_model = checkpoint.get_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"How many bees do I have?\"  # Enter your own instruction here.\n",
    "input = \"I don't have enough bees.\"  # Write additional context for the model here.\n",
    "\n",
    "inputs = tokenizer(instruction, input, return_tensors=\"pt\")\n",
    "outputs = finetuned_model.generate(**inputs)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Optional] Distributed hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from ray.tune import Tuner\n",
    "from ray.tune.schedulers.async_hyperband import ASHAScheduler\n",
    "\n",
    "total_num_trials = 4\n",
    "max_tune_epochs = 16\n",
    "\n",
    "tuner = Tuner(\n",
    "    trainer,\n",
    "    param_space={\n",
    "        \"trainer_init_config\": {\n",
    "            \"learning_rate\": tune.choice([2e-5, 2e-4, 2e-3, 2e-2]),\n",
    "            \"epochs\": tune.choice([2, 4, 8, max_tune_epochs]),\n",
    "            \"weight_decay\": tune.choice([0.01, 0.1, 1.0, 10.0]),\n",
    "        }\n",
    "    },\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"eval_loss\",\n",
    "        mode=\"min\",\n",
    "        num_samples=total_num_trials,\n",
    "        scheduler=ASHAScheduler(\n",
    "            max_t=max_tune_epochs,\n",
    "        ),\n",
    "    ),\n",
    "    run_config=RunConfig(\n",
    "        checkpoint_config=CheckpointConfig(\n",
    "            num_to_keep=1,\n",
    "            checkpoint_score_attribute=\"eval_loss\",\n",
    "            checkpoint_score_order=\"min\",\n",
    "        )\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_grid = tuner.fit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed batch inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.predictor import Predictor\n",
    "from ray.train.batch_predictor import BatchPredictor\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuggingFaceModelPredictor(Predictor):\n",
    "    \"\"\"\n",
    "    A Ray Predictor for HuggingFace models that generates text given input data.\n",
    "\n",
    "    Args:\n",
    "        model (transformers.PreTrainedModel): A trained HuggingFace model.\n",
    "        tokenizer (Optional[transformers.PreTrainedTokenizerBase]): A tokenizer \n",
    "        that can tokenize input text.\n",
    "        preprocessor (Optional[Callable]): A function that takes raw input data \n",
    "        and returns tokenized input data.\n",
    "        use_gpu (bool): Whether to use a GPU or CPU for prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Any,\n",
    "        tokenizer: Optional[Any] = None,\n",
    "        preprocessor: Optional[Any] = None,\n",
    "        use_gpu: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(preprocessor)\n",
    "        self.model = model\n",
    "        self.use_gpu = use_gpu\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    @classmethod\n",
    "    def from_checkpoint(\n",
    "        cls,\n",
    "        checkpoint: Any,\n",
    "        model_cls: Any,\n",
    "        *,\n",
    "        tokenizer: Optional[Any] = None,\n",
    "        use_gpu: bool = False,\n",
    "        **get_model_kwargs: Any,\n",
    "    ) -> \"HuggingFaceModelPredictor\":\n",
    "        \"\"\"\n",
    "        Create a HuggingFaceModelPredictor from a checkpoint.\n",
    "\n",
    "        Args:\n",
    "            checkpoint (Any): A checkpoint containing a trained HuggingFace model.\n",
    "            model_cls (Any): The type of HuggingFace model to load from the checkpoint.\n",
    "            tokenizer (Optional[Any]): A tokenizer that can tokenize input text.\n",
    "            use_gpu (bool): Whether to use a GPU or CPU for prediction.\n",
    "            **get_model_kwargs (Any): Additional keyword arguments for loading \n",
    "            the HuggingFace model.\n",
    "\n",
    "        Returns:\n",
    "            HuggingFaceModelPredictor: A Ray Predictor for the HuggingFace model.\n",
    "        \"\"\"\n",
    "        if not tokenizer:\n",
    "            tokenizer = AutoTokenizer\n",
    "        if isinstance(tokenizer, type):\n",
    "            tokenizer = checkpoint.get_tokenizer(tokenizer)\n",
    "        return cls(\n",
    "            checkpoint.get_model(model_cls, **get_model_kwargs),\n",
    "            tokenizer=tokenizer,\n",
    "            preprocessor=checkpoint.get_preprocessor(),\n",
    "            use_gpu=use_gpu,\n",
    "        )\n",
    "\n",
    "    def _predict_numpy(\n",
    "        self,\n",
    "        data: Dict[str, Any],\n",
    "        feature_columns: Optional[List[str]] = None,\n",
    "        **generate_kwargs: Any,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generates text given input data.\n",
    "\n",
    "        Args:\n",
    "            data (Dict[str, Any]): A dictionary of input data.\n",
    "            feature_columns (Optional[List[str]]): A list of feature column names \n",
    "            to use for prediction.\n",
    "            **generate_kwargs (Any): Additional keyword arguments for generating text.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A Pandas DataFrame with a single column \"generated_output\"\n",
    "            containing the generated text.\n",
    "        \"\"\"\n",
    "        # we get already tokenized text here because we have the tokenizer as an AIR preprocessor\n",
    "        if feature_columns:\n",
    "            data = {k: v for k, v in data.items() if k in feature_columns}\n",
    "\n",
    "        data = {\n",
    "            k: torch.from_numpy(v).to(device=self.model.device) for k, v in data.items()\n",
    "        }\n",
    "        generate_kwargs = {**data, **generate_kwargs}\n",
    "\n",
    "        outputs = self.model.generate(**generate_kwargs)\n",
    "        return pd.DataFrame(\n",
    "            self.tokenizer.batch_decode(outputs, skip_special_tokens=True),\n",
    "            columns=[\"generated_output\"],\n",
    "        )\n",
    "\n",
    "\n",
    "predictor = BatchPredictor.from_checkpoint(\n",
    "    checkpoint=result.checkpoint,\n",
    "    predictor_cls=HuggingFaceModelPredictor,\n",
    "    model_cls=T5ForConditionalGeneration,\n",
    "    tokenizer=T5Tokenizer,\n",
    "    use_gpu=use_gpu,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predictor.predict(\n",
    "    validation_dataset,\n",
    "    num_gpus_per_worker=int(use_gpu),\n",
    "    batch_size=256,\n",
    "    max_new_tokens=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.to_pandas()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
