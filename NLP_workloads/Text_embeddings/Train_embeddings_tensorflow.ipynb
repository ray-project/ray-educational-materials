{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b8f8e2-e1ed-43b5-8da0-deb433e76c46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import zipfile\n",
    "import ray\n",
    "\n",
    "from ray import tune\n",
    "from ray import serve\n",
    "from ray.air.config import ScalingConfig\n",
    "from ray.train.xgboost import XGBoostTrainer\n",
    "from ray.train.xgboost import XGBoostPredictor\n",
    "from ray.train.batch_predictor import BatchPredictor\n",
    "from ray.serve import PredictorDeployment\n",
    "from ray.serve.http_adapters import pandas_read_json\n",
    "from ray.tune import Tuner, TuneConfig\n",
    "\n",
    "import requests\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5d3cba-e02a-4b85-aed0-d4d3a335d3e9",
   "metadata": {},
   "source": [
    "# Ray Train\n",
    "\n",
    "## Intro\n",
    "\n",
    "### Outline\n",
    "\n",
    "-   Goals\n",
    "-   Ray Air `Trainer`\n",
    "    - Design\n",
    "    - Flavors\n",
    "    - In-depth with Tensorflow Trainer\n",
    "\n",
    "### Model scenarios with Ray + Tensorflow Trainer\n",
    "\n",
    "- Start with a minimal model and focus on key elements for Ray Train workflow\n",
    "- Port a Tensorflow tutorial word2vec model to Ray Train\n",
    "\n",
    "### Context: Ray AIR\n",
    "\n",
    "Ray AIR is the Ray AI Runtime, a set of high-level easy-to-use APIs for\n",
    "ingesting data, training models – including reinforcement learning\n",
    "models – tuning those models and then serving them.\n",
    "\n",
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Introduction_to_Ray_AIR/e2e_air.png\" width=600 loading=\"lazy\"/>\n",
    "\n",
    "Key principles behind Ray and Ray AIR are\n",
    "* Performance\n",
    "* Developer experience and simplicity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aa6a9b-e702-45d2-b59a-75609738077c",
   "metadata": {},
   "source": [
    "__Read, preprocess with Ray Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9351ec6-88cc-4ab1-ae1b-ed025d9046bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = ray.data.read_parquet(\"s3://anyscale-training-data/intro-to-ray-air/nyc_taxi_2021.parquet\")\n",
    "\n",
    "train_dataset, valid_dataset = dataset.train_test_split(test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffefcdb9-360b-4244-828a-8a49adb6a8a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "__Fit model with Ray Train__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1291d94e-376f-423c-b624-f1f6651e4dc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = XGBoostTrainer(\n",
    "    label_column=\"is_big_tip\",\n",
    "    scaling_config=ScalingConfig(num_workers=32, use_gpu=False),\n",
    "    params={ \"objective\": \"binary:logistic\", },\n",
    "    datasets={\"train\": train_dataset, \"valid\": valid_dataset},\n",
    ")\n",
    "\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6126df-8f78-4b4d-a955-a12b0b96d371",
   "metadata": {},
   "source": [
    "__Optimize hyperparams with Ray Tune__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fb7f4f-d803-40ee-8e7d-6655a396f950",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuner = Tuner(trainer, \n",
    "            param_space={'params' : {'max_depth': tune.randint(2, 12)}},\n",
    "            tune_config=TuneConfig(num_samples=10, metric='train-logloss', mode='min'))\n",
    "\n",
    "checkpoint = tuner.fit().get_best_result().checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e681533b-0df1-45c9-81bc-74e40ce076d8",
   "metadata": {},
   "source": [
    "__Batch prediction__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1d33dc-d7d2-4a7f-b309-0b7aac030994",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_predictor = BatchPredictor.from_checkpoint(checkpoint, XGBoostPredictor)\n",
    "\n",
    "predicted_probabilities = batch_predictor.predict(valid_dataset.drop_columns(['is_big_tip']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d66b61a-f001-483c-846c-02b5f8c06acc",
   "metadata": {},
   "source": [
    "__Online prediction with Ray Serve__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af460fe-18f9-4af0-8301-9d00ca00bca6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "deployment = PredictorDeployment.bind(XGBoostPredictor, result.checkpoint, http_adapter=pandas_read_json)\n",
    "\n",
    "serve.run(deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592bab94-4fd1-41fc-a038-f77669d15922",
   "metadata": {},
   "source": [
    "__HTTP or Python services__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05578ff7-161b-4e5a-b825-18ceba9393f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_input = dict(valid_dataset.take(1)[0])\n",
    "del(sample_input['is_big_tip'])\n",
    "del(sample_input['__index_level_0__'])\n",
    "requests.post(\"http://localhost:8000/\", json=[sample_input]).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec926284-e57d-4ac8-a2ce-151e0e153dce",
   "metadata": {},
   "source": [
    "## Ray Train Goals\n",
    "\n",
    "* Developer experience\n",
    "* Flexibility\n",
    "* Performance and simplicity via delegation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71b7829-aa50-45ee-8f5c-123d09d17572",
   "metadata": {},
   "source": [
    "## API design and usage\n",
    "\n",
    "### Ideas\n",
    "\n",
    "* `Trainer` and `Checkpoint` are key classes\n",
    "    * Supported by additional classes, e.g., `ScalingConfig`\n",
    "* Train does not re-implement distributed optimizers\n",
    "    * Train coordinates and delegates native library distributed training\n",
    "* `Trainer` produces `Checkpoint`(s)\n",
    "    * `Checkpoint` encapsulates outputs\n",
    "        * Model weights/assets\n",
    "            * Typically by reference to file storage\n",
    "        * Scores/stats\n",
    "* `Trainer` is used by Train, Tune\n",
    "* `Checkpoint` is used for inference (Ray Data [batch], Serve [online]) and reporting\n",
    "* These API patterns apply across __all__ `Trainer` flavors\n",
    "\n",
    "### Trainer Flavors\n",
    "\n",
    "* Tree - e.g., XGBoost\n",
    "* Library - e.g., Huggingface\n",
    "* DL Trainers\n",
    "    * PyTorch, TensorFlow, Horovod, Lightning, Accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d515f2f-7e53-424e-a010-8e290e51e27f",
   "metadata": {},
   "source": [
    "### Focus: Tensorflow Trainer\n",
    "\n",
    "Tensorflow Trainer automates deployment of MultiWorkerMirroredStrategy\n",
    "\n",
    "> This strategy implements synchronous distributed training across multiple workers, each with potentially multiple GPUs. Similar to tf.distribute.MirroredStrategy, it replicates all variables and computations to each local device. The difference is that it uses a distributed collective implementation (e.g. all-reduce), so that multiple workers can work together.\n",
    "> (https://www.tensorflow.org/api_docs/python/tf/distribute/MultiWorkerMirroredStrategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85894e8-86b3-46b0-a8ef-897ceadaf400",
   "metadata": {},
   "source": [
    "## \"Hello TF Trainer World\"\n",
    "\n",
    "We'll build a minimal example example with the iris dataset and a trivial model -- the goal is to look at the data/train code structure using Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dead525b-ea0a-4982-bfd2-ef513abdd0b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from ray.data.preprocessors import Concatenator, OneHotEncoder\n",
    "from ray.air import session\n",
    "from ray.air.integrations.keras import ReportCheckpointCallback\n",
    "from ray.train.tensorflow import TensorflowTrainer\n",
    "from ray.air.config import ScalingConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ebdde4-ccf9-4b75-848c-8a0c48d1674d",
   "metadata": {},
   "source": [
    "In this example, we'll start with a Ray Datase. Using Ray Data and dataset is optional -- we can use existing `tf.data` datasets if we like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255a093b-83df-4a6b-bb25-59e4f95bd87c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = ray.data.read_csv(\"s3://air-example-data/iris.csv\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ab100-441e-4e11-8298-c33c4c8bafe7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b858d09-a244-449b-828f-6ba02746b975",
   "metadata": {},
   "source": [
    "Ray Data directly supports NumPy tensor data, but sometimes we are ingesting tabular business data with distinct columns.\n",
    "\n",
    "If your dataset contains multiple features but your model accepts a single tensor as input, combine features with Concatenator.\n",
    "\n",
    "https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.to_tf.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb72a71-998e-4db2-9eb6-db7fd4f8c7e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_preprocessor = Concatenator(output_column_name=\"features\", exclude=\"target\")\n",
    "\n",
    "ds = feature_preprocessor.transform(ds)\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dba4e6-639c-4573-9c07-536056f2639d",
   "metadata": {},
   "source": [
    "One-hot encode target category label -- similar to `tf.keras.utils.to_categorical`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3e4bc5-47c4-49da-81b0-ba725913b296",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_onehot = OneHotEncoder(columns=[\"target\"])\n",
    "\n",
    "ds = target_onehot.fit_transform(ds)\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b312fd1f-b210-4007-b6b3-2bdec09f4180",
   "metadata": {
    "tags": []
   },
   "source": [
    "Collect one-hot encoded columns into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ef10c9-c097-4881-9534-bcd0f70fe39b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_concat = Concatenator(output_column_name=\"target\", exclude=\"features\")\n",
    "\n",
    "ds = target_concat.transform(ds)\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1aa811-849a-4aea-8adb-0cd4c433548f",
   "metadata": {},
   "source": [
    "Define a function for building our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ef9474-33f9-48d2-8cb7-7ef7946e1bbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model() -> tf.keras.Model:\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.InputLayer(input_shape=(4,)),\n",
    "            tf.keras.layers.Dense(5, activation='relu'),\n",
    "            tf.keras.layers.Dense(3, activation='softmax')\n",
    "        ]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b52255-391c-4a69-8d59-91559ab8f4df",
   "metadata": {},
   "source": [
    "### Per-worker training function\n",
    "\n",
    "The key element of the Ray Train interface pattern is the per-worker training function.\n",
    "\n",
    "* Will run on each worker\n",
    "* Receives a dict of configurations\n",
    "* Can interact with other parts of the distributed training collective via `session`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90f816b-fd69-4161-bf09-c43d98460457",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_func(config: dict):\n",
    "    batch_size = config.get(\"batch_size\", 64)\n",
    "    epochs = config.get(\"epochs\", 3)\n",
    "\n",
    "    strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "    with strategy.scope():\n",
    "        multi_worker_model = build_model()\n",
    "        multi_worker_model.compile(\n",
    "            optimizer=tf.keras.optimizers.SGD(learning_rate=config.get(\"lr\", 1e-3)),\n",
    "            loss=tf.keras.metrics.categorical_crossentropy,\n",
    "            metrics=[tf.keras.metrics.categorical_crossentropy],\n",
    "        )\n",
    "\n",
    "    dataset = session.get_dataset_shard(\"train\")\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    tf_dataset = dataset.to_tf(feature_columns=\"features\", label_columns=\"target\", batch_size=batch_size)\n",
    "        \n",
    "    history = multi_worker_model.fit(tf_dataset, epochs=epochs, callbacks=[ReportCheckpointCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8215ddff-fc32-4e35-83c4-5672dc640441",
   "metadata": {},
   "source": [
    "__Notes:__\n",
    "* Model building/compiling takes place inside `strategy.scope()` context block\n",
    "* Data access and model.fit(...) is inside the function\n",
    "* Inputs are provided to this function via `config` and `session`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c558df5-3fa0-4062-af2d-ff7c670a830e",
   "metadata": {},
   "source": [
    "<img src='https://docs.ray.io/en/latest/_images/session.svg' width=800 />\n",
    "\n",
    "(https://docs.ray.io/en/latest/ray-air/api/session.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054e9c1a-e95a-47fc-8100-c0a49080fe19",
   "metadata": {},
   "source": [
    "Function output is via Callbacks with Checkpoints\n",
    "* ray.air.integrations.keras.ReportCheckpointCallback https://docs.ray.io/en/latest/tune/api/doc/ray.air.integrations.keras.ReportCheckpointCallback.html\n",
    "* *To save a model to use for the TensorflowPredictor, you must save it under the “model” kwarg in Checkpoint passed to session.report().*\n",
    "    * https://docs.ray.io/en/latest/train/api/doc/ray.train.tensorflow.TensorflowTrainer.html#ray.train.tensorflow.TensorflowTrainer\n",
    "\n",
    "If we don't want to use Callbacks, we can use `session` to manually report information including Checkpoints:\n",
    "\n",
    "```python\n",
    "session.report(\n",
    "    {},\n",
    "    checkpoint=Checkpoint.from_dict(dict(epoch=epoch, model=model.get_weights())\n",
    "    ),\n",
    ")\n",
    "```\n",
    "*In this latter example, we're responsible for checkpointing frequency, which means we may need to train on one epoch at a time*\n",
    "\n",
    "(https://docs.ray.io/en/latest/train/dl_guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8faee0-fa3d-432b-8435-b9ee5bed2581",
   "metadata": {},
   "source": [
    "### Using the Trainer\n",
    "\n",
    "The per-worker training function contains our logic. The `TensorflowTrainer` instance ties that function together with configuration and orchestrates the training operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7502a6e-fdcb-46ea-9271-d514a8f32cc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_config = {\"lr\": 1e-3, \"batch_size\": 32, \"epochs\": 4}\n",
    "\n",
    "scaling_config = ScalingConfig(num_workers=2, use_gpu=False)\n",
    "\n",
    "trainer = TensorflowTrainer(\n",
    "    train_loop_per_worker=train_func,\n",
    "    train_loop_config=train_config,\n",
    "    scaling_config=scaling_config,\n",
    "    datasets={\"train\": ds},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7373c811-449d-4f82-b4fe-ccca2b74e803",
   "metadata": {},
   "source": [
    "We fit a trainer to get a result, which wraps metadata and a model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5bc5bb-12e5-41cd-87b1-4028841ec3dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad20af9-34e5-41c5-a420-424171d15865",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a689919-2ae0-4ae9-8a60-48c52ca3c5d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result.checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb60148-fe40-4a1a-b304-b17e33844e59",
   "metadata": {},
   "source": [
    "## Porting a word2vec model from TF to Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00506c91-798f-455f-a3f6-099d6a244c28",
   "metadata": {},
   "source": [
    "In this example, we'll use the skip-gram word2vec model from the Tensorflow word2vec tutorial (https://www.tensorflow.org/tutorials/text/word2vec)\n",
    "* Focus on adapting training to Ray\n",
    "* Start with existing training dataset in `tf.data.Dataset` form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906f9997-d8e8-45c1-942c-59f6bae35a1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.target_embedding = layers.Embedding(vocab_size,\n",
    "                                      embedding_dim,\n",
    "                                      input_length=1,\n",
    "                                      name=\"w2v_embedding\")\n",
    "\n",
    "        num_ns = 4 # from dataset construction\n",
    "        self.context_embedding = layers.Embedding(vocab_size,\n",
    "                                       embedding_dim,\n",
    "                                       input_length=num_ns+1)\n",
    "\n",
    "    def call(self, pair):\n",
    "        target, context = pair\n",
    "        if len(target.shape) == 2:\n",
    "            target = tf.squeeze(target, axis=1)\n",
    "        word_emb = self.target_embedding(target)\n",
    "        context_emb = self.context_embedding(context)\n",
    "        dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "        return dots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62cf971-d2b8-45ea-9b93-d6bc8ffa6f20",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following code is adapted from Ray's documentation walkthrough for porting TF code to Ray AIR: https://docs.ray.io/en/latest/ray-air/examples/convert_existing_tf_code_to_ray_air.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988d9fb2-faee-4244-b3ae-8b8fbe5cbcd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Pass in the hyperparameter config\n",
    "def train_func(config: dict):\n",
    "    epochs = config.get(\"epochs\", 5)\n",
    "    batch_size_per_worker = config.get(\"batch_size\", 32)\n",
    "    buffer_size = config.get(\"buffer_size\", 8192)\n",
    "    \n",
    "    # 2. Synchronized model setup\n",
    "    # \n",
    "    # Important: The strategy must be instantiated at the beginning\n",
    "    #     of the function, since the tf.Dataset that we load later needs\n",
    "    #     to be auto-sharded.\n",
    "    #     See https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras\n",
    "    #     for more details.\n",
    "    strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "    with strategy.scope():\n",
    "        vocab_size = 4096\n",
    "        embedding_dim = 128\n",
    "        model = Word2Vec(vocab_size, embedding_dim)\n",
    "        model.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "    # 3. Set a `global_batch_size` so that every worker gets the specified\n",
    "    #    `batch_size_per_worker` regardless of the number of workers.\n",
    "    #    This is needed because the datasets are sharded across `session.get_world_size()` workers.\n",
    "    global_batch_size = batch_size_per_worker * session.get_world_size()\n",
    "    \n",
    "    # Download data\n",
    "    s3 = boto3.client('s3')\n",
    "    s3.download_file('anyscale-training-data', config.get('tf_data'), 'tfdata.zip')\n",
    "    unique_tempdir = '/tmp/' + str(session.get_world_rank())\n",
    "    with zipfile.ZipFile('tfdata.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall(unique_tempdir)\n",
    "    \n",
    "    ds_path = ds_path = unique_tempdir + '/w2v.data.tf'\n",
    "    train_ds = tf.data.Dataset.load(ds_path).shuffle(buffer_size).batch(global_batch_size).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    # ^ Even though we are loading the dataset as a standard TF dataset, \n",
    "    #   TF will automatically shard the datasets across workers, according to the strategy.\n",
    "      \n",
    "    # 4. Report metrics and checkpoint the model\n",
    "    report_metrics_and_checkpoint_callback = ReportCheckpointCallback(report_metrics_on=\"epoch_end\")\n",
    "    model.fit(\n",
    "        train_ds,\n",
    "        batch_size=batch_size_per_worker,\n",
    "        epochs=epochs,\n",
    "        callbacks=[report_metrics_and_checkpoint_callback],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3abde24-49be-49c3-8683-027380659afa",
   "metadata": {},
   "source": [
    "The `TensorflowTrainer` setup links our configurations and training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47721bc5-4181-4429-96ed-ef6f33ecbea4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "train_config = {\"batch_size\": BATCH_SIZE, \"epochs\": 4, \"buffer_size\" : BUFFER_SIZE, \"tf_data\" : \"tf-w2v-sample-data/w2v.data.tf.zip\" }\n",
    "\n",
    "scaling_config = ScalingConfig(num_workers=8, use_gpu=False)\n",
    "\n",
    "trainer = TensorflowTrainer(\n",
    "    train_loop_per_worker=train_func,\n",
    "    train_loop_config=train_config,\n",
    "    scaling_config=scaling_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f0f9d0-0c35-41ea-bf2e-db7f0a8ff5ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68dd839-d563-413b-9905-5053f96aef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27258995-b060-463a-9e13-a29f8edc734f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result.checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d497e48-a917-47ac-9eb6-4d8cb40554d9",
   "metadata": {},
   "source": [
    "We could use this checkpoint for batch processing or online inference.\n",
    "\n",
    "There are a few additional options for checkpoints (e.g., number of checkpoints to retain and the mechanism for ranking them to support the `best_checkpoints` API) which can be set via `CheckpointConfig` https://docs.ray.io/en/latest/ray-air/api/doc/ray.air.CheckpointConfig.html#ray.air.CheckpointConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954c1f28-7d7c-40a1-8570-10f9e3dd2719",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
