{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e066191-c0be-46c5-9df0-def30f7a06dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.air.config import ScalingConfig, RunConfig\n",
    "from ray import serve\n",
    "import requests, json\n",
    "from starlette.requests import Request\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2120b0-0818-4091-972f-f0d6b3cddc44",
   "metadata": {
    "id": "6eae86f3-34a4-4f59-87ba-df8d9a0d3928"
   },
   "source": [
    "# Declarative (graph) composition pattern\n",
    "\n",
    "What is the Deployment Graph API?\n",
    "\n",
    "* The Deployment Graph API lets us separate the flow of calls from the logic inside our services.\n",
    "\n",
    "Why might we want to use the Deployment Graph (DAG) API to separate flow from logic?\n",
    "\n",
    "* It may be valuable to add a layer of indirection – or abstraction – so that we can more easily create and compose reusable services\n",
    "* The DAG API lets us use similar patterns across the Ray platform (e.g., Ray Workflow)\n",
    "    * We can learn one general pattern for graphs and use that intuition in multiple places in our Ray applications\n",
    "* Although we compose one DAG, we retain the key Ray Serve features of granular autoscaling and resource allocation\n",
    "\n",
    "Let’s reproduce our chat service flow using the Deployment Graph API\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "__Roadmap to multilingual chat with deployment graphs__\n",
    "\n",
    "1. Learn core concepts with a basic linear graph to begin a French-only chat\n",
    "1. Demonstrate split/combine graph pattern to fully enable French chat\n",
    "1. Add conditional flow for English/French chat\n",
    "    \n",
    "</div>\n",
    "\n",
    "## Getting started with deployment graphs\n",
    "\n",
    "As a first step, to keep things simple, let’s assume for a moment that we are always interacting with the service in French. \n",
    "\n",
    "<img src='https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Serve/deployment_graph_simple.png' width=900/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24097c90-cbab-410c-a983-a5e92ef4f105",
   "metadata": {
    "id": "3500503d-0831-47ae-88e0-88b8a8a7e2cd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ray.serve.dag import InputNode\n",
    "from ray.serve.drivers import DAGDriver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955ca50c-2af5-42d4-afb6-57522f1316ff",
   "metadata": {},
   "source": [
    "`InputNode` is a special type of graph node, defined by Ray Serve, which represents values supplied to our service endpoint. \n",
    "\n",
    "We can only have one `InputNode` but we can get access to multiple parameters from that node using a Python context manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856a6f3f-4592-4b25-a2dc-a63bc40ce63d",
   "metadata": {
    "id": "867cba3f-7d6e-4593-9dac-49f1069c93b0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "with InputNode() as inp:\n",
    "    user_input = inp[0]\n",
    "    history = inp[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8990d4bd-4647-4ab7-9b31-8b6e77f062ec",
   "metadata": {
    "id": "d2463417-b8c8-41ee-a259-7708d3952ff8"
   },
   "source": [
    "Here is a minimal, linear pipeline that allows us to begin a chat in French.\n",
    "\n",
    "We build up the graph step by step, `bind`ing each deployment to its dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac30c90c-9fd3-4d7e-a98c-88173832229f",
   "metadata": {
    "id": "d1b79e65-6a72-4046-a551-dea32116b83d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment(ray_actor_options={\"runtime_env\" : { \"pip\": [\"sentencepiece\"]} })\n",
    "class Translate:\n",
    "    def __init__(self, task: str, model: str):\n",
    "        self._task = task\n",
    "        self._model = model\n",
    "        self._pipeline = None\n",
    "    \n",
    "    def get_response(self, user_input: str) -> str:\n",
    "        if (self._pipeline is None):\n",
    "            self._pipeline = pipeline(task=self._task, model=self._model)\n",
    "        outputs = self._pipeline(user_input)\n",
    "        response = outputs[0]['translation_text']\n",
    "        return response\n",
    "        \n",
    "translate_en_fr = Translate.bind(task='translation_en_to_fr', model='t5-small')\n",
    "translate_fr_en = Translate.bind(task='translation_fr_to_en', model='Helsinki-NLP/opus-mt-fr-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a782f7-6a82-49da-821f-01fa8f68527e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment(ray_actor_options={'num_gpus': 0.5})\n",
    "class Chat:\n",
    "    def __init__(self, model: str):\n",
    "        # configure stateful elements of our service such as loading a model\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        self._model =  AutoModelForSeq2SeqLM.from_pretrained(model).to(0)\n",
    "\n",
    "    async def __call__(self, request: Request) -> dict:\n",
    "        # path to handle HTTP requests\n",
    "        data = await request.json()\n",
    "        data = json.loads(data)\n",
    "        # after decoding the payload, we delegate to get_response for logic\n",
    "        return {'response': self.get_response(data['user_input'], data['history']) }\n",
    "    \n",
    "    def get_response(self, user_input: str, history: list[str]) -> str:\n",
    "        # this method receives calls directly (from Python) or from __call__ (from HTTP)\n",
    "        history.append(user_input)\n",
    "        # the history is client-side state and will be a list of raw strings;\n",
    "        # for the default config of the model and tokenizer, history should be joined with '</s><s>'\n",
    "        inputs = self._tokenizer('</s><s>'.join(history), return_tensors='pt').to(0)\n",
    "        reply_ids = self._model.generate(**inputs, max_new_tokens=500)\n",
    "        response = self._tokenizer.batch_decode(reply_ids.cpu(), skip_special_tokens=True)[0]\n",
    "        return response\n",
    "    \n",
    "chat = Chat.bind(model='facebook/blenderbot-400M-distill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9643a7e-0710-4a29-9f27-fd6f591dbac5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6552ef01-a62b-497f-b578-8fc20edc6000",
    "outputId": "ad1eb72e-6c8c-473f-a92f-89d5bbebdeb1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input_en = translate_fr_en.get_response.bind(user_input)    # French->English translator depends on the user input text\n",
    "chat_response = chat.get_response.bind(user_input_en, history)   # the chat deployment requires the English user input and the history\n",
    "output = translate_en_fr.get_response.bind(chat_response)        # English->French translator depends on the English chat output\n",
    "serve_dag = DAGDriver.bind(output)                               # the graph returns the output from the English->French translator\n",
    "\n",
    "handle = serve.run(serve_dag, name='basic_linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464ec78b-f1f9-4fa5-be61-856ca51601f2",
   "metadata": {},
   "source": [
    "We start the application by calling `serve.run()` on the DAGDriver, a Ray Serve component which routes HTTP requests through your call graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49601090-f956-4780-a889-ce671b31ae33",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234
    },
    "id": "5e740b79-0e42-4883-9b38-ab3976e8f160",
    "outputId": "28adadf4-9a78-44ae-e235-9f1ddf233a75",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray.get(handle.predict.remote('Mes amis sont cool mais ils mangent trop de glucides.', []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7c8b83-8ee1-4728-b456-11eef5aa2e00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.delete('basic_linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d739f9e4-fcc3-4f71-a296-ef1f663d811a",
   "metadata": {
    "id": "d7505b58-70d8-4f50-963b-919ed251ca2e"
   },
   "source": [
    "How can we continue the chat?\n",
    "\n",
    "We need to supply English history ... but we only have French responses so far.\n",
    "\n",
    "We can use the pattern of adding a __combine node__ to our graph in order to merge the 3 elements we need to output (English chat message, English chat response, and French chat response).\n",
    "\n",
    "Combining multiple values is a common requirement -- e.g., in collecting values from a model ensemble.\n",
    "\n",
    "<img src='https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Serve/ensemble.png' width=900 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc6ea6e-9975-4340-8c52-eec459bbe2c7",
   "metadata": {
    "id": "3888b068-b6ab-4f3f-81c4-86c1dd7958e9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "def combine(user_input_en:str, chat_response_en: str, chat_response_fr:str)->str:\n",
    "    return chat_response_fr + '|' + user_input_en + '|' + chat_response_en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a8b7a5-254f-44fb-9c51-e0ddf857b842",
   "metadata": {},
   "source": [
    "The combine node here implemented here is a very simple deployment: it's built from a single function definition instead of a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91177d7b-d0b3-4957-9fa9-db3764b726ff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a26afc34-0c4a-49ab-b408-abba019d14a0",
    "outputId": "d5797540-1e37-406c-85b0-fdfb76c9326e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "translate_en_fr = Translate.bind(task='translation_en_to_fr', model='t5-small')\n",
    "translate_fr_en = Translate.bind(task='translation_fr_to_en', model='Helsinki-NLP/opus-mt-fr-en')\n",
    "chat = Chat.bind(model='facebook/blenderbot-400M-distill')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfc7760-2c4c-41ef-8ae2-d2fffae426e3",
   "metadata": {},
   "source": [
    "Event though the definitions of the `Translate` and `Chat` deployments have not changed, we call `.bind()` again to create new DAG nodes since we're composing a new DAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18cd306-312d-46a8-b47e-7c173085441d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a26afc34-0c4a-49ab-b408-abba019d14a0",
    "outputId": "d5797540-1e37-406c-85b0-fdfb76c9326e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "with InputNode() as inp:\n",
    "    user_input = inp[0]\n",
    "    history = inp[1]\n",
    "    user_input_en = translate_fr_en.get_response.bind(user_input)\n",
    "    chat_response_en = chat.get_response.bind(user_input_en, history)\n",
    "    chat_response_fr = translate_en_fr.get_response.bind(chat_response_en)\n",
    "\n",
    "# We route the user input, the English chat response, and the French chat response into the combine node\n",
    "output = combine.bind(user_input_en, chat_response_en, chat_response_fr)\n",
    "\n",
    "# and we serve the output of the combine node\n",
    "serve_dag = DAGDriver.bind(output)\n",
    "\n",
    "handle = serve.run(serve_dag, name='enhanced_linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ef32e8-5db3-49f7-840d-8ea5ddd60810",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "142fcb77-1366-42a8-bd8f-30a8a960db98",
    "outputId": "cc62a275-6b55-42da-bf68-a62e04b7c784",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray.get(handle.predict.remote('Mes amis sont cool mais ils mangent trop de glucides.', []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74dbba9-49a1-43e8-b3b7-d8402caab04e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.delete('enhanced_linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4e92a6-1bbd-4894-8914-afc21203f562",
   "metadata": {
    "id": "4946be16-bd7a-49a9-ae41-6b6a29643ed2"
   },
   "source": [
    "Using this pattern, we are getting everything back that we would need to offer a conversation service with the chatbot ... but only in French!\n",
    "\n",
    "## Adding Conditional Flow\n",
    "\n",
    "Our real chatbot is a bit more complex. It has a conditional flow where we invoke the translation service only when the user is *not* interacting in English.\n",
    "\n",
    "We can add the remaining elements of our service and the basic API changes will be fairly minimal. But there is one aspect that requires us to do a little bit of thinking and employ a new pattern.\n",
    "\n",
    "### Static Graphs and Conditional Control Flow\n",
    "\n",
    "The graph we define with the DAG API is static – it’s created ahead of time. \n",
    "\n",
    "In the first DAG demo, we were always invoking the same sequence of services, so the static character of the graph might not have been obvious… but now we’re focusing on it so you can see where things might get a bit more complicated.\n",
    "\n",
    "To implement branching flow control with the DAG API, we’ll use a special pattern so that the same graph always runs … but certain nodes (in our case, translator nodes) behave differently based on data they receive.\n",
    "\n",
    "<img src='https://technical-training-assets.s3.us-west-2.amazonaws.com/Ray_Serve/deployment_graph_complex.png' width=900 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde0e680-e836-4b2f-af1e-53897840e13f",
   "metadata": {
    "id": "0e793082-96ce-447c-9af7-1be949be9b21",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment(ray_actor_options={\"runtime_env\" : { \"pip\": [\"sentencepiece\"]} })\n",
    "class Translate:\n",
    "    def __init__(self, task: str, model: str):\n",
    "        self._task = task\n",
    "        self._model = model\n",
    "        self._pipeline = None\n",
    "    \n",
    "    def get_response(self, user_input:str, user_lang:str) -> str:\n",
    "        if (user_lang == 'en'):\n",
    "            return user_input # no-op\n",
    "        \n",
    "        if (self._pipeline is None):\n",
    "            self._pipeline = pipeline(task=self._task, model=self._model)\n",
    "            \n",
    "        outputs = self._pipeline(user_input)\n",
    "        response = outputs[0]['translation_text']\n",
    "        return response        \n",
    "        \n",
    "\n",
    "translate_en_fr = Translate.bind(task='translation_en_to_fr', model='t5-small')\n",
    "translate_fr_en = Translate.bind(task='translation_fr_to_en', model='Helsinki-NLP/opus-mt-fr-en')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9baa70-e8bc-4604-a7da-e94e1888ec57",
   "metadata": {},
   "source": [
    "The if-else control flow inside `get_response()` calls the transation logic only when the user is *not* using English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60afa575-7feb-4fe6-bcc0-b31cfab594ef",
   "metadata": {
    "id": "5f6bd63d-4958-4580-b72a-caeba1a1f578",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@serve.deployment(ray_actor_options={\"runtime_env\" : {\"pip\": [\"lingua-language-detector==1.3.2\"]}})\n",
    "class LangDetect:\n",
    "    def __init__(self):\n",
    "        self._detector = None\n",
    "        \n",
    "    def get_response(self, user_input: str) -> str:\n",
    "        from lingua import Language, LanguageDetectorBuilder\n",
    "        \n",
    "        if (self._detector is None):\n",
    "            languages = [Language.ENGLISH, Language.FRENCH]\n",
    "            self._detector = LanguageDetectorBuilder.from_languages(*languages).build()\n",
    "        \n",
    "        output = self._detector.detect_language_of(user_input)\n",
    "        if (output == Language.ENGLISH):\n",
    "            return 'en'\n",
    "        else:\n",
    "            return 'fr'\n",
    "        \n",
    "lang_detect = LangDetect.bind()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5498ecc6-35de-4217-a39e-3a387d3a072a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0105c9c-58e2-48c7-a4ff-e5d1a26632e6",
    "outputId": "2aeb1446-0e3e-463f-95e5-24fa9336f9cc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat = Chat.bind(model='facebook/blenderbot-400M-distill')\n",
    "\n",
    "with InputNode() as inp:\n",
    "    user_input = inp[0]\n",
    "    history = inp[1]\n",
    "    user_lang = lang_detect.get_response.bind(user_input)\n",
    "    user_input_en = translate_fr_en.get_response.bind(user_input, user_lang)\n",
    "    chat_response_en = chat.get_response.bind(user_input_en, history)\n",
    "    chat_response_fr = translate_en_fr.get_response.bind(chat_response_en, user_lang)\n",
    "    output = combine.bind(user_input_en, chat_response_en, chat_response_fr)\n",
    "    serve_dag = DAGDriver.bind(output)\n",
    "\n",
    "handle = serve.run(serve_dag, name='full_chatbot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e66733-844d-4b89-b82e-f2c52cc56b9c",
   "metadata": {},
   "source": [
    "In this code, the translation services are always part of the graph and participate in the data flow. So the graph is static, even though the translation behavior is dynamic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c353e36-3b7f-483d-9bd9-a1b9c13d86e7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "id": "5bf055b3-2c97-435d-ab36-d540b3fef963",
    "outputId": "1a7da7c7-fb2b-4bde-cd58-8142c430b83b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "message = 'Mes amis sont cool mais ils mangent trop de glucides.'\n",
    "history = []\n",
    "\n",
    "response = ray.get(handle.predict.remote(message, history))\n",
    "\n",
    "response.split('|')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9be5ee1-e0c4-46de-aeb9-d04b56ad819f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history += response.split('|')[1:]\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af74c6c2-18a6-45cd-b5b3-0a49bf694c25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "d7cb4815-093b-49bd-b6d4-98fdb1f9c038",
    "outputId": "ae2a3adc-dcc0-4613-bcbe-3945a415cfee",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray.get(handle.predict.remote('Truly bread is delightful', history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee14a284-54bf-4269-ae88-37378d5fa47a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "serve.delete('full_chatbot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28328dd7-51a1-4ab0-b974-c281364246ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
