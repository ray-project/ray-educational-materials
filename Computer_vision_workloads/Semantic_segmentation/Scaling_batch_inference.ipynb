{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcb9a269-765a-46ef-abb6-d60682d38b0d",
   "metadata": {},
   "source": [
    "# Scalable Batch Inference with Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b114120-a325-4597-bf7b-8310c9c8d776",
   "metadata": {},
   "source": [
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Generic/ray_logo.png\" width=\"20%\" loading=\"lazy\">\n",
    "\n",
    "## About this notebook\n",
    "\n",
    "### Is this module right for you?\n",
    "\n",
    "This module presents several approaches for scaling batch inference on Ray. Through hands-on practice with inference on a computer vision task, you will implement and compare different inference architectures to better understand Ray AIR and Ray Core.\n",
    "\n",
    "To get the most out of this notebook, the following scenarios may apply to you:\n",
    "\n",
    "* You observe performance bottlenecks when working on batch inference problems in computer vision projects.\n",
    "* You want to scale or increase throughput of existing batch inference pipelines.\n",
    "* You wish to explore different architectures for scaling batch inference with Ray AIR and Ray Core.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "For this notebook you should satisfy the following requirements:\n",
    "\n",
    "* Practical Python and machine learning experience.\n",
    "* Familiarity with batch inference in ML.\n",
    "* Familiarity with Ray and Ray AIR equivalent to completing these training modules:\n",
    "  * [Overview of Ray](https://github.com/ray-project/ray-educational-materials/blob/main/Introductory_modules/Overview_of_Ray.ipynb)\n",
    "  * [Introduction to Ray AIR](https://github.com/ray-project/ray-educational-materials/blob/main/Introductory_modules/Introduction_to_Ray_AIR.ipynb)\n",
    "  * [Ray Core](https://github.com/ray-project/ray-educational-materials/tree/main/Ray_Core)\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "* Understand common design patterns for distributed batch inference.\n",
    "* Implement scalable batch inference with Ray.\n",
    "* Extend each approach by tuning performance.\n",
    "* Compare scalable batch inference architectures on Ray to evaluate which is most relevant to your work.\n",
    "\n",
    "### What will you do?\n",
    "\n",
    "* Learn about three distributed batch inference design patterns with Ray.\n",
    "* Get to know the inference task.\n",
    "  * Semantic (image) segmentation using the SegFormer model.\n",
    "* Implement sequential inference.\n",
    "* Implement distributed inference patterns.\n",
    "  * Inference with Ray AIR Datasets and **BatchPredictor** abstractions.\n",
    "  * Inference with Ray Core, using key abstrations: Ray tasks and actors.\n",
    "* Compare approaches to identify situations best fit for each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a02709-0ed5-489a-b344-27005261eb72",
   "metadata": {},
   "source": [
    "## Part 1: Ray design patterns for scaling batch inference\n",
    "\n",
    "The ultimate goal for machine learning models is often to generate predictions on a set of unseen data. In this notebook, you focus on the inference stage of the ML workflow and explore different approaches to scaling it.\n",
    "\n",
    "Ray Core and Ray AIR provide APIs that allow you to perform batch inference at scale, processing millions of examples and offering various performance tuning options.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Scaling_inference/example_ml_workflow.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|An example of a machine learning workflow that starts with reading raw data and preprocessing it. These steps are followed by training and tuning that produce a trained model. This model is then used for inference, often on large datasets.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2450181f",
   "metadata": {},
   "source": [
    "### What is (batch) inference?\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong>Batch inference</strong> (also known as offline inference): is the process of generating predictions on a large set or \"batch\" of data.\n",
    "</div>\n",
    "\n",
    "Unlike *online inference* where predictions are generated as each observation is produced, batch inference generates predictions over a large number of input data when immediate response is not required or feasible. \n",
    "\n",
    "For example, batch inference is relevant when generating weekly product recommendations using historical customer data or sales forecasting using time-aggregated observations.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Scaling_inference/batch_inference.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Batch inference is the process of applying a trained model to a batch of data to generate predictions.|\n",
    "\n",
    "In a non-distributed setting, inference executes sequentially. The model processes incoming batches of data one at a time, limiting performance to a single machine or GPU. Below, you will learn about three approaches for distributing batch inference on Ray."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c500d89-bdae-41fd-9b12-264c0839af90",
   "metadata": {},
   "source": [
    "### Batch inference using Ray AIR BatchPredictor\n",
    "\n",
    "Ray AIR [BatchPredictor](https://docs.ray.io/en/latest/ray-air/predictors.html#batch-prediction) is a utility for large-scale, distributed batch inference. `BatchPredictor` has out-of-the-box features:\n",
    "\n",
    "* supports various predictors like [TorchPredictor](https://docs.ray.io/en/latest/ray-air/api/doc/ray.train.torch.TorchPredictor.html#ray.train.torch.TorchPredictor), [HuggingFacePredictor](https://docs.ray.io/en/latest/ray-air/api/doc/ray.train.huggingface.HuggingFacePredictor.html#ray.train.huggingface.HuggingFacePredictor) or [XGBoostPredictor](https://docs.ray.io/en/latest/ray-air/api/doc/ray.train.xgboost.XGBoostPredictor.html#ray.train.xgboost.XGBoostPredictor))\n",
    "* it handles framework-native batch conversions\n",
    "* it has options to resume operations from AIR checkpoint to prediction, selection / keep columns, etc.\n",
    "\n",
    "`BatchPredictor` takes in two components:\n",
    "\n",
    "* **`Checkpoint`**. A trained model, could be from training or tuning step.\n",
    "* **`Predictor`**. A class that loads models from `Checkpoint` to perform inference; supports framework-specific predictors (e.g. TorchPredictor and TensorflowPredictor).\n",
    "\n",
    "Once instantiated, BatchPredictor can call `predict()` on a Ray Dataset. [Ray Datasets](https://docs.ray.io/en/latest/data/dataset.html#datasets) are the standard way to load and exchange data in Ray AIR. Datasets load and preprocess data for parallel compute, internally handling operations like batching, pipelining, autoscaling the actor pool, and memory management.\n",
    "\n",
    "\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Scaling_inference/air_batchpredictor.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Ray Datasets parallelize data loading, preprocessing, and batching. Ray AIR `BatchPredictor` takes both `Checkpoint` and `Predictor` objects to call `predict()` on a Ray Dataset for distributed batch inference.|\n",
    "\n",
    "These high-level abstractions automate the challenging aspects of scaling batch inference in exchange for less direct control over the way Ray distributes.\n",
    "\n",
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Scaling_inference/code_batchpredictor.png\" width=\"70%\" loading=\"lazy\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccda42b-f826-459e-b226-59039d68b6e8",
   "metadata": {},
   "source": [
    "### Batch inference using Ray Core API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8becdd0-3f1e-4503-92b6-0e275e03ad75",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Stateless inference using Ray Tasks\n",
    "\n",
    "Part of the Ray Core primitives, [Ray Tasks](https://docs.ray.io/en/latest/ray-core/tasks.html#ray-remote-functions) offer an easy way to distribute inference across a compute cluster. Tasks are Python functions that execute remotely in the cluster, allowing multiple processes to work on different tasks concurrently (see: [Remote Procedure Call](https://en.wikipedia.org/wiki/Remote_procedure_call)).\n",
    "\n",
    "In this approach, tasks contain replicas of the trained model to compute predictions on input data. Since tasks do not store or modify any internal state, we say they are *stateless*. \n",
    "\n",
    "An example of a stateless function in deep learning is the [SGD optimizer](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) because it only updates weights (the output) based on the gradient of the loss function (the input, along with the current weights). No internal state about previously calculated gradients influences how future gradients are calculated.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Scaling_inference/task_inference.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|During stateless inference, each Ray Task loads the trained model and outputs predictions on assigned batches. This approach scales with the number of available CPUs and GPUs because each inference task is independent of the other concurrent jobs.|\n",
    "\n",
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Scaling_inference/code_task.png\" width=\"70%\" loading=\"lazy\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5687ea84-b693-4a97-8de4-26954a4c97c4",
   "metadata": {},
   "source": [
    "#### Stateful inference using Ray Actors\n",
    "\n",
    "In the previous approach, the trained model is loaded and discarded after each batch. This works great for smaller models, however, loading large, complex models into memory can be computationally expensive. In addition, you may want the ability to capture some persistent internal state.\n",
    "\n",
    "[Ray Actors](https://docs.ray.io/en/latest/ray-core/actors.html) are *stateful objects*, meaning they maintain an internal state. Other examples of stateful objects include Python classes and the [Adam optimizer](https://arxiv.org/abs/1412.6980) commonly used in deep learning. Due to this property, actors can run inference on multiple batches and avoid the overhead of reloading the model after each batch.\n",
    "\n",
    "Setting up stateful inference involves a few important steps:\n",
    "\n",
    "1. Create replicas of the trained model as Ray Actors.\n",
    "2. Feed data into these model replicas in parallel and retrieve predictions.\n",
    "3. Continue to manage idle actors and assign tasks until the entire inference job completes.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Scaling_inference/actor_inference.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Ray Actors can generate predictions on batches of data. Because each actor keeps track of an internal state, it can be reused for inference on multiple batches.|\n",
    "\n",
    "When using Ray Actors for stateful inference, it is important to implement *load balancing*, or appropriate distribution of work among workers to utilize resources efficiently. This process involves keeping track of in-flight tasks to assign new batches to available actors continuously until the entire process completes.\n",
    "\n",
    "Using actors directly offers more control over how tasks are assigned. However, you may opt to use the convenient [Ray ActorPool](https://docs.ray.io/en/latest/ray-core/package-ref.html#ray-util-actorpool) utility which handles load balancing (futures management) automatically. This abstraction wraps a list of actors and distributes the workload, allowing you to focus on the inference logic.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Scaling_inference/actor_pool.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|The ActorPool wraps around a list of `n` actors so you do not have to manage idle actors and manually distribute workloads.|\n",
    "\n",
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Scaling_inference/code_actor.png\" width=\"70%\" loading=\"lazy\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee761b0-7549-46ca-99bc-e3588a7a1deb",
   "metadata": {},
   "source": [
    "## Part 2: Batch inference example using computer vision transformers\n",
    "\n",
    "To demonstrate the three design patterns introduced in the previous section, you will apply each approach on a computer vision task: semantic segmentation.\n",
    "\n",
    "Semantic segmentation, similar to object detection, involves assigning labels to objects in a scene pixel-by-pixel. In this hands-on example, you will run batch inference on image data by using a pretrained model to generate predictions.\n",
    "\n",
    "### Data\n",
    "\n",
    "#### MIT ADE20K - scene parsing benchmark\n",
    "\n",
    "The [MIT ADE20K Dataset](http://sceneparsing.csail.mit.edu/) (also known as \"SceneParse150\") provides the largest open source dataset for scene parsing. It is often used as a standard for assessing semantic segmentation model performance due to its high-quality annontations. For this example, you will use the unlabeled test data to implement different batch inference architectures.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Scaling_inference/scene.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Unannotated scene image from MITADE20K on the left. Pixel-by-pixel predictions on the right. [*Date accessed: November 10, 2022*](https://github.com/CSAILVision/semantic-segmentation-pytorch)|\n",
    "\n",
    "Dataset highlights\n",
    "\n",
    "* 20k annotated, scene-centric training images\n",
    "* 3.3k unlabeled test images\n",
    "* 150 [semantic categories](https://docs.google.com/spreadsheets/d/1se8YEtb2detS7OuPE86fXGyD269pMycAWe2mtKUj2W8/edit?usp=sharing) (such as person, car, bed, sky, etc.)\n",
    "\n",
    "### Model\n",
    "\n",
    "#### SegFormer - transformer-based framework for semantic segmentation\n",
    "\n",
    "[SegFormer](https://arxiv.org/pdf/2105.15203.pdf) is an effective semantic segmentation method based on a *transformer* architecture. [Transformers](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) are a type of deep learning architecture that process sequential data via a series of self-attention layers and then transform them via a feedforward neural network.\n",
    "\n",
    "What sets SegFormer apart from previous transformer-based approaches are two key features:\n",
    "\n",
    "1. A hierarchically structured transformer encoder which does not depend on positional encoding that avoids interpolation when training and testing resolutions differ.\n",
    "2. A lightweight MLP layer that avoids complex decoders.\n",
    "\n",
    "You will use a pretrained SegFormer model finetuned on [MITADE20K](http://sceneparsing.csail.mit.edu/) to perform batch inference.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Scaling_inference/segformer_architecture.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|SegFormer architecture showcasing the hierarical transformer encoder and all-MLP decoder. [*Date accessed: November 10, 2022*](https://arxiv.org/pdf/2105.15203.pdf).|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c018d564-0e4a-453f-9f7a-74466c0273e8",
   "metadata": {},
   "source": [
    "## Part 3: Sequential batch inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a07b49f-ee7b-48c7-a4d0-bc4043ff24c9",
   "metadata": {},
   "source": [
    "In order to establish familiarity with this batch inference task, you will implement a basic approach with one worker that generates predictions on batches sequentially. To get set up, the semantic segmentation example requires the following steps:\n",
    "\n",
    "1. Load the pretrained [SegFormer](https://huggingface.co/nvidia/segformer-b0-finetuned-ade-512-512) model.\n",
    "2. Load the [feature extractor](https://huggingface.co/docs/transformers/v4.16.2/en/model_doc/segformer#transformers.SegformerFeatureExtractor) (preprocessor for scene data).\n",
    "3. Load [SceneParse150](https://huggingface.co/datasets/scene_parse_150) dataset.\n",
    "4. Run batch inference on images from the test set.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Scaling_inference/single_sequential_timeline.png\" width=\"90%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Timeline of sequential batch inference using a single worker. Tasks can vary in runtime due variations in complexity, data size, and more. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f83049",
   "metadata": {},
   "source": [
    "### Set up necessary imports and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb5419c-26af-4b09-8fd2-e885aaa60021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from PIL.JpegImagePlugin import JpegImageFile\n",
    "\n",
    "# Set the seed to a fixed value for reproducibility.\n",
    "torch.manual_seed(201)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245bf0ab",
   "metadata": {},
   "source": [
    "### Load the model components from the HuggingFace Hub\n",
    "\n",
    "From the [Hugging Face Hub](https://huggingface.co/docs/hub/index), retrieve the pretrained SegFormer model by specifying the model name and [label files](https://huggingface.co/datasets/huggingface/label-files/blob/main/ade20k-id2label.json) which map indices to semantic categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff44bdc",
   "metadata": {},
   "source": [
    "#### Load label mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e32feab-170b-4220-a263-bb678592d106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4346c5a0-36b4-46ce-bdc0-3102543d2dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label, label2id = get_labels()\n",
    "\n",
    "print(f\"Total number of labels: {len(id2label)}\")\n",
    "print(f\"Example labels: {list(id2label.values())[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9354716-8ac2-4e4f-97e1-e48ae174f0d9",
   "metadata": {},
   "source": [
    "The utility function `get_labels` fetches two dictionary mappings from [Hugging Face](https://huggingface.co/datasets/huggingface/label-files/blob/main/ade20k-id2label.json), `id2label` and `label2id`, which are used to convert between numerical and string labels for the 150 available [semantic categories](https://docs.google.com/spreadsheets/d/1se8YEtb2detS7OuPE86fXGyD269pMycAWe2mtKUj2W8/edit#gid=0) of objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d57314e",
   "metadata": {},
   "source": [
    "#### Load SegFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046f3990-3c6f-4ae4-9186-fa9fb1800f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SegformerForSemanticSegmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aec6f8-22bf-41f9-bd7b-fe54889d762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"nvidia/segformer-b0-finetuned-ade-512-512\"\n",
    "\n",
    "segformer = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    MODEL_NAME, id2label=id2label, label2id=label2id\n",
    ")\n",
    "\n",
    "print(f\"Number of model parameters: {segformer.num_parameters()/(10**6):.2f} M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f3ef57",
   "metadata": {},
   "source": [
    "The [Hugging Face Hub](https://huggingface.co/nvidia/segformer-b0-finetuned-ade-512-512) makes available many variations on SegFormer. Here, you specify a version finetuned on the MITADE20K (SceneParse150) dataset on images with a 512 x 512 resolution.\n",
    "\n",
    "Note: This \"b0\" model is the smallest, with [other options](https://huggingface.co/nvidia/segformer-b5-finetuned-ade-640-640) ranging up to and including \"b5\". Keep this in mind as something to experiment with when comparing different batch inference architectures later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41546abd-2f3d-4128-8176-19cd0604d6f4",
   "metadata": {},
   "source": [
    "#### Load the feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51be71f-1c3f-45fb-b9b2-af3393bfadc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SegformerFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f888ed08-cde2-4f4a-b05f-e467de4b7ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "segformer_feature_extractor = SegformerFeatureExtractor.from_pretrained(\n",
    "    MODEL_NAME, reduce_labels=True\n",
    ")\n",
    "segformer_feature_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9a23f3",
   "metadata": {},
   "source": [
    "[Feature extractors](https://huggingface.co/docs/transformers/main_classes/feature_extractor) preprocess input features (e.g. image data) by normalizing, resizing, padding, and converting raw images into the shape expected by SegFormer.\n",
    "\n",
    "The [`reduce_labels`](https://huggingface.co/docs/transformers/model_doc/segformer#segformer) flag ensures that the background of an image (anything that is not explicitly an object) isn't included when computing loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3109152b-3572-40f9-8fc6-b04ecce59896",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d58d4b-d13c-4c34-8e80-194c8f6bdae9",
   "metadata": {},
   "source": [
    "#### Set up necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e93db0-6218-4460-8f8c-14703a57f29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from utils import convert_image_to_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24ed45d-8700-40de-99ac-2bd4c5999974",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_DATA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25fc198-d3f4-47a0-9c16-9054a342c4f7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "  <strong>SMALL_DATA</strong>: a flag to download a subset (160 images) of the available test data. Defaults to True. Set to False (recommended) to work with the full test data (3352 images).\n",
    "</div>\n",
    "\n",
    "If you set `SMALL_DATA` to `False`, expect it to take some time (depending on your connection download speed) because you are downloading all test images to your local machine or cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3066409",
   "metadata": {},
   "source": [
    "#### Load SceneParse150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70f31fe-e8a9-41ea-857c-e2e2b36503e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"scene_parse_150\"\n",
    "\n",
    "# Load data from the Hugging Face datasets repository.\n",
    "if SMALL_DATA:\n",
    "    train_dataset = load_dataset(DATASET_NAME, split=\"train[:10]\")\n",
    "    test_dataset = load_dataset(DATASET_NAME, split=\"test[:160]\")\n",
    "else:\n",
    "    train_dataset = load_dataset(DATASET_NAME, split=\"train[:10]\")\n",
    "    test_dataset = load_dataset(DATASET_NAME, split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a16ce0",
   "metadata": {},
   "source": [
    "The two datasets serve different purposes:\n",
    "\n",
    "* **`train_dataset`**  \n",
    "    * Retrieve a small sample of images for visualization purposes only. Training samples include ground-truth, annotated image regions. Full training dataset contains 20210 images.\n",
    "* **`test_dataset`**  \n",
    "    * Used for batch inference purposes. Test samples do not contain ground-truth labels. Full test dataset contains 3352 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf19f5d-52cf-4d15-a958-e143bcf711c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadba2c1-eb5e-42f9-8de7-bb3311a04786",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_dataset.map(convert_image_to_rgb)\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6e66d8-2587-43c9-91bb-cdfc6f22b33d",
   "metadata": {},
   "source": [
    "Each sample contains three components:\n",
    "* **`image`** \n",
    "    * The PIL image.\n",
    "* **`annotation`**  \n",
    "    * Human annotations of image regions (annotation mask is `None` in testing set).\n",
    "* **`category`**  \n",
    "    * Category of the scene generally (e.g. driveway, voting booth, dairy_outdoor)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2a63ad-3bfd-453c-827e-6a0f664ac310",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Display example images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c22f28-f63c-42e8-ab43-1684aa692d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import display_example_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1054dfa-90a6-4569-bc0f-cb059f4b4893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try running this multiple times!\n",
    "display_example_images(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7243ec-d207-4b33-89ca-b80309c19c06",
   "metadata": {},
   "source": [
    "### Run sequential inference on 1 batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eccd1dc",
   "metadata": {},
   "source": [
    "#### Define inference logic\n",
    "\n",
    "This `predict` function forms the basis for the inference step, and you will reuse variations of this function multiple times throughout each approach for batch inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba82c68-f3cd-46ae-8b23-6fee43ccb3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    model: SegformerForSemanticSegmentation,\n",
    "    feature_extractor: SegformerFeatureExtractor,\n",
    "    images: list[JpegImageFile],\n",
    ") -> list[np.array]:\n",
    "    # Set the device on which PyTorch will run.\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)  # Move the model to specified device.\n",
    "    model.eval()  # Set the model in evaluation mode on test data.\n",
    "\n",
    "    # The feature extractor processes raw images.\n",
    "    inputs = feature_extractor(images=images, return_tensors=\"pt\")\n",
    "\n",
    "    # The model is applied to input images in the inference step.\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=inputs.pixel_values.to(device))\n",
    "\n",
    "    # Post-process the output for display.\n",
    "    image_sizes = [image.size[::-1] for image in images]\n",
    "    segmentation_maps_postprocessed = (\n",
    "        feature_extractor.post_process_semantic_segmentation(\n",
    "            outputs=outputs, target_sizes=image_sizes\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Return list of segmentation maps detached from the computation graph.\n",
    "    return [j.detach().cpu().numpy() for j in segmentation_maps_postprocessed]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336d6a94-8399-4616-8221-e1f5c357baf2",
   "metadata": {},
   "source": [
    "#### Prepare 1 batch of 16 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70115952-fdb0-40d3-87d0-a7903502a289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_image_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930f4caf-8964-4170-8bde-b126a5a0ae57",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "# Get BATCH_SIZE randomly shuffled image IDs from the test dataset.\n",
    "image_indices = get_image_indices(dataset=test_dataset, n=BATCH_SIZE)\n",
    "image_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3302db4-6b93-4ef6-96b2-2bd316023044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of images by extracting images from random indices sampled from the test data.\n",
    "batch = [test_dataset[i][\"image\"] for i in image_indices]\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d33e48",
   "metadata": {},
   "source": [
    "#### Run batch inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b9f0f1-be22-4910-a5a1-e6d861cf3a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_maps = predict(\n",
    "    model=segformer,\n",
    "    feature_extractor=segformer_feature_extractor,\n",
    "    images=batch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b7394d-eef4-498e-bee4-38214df85406",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_maps[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e378ad0",
   "metadata": {},
   "source": [
    "Performing batch inference outputs a list of segmentation maps. Each element in the segmentation map array represents the [semantic category](https://docs.google.com/spreadsheets/d/1se8YEtb2detS7OuPE86fXGyD269pMycAWe2mtKUj2W8/edit#gid=0) of the corresponding pixel in the input image.\n",
    "\n",
    "Together, you can visualize these predicted segmentation maps by overlaying them onto the original image to see defined regions of objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee752e3a-d615-4773-a932-83018aee119e",
   "metadata": {},
   "source": [
    "#### Visualize example predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7d6a72-ed3e-4548-9b04-8490ee25f322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import visualize_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99ef21c-879e-46b3-b97f-1c00efcae05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(image=batch[0], segmentation_maps=segmentation_maps[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52be8531-ad94-4fde-b13d-a864f3020a37",
   "metadata": {},
   "source": [
    "### Run sequential inference on 10 batches\n",
    "\n",
    "Next, you will test the scalability and performance of the sequential batch inference approach by increasing the number of batches from 1 to 10. This will allow you to observe and verify that this approach can limit performance when scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93faafea-ddec-4148-a60a-a89d22b12aa8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Prepare batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e17bf3-9c04-47eb-a7c5-604a0ffbb3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "N_BATCHES = 10\n",
    "\n",
    "# Get BATCH_SIZE * N_BATCHES randomly shuffled image IDs from the test dataset.\n",
    "image_indices = get_image_indices(dataset=test_dataset, n=BATCH_SIZE * N_BATCHES)\n",
    "\n",
    "# Split indices into N_BATCHES\n",
    "image_indices_grouped = np.split(np.asarray(image_indices), N_BATCHES)\n",
    "image_indices_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f6004f-1cd5-46c9-89ad-708bbfe67312",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = []\n",
    "\n",
    "# Create a list of images for each batch of indices sampled from the test dataset.\n",
    "for image_idx in image_indices_grouped:\n",
    "    batch = [test_dataset[int(i)][\"image\"] for i in image_idx]\n",
    "    batches.append(batch)\n",
    "\n",
    "batches[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7907202d-bc99-40d0-aa15-e8200a206426",
   "metadata": {},
   "source": [
    "#### Run batch inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ca1763-8d9d-46aa-90c0-29b4b2be3c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9154c7c3-8fae-44f9-9e4c-bca3b62b7328",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in batches:\n",
    "    segmentation_maps = predict(\n",
    "        model=segformer,\n",
    "        feature_extractor=segformer_feature_extractor,\n",
    "        images=batch,\n",
    "    )\n",
    "    predictions.append(segmentation_maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24d6603-50ee-4ed7-b17e-411fb5ca92b1",
   "metadata": {},
   "source": [
    "Notice that increasing the number of batches by 10 leads to approximately a 10x increase in runtime/ This is the expected result for a sequential approach, which scales linearly with the number of batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d425f7-d23c-41d1-adee-3cc9509e190f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the resulting segmentation maps array.\n",
    "predictions[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e69cad-d947-42d0-9cad-8f0a2d4941c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Summary: Sequential batch inference\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Scaling_inference/single_sequential_timeline.png\" width=\"90%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Timeline of sequential batch inference using a single worker. Tasks can vary in runtime due variations in complexity, data size, and more. |\n",
    "\n",
    "#### Key concepts\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong>Batch inference</strong> (also known as offline inference): is the process of generating predictions on a large set or \"batch\" of data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7289f04c-337c-47e1-a415-a5c5d80b61cc",
   "metadata": {},
   "source": [
    "## Part 4: Distributed batch inference with Ray AIR\n",
    "\n",
    "These high-level APIs automate the challenging aspects of parallelizing and distributing batch inference tasks, allowing you to focus on the inference logic. \n",
    "\n",
    "There are four main abstractions that work together to optimize this process:\n",
    "\n",
    "* [**`Datasets`**](https://docs.ray.io/en/latest/data/dataset.html)  \n",
    "    * These are used to parallelize data loading, preprocessing, and exchanging data in Ray AIR.\n",
    "* [**`Checkpoint`**](https://docs.ray.io/en/latest/tune/tutorials/tune-checkpoints.html)  \n",
    "    * `Checkpoint` objects represent saved models created during training or tuning and provide a common interface for restoring the model's state for tasks such as inference.\n",
    "* [**`Predictor`**](https://docs.ray.io/en/latest/ray-air/predictors.html)  \n",
    "    * Ray AIR `Predictors` are a class that load models from `Checkpoint` to perform inference and can be used by `BatchPredictor` to do large-scale inference.\n",
    "* [**`BatchPredictor`**](https://docs.ray.io/en/latest/ray-air/predictors.html#batch-prediction)  \n",
    "    * Ray AIR `BatchPredictor` utility takes in a `Checkpoint` and a `Predictor` class and executes large-scale distributed batch prediction on a Ray Dataset when calling `predict()`.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Scaling_inference/air_batchpredictor.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Ray Datasets parallelize data loading, preprocessing, and batching. Ray AIR `BatchPredictor` takes both `Checkpoint` and `Predictor` objects to call `predict()` on a Ray Dataset for distributed batch inference.|\n",
    "\n",
    "Ray handles operations such as batching, pipelining, actor pool autoscaling, and memory management internally, so you can benefit from the scalability and ease of use of Ray AIR without needing to worry about the details of task distribution. Using these abstractions does come with some trade-offs, as you have less control over how Ray distributes the workload."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665bc8b6-878e-4fde-9054-2891acf83c1b",
   "metadata": {},
   "source": [
    "### Initialize Ray runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce928af7-2037-4b65-9dd1-6d67b5335e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c97168",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d050e876-1b42-41ee-b3f2-d0caa8a31fbc",
   "metadata": {},
   "source": [
    "### Create a Ray Dataset with 160 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ccdeb8-4c31-4752-8f16-9f13e3cf3366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get BATCH_SIZE * N_BATCHES randomly shuffled image IDs from the test dataset.\n",
    "image_indices = get_image_indices(dataset=test_dataset, n=BATCH_SIZE * N_BATCHES)\n",
    "\n",
    "# Create a list of images for the indices sampled from the test dataset.\n",
    "data = [test_dataset[i][\"image\"] for i in image_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1f4066-7d32-4cb6-813f-9a880263b2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Ray Dataset from the list of images to use in Ray AIR.\n",
    "dataset = ray.data.from_items(data)\n",
    "dataset.show(limit=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ef6e96-f29e-4db9-be57-0aee104a34d9",
   "metadata": {},
   "source": [
    "### Define a custom Predictor for image data\n",
    "\n",
    "`BatchPredictor` takes in a `Checkpoint` (which will be constructed from the SegFormer model and feature extractor) and a `Predictor`. Ray AIR supports multiple framework-specific [`Predictors`](https://docs.ray.io/en/latest/ray-air/package-ref.html#predictor) such as TorchPredictor and TensorflowPredictor while also allowing for the ability to implement a [custom](https://docs.ray.io/en/latest/ray-air/predictors.html#developer-guide-implementing-your-own-predictor) one. \n",
    "\n",
    "Here, you will implement a custom `SemanticSegmentationPredictor`, with the same replicas and core `predict()` logic as before, but with some modifications to fit the `BatchPredictor` pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ae3c1e-31f6-4388-86a8-10ba84860f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.air import Checkpoint\n",
    "from ray.train.predictor import Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3024c0-f651-44f4-93f6-8964918b3bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSegmentationPredictor(Predictor):\n",
    "    # The constructor method initializes the class to load/cache the model and feature extractor.\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: SegformerForSemanticSegmentation,\n",
    "        feature_extractor: SegformerFeatureExtractor,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    # This is the same logic as the `predict()` function defined in Part 3,\n",
    "    # only with pandas DataFrames as inputs and outputs.\n",
    "    def _predict_pandas(self, batch: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Set the device on which PyTorch will run.\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "        # The feature extractor processes raw images.\n",
    "        batch = [batch[\"value\"][0]]\n",
    "        inputs = self.feature_extractor(images=batch, return_tensors=\"pt\")\n",
    "\n",
    "        # The model is applied to input images in the inference step.\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(pixel_values=inputs.pixel_values.to(device))\n",
    "\n",
    "        # Post-process the output for display.\n",
    "        image_sizes = [image.size[::-1] for image in batch]\n",
    "        segmentation_maps_postprocessed = (\n",
    "            self.feature_extractor.post_process_semantic_segmentation(\n",
    "                outputs=outputs, target_sizes=image_sizes\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Post-process the list of segmentation maps into a pandas DataFrame\n",
    "        df = pd.DataFrame(columns=[\"segmentation_maps\"])\n",
    "        df.loc[0, \"segmentation_maps\"] = segmentation_maps_postprocessed\n",
    "\n",
    "        return df\n",
    "\n",
    "    # Creates an instance of SemanticSegmentationPredictor using the model and\n",
    "    # feature extractor contained in the Checkpoint.\n",
    "    @classmethod\n",
    "    def from_checkpoint(\n",
    "        self, checkpoint: Checkpoint, **kwargs\n",
    "    ) -> \"SemanticSegmentationPredictor\":\n",
    "        checkpoint_data = checkpoint.to_dict()\n",
    "        return SemanticSegmentationPredictor(\n",
    "            model=checkpoint_data[\"model\"],\n",
    "            feature_extractor=checkpoint_data[\"feature_extractor\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0790eeb-3207-4f8b-b520-e3751f93cf2b",
   "metadata": {},
   "source": [
    "### Create a BatchPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b73b9a-b04b-4ded-8322-fbc4fa2620a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.batch_predictor import BatchPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3118f64",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Construct a BatchPredictor using the SegFormer model and feature extractor along with an instance\n",
    "# of the custom SemanticSegmentationPredictor class.\n",
    "batch_predictor = BatchPredictor(\n",
    "    checkpoint=Checkpoint.from_dict(\n",
    "        {\"model\": segformer, \"feature_extractor\": segformer_feature_extractor}\n",
    "    ),\n",
    "    predictor_cls=SemanticSegmentationPredictor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0509bf8-d124-4a6c-9720-82bf01b03faa",
   "metadata": {},
   "source": [
    "### Run parallel batch inference on a Ray Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74717154-8664-46b6-b54e-ec6d687d644f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dataset = batch_predictor.predict(data=dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01247c69-c2f4-4563-a8d2-71b4e5b7b364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the resulting segmentation maps in this DataFrame.\n",
    "predictions_dataset.take(limit=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425505c4",
   "metadata": {},
   "source": [
    "**Coding Exercise**\n",
    "\n",
    "In our example, we used a custom `Predictor`, but Ray AIR's BatchPredictor offers support for a number of framework specific predictors. \n",
    "\n",
    "Refer to this [user guide](https://docs.ray.io/en/latest/ray-air/predictors.html#developer-guide-implementing-your-own-predictor) for assistance. Try to implement the same inferencing logic, but this time, use a [HuggingFacePredictor](https://docs.ray.io/en/master/train/api.html?highlight=huggingfacepredictor#ray.train.huggingface.HuggingFacePredictor.predict) instead.\n",
    "\n",
    "Hint: HuggingFace models expect a specific [configuration](https://huggingface.co/docs/transformers/main_classes/configuration) for loading models as checkpoints. You will want to provide the model and feature extractor from HuggingFace to create a Checkpoint to ensure that you include all the required files. For example:\n",
    "\n",
    "```python\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    huggingface_checkpoint = HuggingFaceCheckpoint.from_model(\n",
    "        model=segformer, path=tmpdir\n",
    "    )\n",
    "    predictor = BatchPredictor.from_checkpoint(\n",
    "        checkpoint=huggingface_checkpoint,\n",
    "        predictor_cls=HuggingFacePredictor,\n",
    "        feature_extractor=segformer_feature_extractor,  # passed to HF pipeline\n",
    "        task=\"image-segmentation\",  # passed to HF pipeline\n",
    "        device=-1,\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfc8a4e-a38e-4b7e-8c7c-d3fd6d2fc2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94188018-aa30-4860-9355-2b51bc4d0e00",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8a82cf-6e7d-48be-b708-6b6267401d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAMPLE IMPLEMENTATION ###\n",
    "\n",
    "import tempfile\n",
    "from ray.train.huggingface import HuggingFaceCheckpoint, HuggingFacePredictor\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    huggingface_checkpoint = HuggingFaceCheckpoint.from_model(\n",
    "        model=segformer, path=tmpdir\n",
    "    )\n",
    "    predictor = BatchPredictor.from_checkpoint(\n",
    "        checkpoint=huggingface_checkpoint,\n",
    "        predictor_cls=HuggingFacePredictor,\n",
    "        feature_extractor=segformer_feature_extractor,  # passed to HF pipeline\n",
    "        task=\"image-segmentation\",  # passed to HF pipeline\n",
    "        device=-1,\n",
    "    )\n",
    "\n",
    "    predictions_dataset = predictor.predict(data=dataset, batch_size=1)\n",
    "\n",
    "predictions_dataset.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf1a41b-0e05-427f-8cf2-ec393484bf16",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Summary: Distributed batch inference with Ray AIR\n",
    "\n",
    "#### Key API elements\n",
    "\n",
    "* [**`Datasets`**](https://docs.ray.io/en/latest/data/dataset.html)  \n",
    "    * These are used to parallelize data loading, preprocessing, and exchanging data in Ray AIR.\n",
    "* [**`Checkpoint`**](https://docs.ray.io/en/latest/tune/tutorials/tune-checkpoints.html)  \n",
    "    * `Checkpoint` objects represent saved models created during training or tuning and provide a common interface for restoring the model's state for tasks such as inference.\n",
    "* [**`Predictor`**](https://docs.ray.io/en/latest/ray-air/predictors.html)  \n",
    "    * Ray AIR `Predictors` are a class that load models from `Checkpoint` to perform inference and can be used by `BatchPredictor` to do large-scale inference.\n",
    "* [**`BatchPredictor`**](https://docs.ray.io/en/latest/ray-air/predictors.html#batch-prediction)  \n",
    "    * Ray AIR `BatchPredictor` utility takes in a `Checkpoint` and a `Predictor` class and executes large-scale distributed batch prediction on a Ray Dataset when calling `predict()`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c36469-6528-4b58-9d9d-9059737beeb6",
   "metadata": {},
   "source": [
    "## Part 5: Distributed batch inference with Ray Core API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cba2f68-c762-439e-850b-3af83ba1eaae",
   "metadata": {},
   "source": [
    "### Batch inference with Ray Tasks\n",
    "\n",
    "Starting with stateless inference with Ray Tasks, you will load the replicas of the SegFormer model and feature extractor onto tasks which will run inference on different batches concurrently. Because these tasks do not store or modify any internal state, we refer to them as stateless tasks.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Scaling_inference/task_inference.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|In the stateless inference approach, each Ray Task loads the trained model and generates predictions for the assigned batches. This method scales well with the number of available CPUs and GPUs because each inference task can be executed concurrently and independently of the other tasks.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc24d09d-3b63-4307-a47e-1a72b2e33c9a",
   "metadata": {},
   "source": [
    "#### Put the model and feature extractor in the object store\n",
    "\n",
    "When using Ray, you can pass objects as arguments to remote functions. Ray will automatically store these objects in the local object store (on the worker node where the function is running) using the [`ray.put()`](https://docs.ray.io/en/latest/ray-core/package-ref.html#ray-put) function. This makes the objects available to all local tasks. However, if the objects are large, this can be inefficient as the objects will need to be copied every time they are passed to a remote function.\n",
    "\n",
    "To improve performance, you can explicitly store both the model and feature extractor in the object store by using `ray.put()`. This avoids the need to create multiple copies of the objects.\n",
    "\n",
    "It is important to note that if you have multiple worker nodes in your cluster, the objects will need to be copied in memory when they are used on a worker node different from where they are stored. Zero copy is not guaranteed in this case.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/object_store.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Diagram of workers in worker nodes using `ray.put()` to place objects and using `ray.get()` to retrieve them from each node's object store.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abbe8d8-f00d-4ba8-a81f-aa0a5f480854",
   "metadata": {},
   "outputs": [],
   "source": [
    "segformer_ref = ray.put(segformer)\n",
    "segformer_feature_extractor_ref = ray.put(segformer_feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffc8a08-5629-42ae-94a0-1bdafad47c74",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "  <strong>Tip</strong>\n",
    "\n",
    "  Passing the same large argument (model), by value repeatedly <a href=\"https://docs.ray.io/en/latest/ray-core/patterns/pass-large-arg-by-value.html\">harms performance and can cause Out-of-disk for the driver node</a>.\n",
    "  \n",
    "  Use the **object store** and **ray.put()** to pass by reference instead (for example, model_ref instead of model).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946db436-ac8b-4ac9-8dca-e557dbd126fb",
   "metadata": {},
   "source": [
    "#### Define remote function for inference\n",
    "\n",
    "One way to parallelize predictions in a stateless manner (similar to using lambdas) is to use Ray tasks. Each time a Ray task is called, it loads the trained model from the local object store in order to perform inference. \n",
    "\n",
    "This approach allows the prediction task to be stateless, but it incurs the overhead of loading the model each time it is called. This may not be a significant issue for small models, but larger models may experience bottlenecks when loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797b9e38",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Use decorator to designate this as a remote function.\n",
    "@ray.remote\n",
    "def inference_task(\n",
    "    model: SegformerForSemanticSegmentation,\n",
    "    feature_extractor: SegformerFeatureExtractor,\n",
    "    images: list[JpegImageFile],\n",
    ") -> list[np.array]:\n",
    "    # The `predict` function is the same one defined earlier in Part 3.\n",
    "    return predict(\n",
    "        model=model,\n",
    "        feature_extractor=feature_extractor,\n",
    "        images=images,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e695c89f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "  <strong>Tip</strong>: Batches should be large enough to avoid the anti-pattern of having  <a href=\"https://docs.ray.io/en/latest/ray-core/patterns/too-fine-grained-tasks.html\"> tasks which are too fine-grained</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512a2a90-83bf-4b26-a8e4-9f94e185b6fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Prepare batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adfc86e-176e-41fa-97dd-c1cf681334f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get BATCH_SIZE * N_BATCHES randomly shuffled image IDs from the test dataset.\n",
    "image_indices = get_image_indices(dataset=test_dataset, n=BATCH_SIZE * N_BATCHES)\n",
    "\n",
    "# Split indices into N_BATCHES\n",
    "image_indices_grouped = np.split(np.asarray(image_indices), N_BATCHES)\n",
    "\n",
    "batches = []\n",
    "\n",
    "# Create a list of images for each batch of indices sampled from the test dataset.\n",
    "for image_idx in image_indices_grouped:\n",
    "    batch = [test_dataset[int(i)][\"image\"] for i in image_idx]\n",
    "    batches.append(batch)\n",
    "\n",
    "batches[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c403178-5b95-4849-bb9d-fe453c88f0f5",
   "metadata": {},
   "source": [
    "#### Run parallel inference on 10 batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132c9494-40f9-4ddc-904e-f1e12a5e0647",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_refs = []\n",
    "predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc90802-72b2-4ac1-b61d-60b32cdbad39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch all prediction tasks.\n",
    "for batch in batches:\n",
    "    # Launch a prediction task by passing model reference, feature extractor\n",
    "    # reference, and batch of images.\n",
    "    task_ref = inference_task.remote(\n",
    "        model=segformer_ref,\n",
    "        feature_extractor=segformer_feature_extractor_ref,\n",
    "        images=batch,\n",
    "    )\n",
    "    # Collect all object references to batches.\n",
    "    prediction_refs.append(task_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5366e165-3aec-4e91-9693-53bbef29c2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve results.\n",
    "# Note: This is a synchronous/blocking operation which waits for all processes to complete\n",
    "# before returning.\n",
    "predictions = ray.get(prediction_refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d4d685-315f-4bc9-8d25-e3422e12054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the resulting segmentation maps array.\n",
    "predictions[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fadeb4b",
   "metadata": {},
   "source": [
    "**Coding Exercise**\n",
    "\n",
    "You have seen how the sequential version and stateless inference using Ray Tasks performs on 10 batches of 16 images each. Try scaling the number of batches as well as the number of images per batch to see the effect on performance.\n",
    "\n",
    "Hint: `BATCH_SIZE` and `N_BATCHES` is set in the Part 3 under \"Prepare batches\"\n",
    "\n",
    "Note: In order to perform inference on more than 160 images, you need to set the `SMALL_DATA` flag to `False` to download the complete test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711cf20f-4cb2-4e16-ad66-bd273352c9d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Summary: Distributed, stateless batch inference with Ray Tasks\n",
    "\n",
    "##### Key concepts\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong>Object store</strong>: Ray's distributed shared-memory store that makes remote objects available anywhere in a Ray cluster.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong>Stateless inference</strong>: Inference that depends only on an inputted trained model and does not preserve state once predictions are generated.\n",
    "</div>\n",
    "\n",
    "##### Key API elements\n",
    "\n",
    "* **`ray.init()`**  \n",
    "Start Ray runtime and connect to the Ray cluster.\n",
    "\n",
    "* **`@ray.remote`**  \n",
    "Decorator that specifies a Python function or class to be executed as a task (remote function) or actor (remote class) in a different process.\n",
    "\n",
    "* **`.remote`**  \n",
    "Postfix to the remote functions and classes; remote operations are asynchronous.\n",
    "\n",
    "* **`ray.put()`**  \n",
    "Put an object in the in-memory object store; returns an object reference used to pass the object to any remote function or method call.\n",
    "\n",
    "* **`ray.get()`**  \n",
    "Get a remote object(s) from the object store by specifying the object reference(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e2f9e2-945b-4b08-95ec-74b09b8b07d5",
   "metadata": {},
   "source": [
    "### Batch inference with Ray Actors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff1bb7d-b8f9-4a2d-89d2-01446e114525",
   "metadata": {},
   "source": [
    "Moving from stateless to stateful inference, Ray Actors offer the advantage of holding some mutable internal state which allows the actor to fetch the model once and reuse it for all tasks assigned to the actor.\n",
    "\n",
    "To set up stateful inference using Ray Actors, you will need to follow a few important steps:\n",
    "\n",
    "1. Create replicas of your trained model as Ray Actors, which can hold mutable internal state and avoid the need to reload large models for each inference job.\n",
    "2. Feed data into these model replicas in parallel and retrieve predictions.\n",
    "\n",
    "You can either manually assign tasks to actors (more control) or use the ActorPool utility (more convenient), which automates load balancing for you. If you choose to assign actors manually, you will need to continually manage idle actors and assign tasks until the entire inference job is completed.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Scaling_inference/actor_inference.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Ray Actors can generate predictions on batches of data. Because each actor keeps track of an internal state, it can be reused for inference on multiple batches.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70563f89-c95c-4ee8-8374-0288d918d5fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Define remote class for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4030c8a1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Specify this class as a Ray Actor.\n",
    "@ray.remote\n",
    "class PredictionActor:\n",
    "    # An interface (i.e. constructor) to load/cache the model and feature extractor.\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: SegformerForSemanticSegmentation,\n",
    "        feature_extractor: SegformerFeatureExtractor,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    # This is the same logic as the `predict()` function defined in Part 3.\n",
    "    def predict(self, images: list[JpegImageFile]) -> list[np.array]:\n",
    "        # Set the device on which PyTorch will run.\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "        # The feature extractor processes the raw images.\n",
    "        inputs = self.feature_extractor(images=images, return_tensors=\"pt\")\n",
    "\n",
    "        # The model is applied to input images in the inference step.\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(pixel_values=inputs.pixel_values.to(device))\n",
    "\n",
    "        # Post-process the output for display.\n",
    "        image_sizes = [image.size[::-1] for image in images]\n",
    "        segmentation_maps_postprocessed = (\n",
    "            self.feature_extractor.post_process_semantic_segmentation(\n",
    "                outputs=outputs, target_sizes=image_sizes\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Return list of segmentation maps detached from the computation graph.\n",
    "        return [j.detach().cpu().numpy() for j in segmentation_maps_postprocessed]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd911b90-2006-4fc9-bbbb-90e92ef79ee3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create a list of Ray Actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f910cd41",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "N_ACTORS = 2\n",
    "\n",
    "# Create a list of actors with N_ACTORS instances of `PredictionActor`.\n",
    "actors = [\n",
    "    PredictionActor.remote(\n",
    "        model=segformer_ref, feature_extractor=segformer_feature_extractor_ref\n",
    "    )\n",
    "    for _ in range(N_ACTORS)\n",
    "]\n",
    "\n",
    "actors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b915bfba-7baf-4761-a99d-e6e36dfbfa0f",
   "metadata": {},
   "source": [
    "Note: `N_ACTORS` is initally set to 2 here, which hinders performance. Ideally, you want to set the number of actors to be proportional to the amount of resources you have available, such as number of CPUs and/or GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabc6183-1d5c-4618-bc20-ee97365f7de5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Prepare batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f2265a-0907-4a42-8755-52515a53a66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get BATCH_SIZE * N_BATCHES randomly shuffled image IDs from the test dataset.\n",
    "image_indices = get_image_indices(dataset=test_dataset, n=BATCH_SIZE * N_BATCHES)\n",
    "\n",
    "# Split indices into N_BATCHES\n",
    "image_indices_grouped = np.split(np.asarray(image_indices), N_BATCHES)\n",
    "\n",
    "batches = []\n",
    "\n",
    "# Create a list of images for each batch of indices sampled from the test dataset.\n",
    "for image_idx in image_indices_grouped:\n",
    "    batch = [test_dataset[int(i)][\"image\"] for i in image_idx]\n",
    "    batches.append(batch)\n",
    "\n",
    "batches[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d39dc3-a2ac-45a6-aab6-6a26147dd9b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Run parallel inference on 10 batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a80f96-3561-4435-8661-1b989c22692e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_results_postprocessing(\n",
    "    predictions: list[list[np.array]], segmentation_maps: list[np.array]\n",
    ") -> list[list[np.array]]:\n",
    "    predictions.append(segmentation_maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26d89d1",
   "metadata": {},
   "source": [
    "`prediction_results_postprocessing` is simple function in this tutorial and exists to abstract away the final processing step. In practice it will likely be much more complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa802a4b-24cc-4716-bb94-a033b0394b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []  # A list of final predictions.\n",
    "future_to_actor_mapping = (\n",
    "    {}\n",
    ")  # A mapping of object references to the actor that promised them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbce1966-c4be-406e-8df1-4ff1650b531b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy to avoid modifying the original list of actors.\n",
    "idle_actors = actors.copy()\n",
    "\n",
    "while batches:\n",
    "    # Assign batches to available actors.\n",
    "    if idle_actors:\n",
    "        actor = idle_actors.pop()\n",
    "        batch = batches.pop()\n",
    "        future = actor.predict.remote(images=batch)\n",
    "        # Map the future to the actors executing prediction.\n",
    "        future_to_actor_mapping[future] = actor\n",
    "\n",
    "    # Retrieve the completed tasks and process them.\n",
    "    else:\n",
    "        # Retrieve the first future to return.\n",
    "        [ready], _ = ray.wait(list(future_to_actor_mapping.keys()), num_returns=1)\n",
    "\n",
    "        # Get the actor with the completed task and add back to idle list.\n",
    "        actor = future_to_actor_mapping.pop(ready)\n",
    "        idle_actors.append(actor)\n",
    "\n",
    "        # Post-processing on on result using ray.get() to retrieve result from reference.\n",
    "        prediction_results_postprocessing(\n",
    "            predictions=predictions, segmentation_maps=ray.get(ready)\n",
    "        )\n",
    "\n",
    "# Process any leftover results at the end.\n",
    "for future in future_to_actor_mapping.keys():\n",
    "    prediction_results_postprocessing(\n",
    "        predictions=predictions, segmentation_maps=ray.get(future)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462f5b7b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "  <strong>Tip</strong>: <a href=\"https://docs.ray.io/en/latest/ray-core/package-ref.html#ray-wait\">ray.wait() </a> is a synchronous operation that allows you to process results without waiting on all tasks to complete. It also <a href=\"https://docs.ray.io/en/master/ray-core/patterns/limit-pending-tasks.html\"> limits the number of  pending tasks </a> so that the pending task queue won't grow indefinitely and cause out of memory problems.\n",
    "</div>\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Scaling_inference/distributed_timeline.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Timeline of distributed batch inference where batches are assigned as soon as a worker completes a task and becomes available.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a7b0fd-b23e-4212-bfcd-483c6a604376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the resulting segmentation maps array.\n",
    "predictions[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60cb444",
   "metadata": {},
   "source": [
    "**Coding Exercise**\n",
    "\n",
    "In this tutorial, the default setting for `N_ACTORS` is 2. Try setting the number of actors to the number of CPUs/GPUs you have available. \n",
    "\n",
    "How does this affect runtime performance?\n",
    "\n",
    "Hint: Change `N_ACTORS` in the section called \"Create list of Ray Actors.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7036efa3-2dd1-4b55-82e3-da81a0a33f13",
   "metadata": {},
   "source": [
    "#### Using Ray ActorPool utility\n",
    "\n",
    "You have just manually managed batch assignment and task scheduling on Ray Actors for batch inference. This offers plenty of granular control over exactly how to distribute work and monitor in-flight tasks. \n",
    "\n",
    "However, you may choose to opt to use the convenient Ray [ActorPool](https://docs.ray.io/en/latest/ray-core/package-ref.html#ray-util-actorpool) utility which wraps the list of actors to automatically handle futures management. In this short example, we will recreate this approach and demonstrate how to use this utility abstraction.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Scaling_inference/actor_pool.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|The ActorPool wraps around a list of `n` actors so you do not have to manage idle actors and manually distribute workloads.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5408bc-f571-488a-9fa4-6c97b48df284",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Prepare batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b33984-663a-453f-9038-c106c0a8f5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get BATCH_SIZE * N_BATCHES randomly shuffled image IDs from the test dataset.\n",
    "image_indices = get_image_indices(dataset=test_dataset, n=BATCH_SIZE * N_BATCHES)\n",
    "\n",
    "# Split indices into N_BATCHES\n",
    "image_indices_grouped = np.split(np.asarray(image_indices), N_BATCHES)\n",
    "\n",
    "batches = []\n",
    "\n",
    "# Create a list of images for each batch of indices sampled from the test dataset.\n",
    "for image_idx in image_indices_grouped:\n",
    "    batch = [test_dataset[int(i)][\"image\"] for i in image_idx]\n",
    "    batches.append(batch)\n",
    "\n",
    "batches[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6a75b0-648e-4806-9ae6-f49f9ce10ba0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create ActorPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2baad5-ea29-44d3-88d8-28f81190caf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.util.actor_pool import ActorPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb97c5e3-0cec-41b2-ad44-0a5e4dda0b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the actors in an ActorPool utility to automatically handle future management.\n",
    "actor_pool = ActorPool(actors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c8ade5",
   "metadata": {},
   "source": [
    "Note: The [ActorPool](https://docs.ray.io/en/latest/ray-core/package-ref.html#ray-util-actorpool) is fixed in size, unlike task-based approach where the number of parallel tasks can be dynamic. To have autoscaling of the ActorPool, you will need to use the Ray AIR approach discussed in the next approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755b1725-82ce-409f-8251-6eb24964fabc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Run parallel inference on 10 batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50e7b08",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Runs prediction and returns object references to segmentation maps.\n",
    "def actor_call(\n",
    "    actor: ray.actor.ActorHandle, batch_of_images: list[list[JpegImageFile]]\n",
    ") -> list[ray._raylet.ObjectRef]:\n",
    "    return actor.predict.remote(images=batch_of_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058dbfae-10b2-452e-9054-e01062ca4321",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []  # A list of final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaacec5d-1e99-40bc-b160-642c2a32b554",
   "metadata": {},
   "outputs": [],
   "source": [
    "for segmentation_maps in actor_pool.map_unordered(actor_call, batches):\n",
    "    prediction_results_postprocessing(\n",
    "        predictions=predictions, segmentation_maps=segmentation_maps\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029149ee",
   "metadata": {},
   "source": [
    "By using the `ActorPool` utility, you were able to easily run distributed batch inference with just a few lines of code. The `map_unordered` function runs the defined inference logic on each batch and handles post-processing, eliminating the need for manual orchestration of actors. This simplifies the process and reduces the need for monitoring tasks and actors at various stages of completion.\n",
    "\n",
    "Note: `map_unordered` has slightly better efficiency that a similar method `actor_pool.map` since this example does not preference the order of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5730b12c-c5e2-4935-8b06-d0d01b36e3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the resulting segmentation maps array.\n",
    "predictions[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfbe82e",
   "metadata": {},
   "source": [
    "**Coding Exercise**\n",
    "\n",
    "While the `ActorPool` utility offers a good level of abstraction above orchestrating actors directly, there are [methods](https://docs.ray.io/en/latest/ray-core/package-ref.html?highlight=actorpool#ray-util-actorpool) available to you to schedule tasks, inspect in-flight jobs, and retrieve idle actors.\n",
    "\n",
    "Try look into the actor pool by printing out which actors are idle and which tasks remain during the inferencing step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8a0b7a-3cff-4aea-9b92-c28200560728",
   "metadata": {},
   "source": [
    "#### Shutdown Ray runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3aae2c-491f-41a9-bd67-09395f398890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terminate processes started by ray.init().\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941f3d6d-42c8-4726-ab46-21527fd737e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Summary: Distributed, stateful batch inference with Ray Actors\n",
    "\n",
    "##### Key concepts\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong>Stateful inference</strong>: Inference carried out over stateful processes where Ray actors hold model replicas and can mutate and persist state.\n",
    "</div>\n",
    "\n",
    "##### Key API elements\n",
    "\n",
    "* **`ActorPool()`**  \n",
    "Wraps the list of actors that run inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86ade2c-5e33-46af-804a-a10dd0e2bfc8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Summary: Choosing a Ray architecture for batch inference\n",
    "\n",
    "In this module, you have encountered two approaches for scalable, distributed batch inference with Ray:\n",
    "\n",
    "1. Batch inference with Ray AI Runtime (AIR)\n",
    "1. Batch inference with Ray Core API\n",
    "\n",
    "To compare these architectures and choose the appropriate one for your specific needs, consider two main categories: Ray Core-based approaches and Ray AI Runtime (AIR)-based approaches.\n",
    "\n",
    "### Ray Core-based approaches\n",
    "These approaches involve using Ray Core primitives, such as Tasks, Actors, and the optional [ActorPool utility](https://docs.ray.io/en/latest/ray-core/package-ref.html#ray-util-actorpool). Each method shares the ability to offer fine-grained control over the behavior of a distributed application. Ray Core-based batch inference exposes the parallelism mechanisms and allows you to specify *how* inference should be executed.\n",
    "\n",
    "For some, this high level of control fits their use case well. One of Anyscale's customers performs large-scale batch inference on satellite images. Because each image is so large, it makes sense for them to run prediction on single images. Using Ray to parallelize inference allows them to control compute resources assigned to individual tasks, and this straightforward application significantly cuts down on runtime over alternative methods.\n",
    "\n",
    "In a similar vein, [Dendra](https://dendra.io/) utilizes model replicas on Ray Actors to run batch inference on aerial imagery for [ecological restoration](https://www.anyscale.com/blog/how-ray-and-anyscale-make-it-easy-to-do-massive-scale-machine-learning-on). Ray allows their inference pipeline to scale from thousands to millions of ecosystem images without having to change any underlying infrastructure. Richard Decal, the Lead ML Engineer at Dendra, explains \"This approach allowed us to maximize our network I/O and GPUs usage across the cluster.\" Now, Dendra can focus on its core business while offloading the compute management to Ray and Anyscale.\n",
    "\n",
    "### Ray AI Runtime (AIR)-based approaches\n",
    "These approaches involve using Ray AIR's functionality through [BatchPredictor](https://docs.ray.io/en/latest/ray-air/package-ref.html#batch-predictor) performing inference on [Ray Datasets](https://docs.ray.io/en/latest/data/api/dataset.html). By using these high-level abstractions, you define *what* needs to be done rather than *how* it should be done. You can focus on the inference logic for your specific use case while delegating the responsibility of distributing execution to Ray.\n",
    "\n",
    "Ray AIR may be a convenient choice for a variety of reasons:\n",
    "\n",
    "* [BatchPredictor](https://docs.ray.io/en/latest/ray-air/package-ref.html#batch-predictor) gives you out-of-the-box predictors, handles framework native batch conversions, and gives you an option to resume from AIR Checkpoint.\n",
    "* Ray Datasets handle distributed processing, creating batches of data, pipelining, autoscaling, and memory management.\n",
    "* Ray AIR libraries are connected, giving you an option to scale other parts of your pipeline in the future.\n",
    "\n",
    "### Key differences\n",
    "\n",
    "| |Ray Core|Ray AI Runtime|\n",
    "|:-:|:-:|:-:|\n",
    "|**Expose parallelism**|Yes|No|\n",
    "|**Scalable from workstation to large cluster**|Yes|Yes|\n",
    "|**Integrations**|Build yourself|Out-of-the-box (PyTorch, TF and [more](https://docs.ray.io/en/latest/ray-air/package-ref.html#trainer-and-predictor-integrations))|\n",
    "|**Data pre-processing**|Build yourself|Out-of-the-box support|\n",
    "\n",
    "### Developer experience\n",
    "\n",
    "When working with [Ray Core]((https://docs.ray.io/en/latest/ray-core/walkthrough.html)) and [Ray AIR](https://docs.ray.io/en/latest/ray-air/getting-started.html), you will notice that they have different focuses. Ray Core enables developers to build and scale distributed Python applications and provides a high level of control, as you write Python code and scale it using Ray primitives. \n",
    "\n",
    "On the other hand, Ray AIR is a toolkit for simplifying ML compute and abstracts away parallel computing primitives, making it easier to use but with a lower level of control.\n",
    "\n",
    "| |Ray Core|Ray AI Runtime|\n",
    "|:-:|:-:|:-:|\n",
    "|**Level of control**|High|Medium|\n",
    "|**Directly program distributed apps**|Yes|No|\n",
    "|**Flexibility**|High|Medium|\n",
    "|**Ease of use**|Medium|High|\n",
    "|**Entry barrier**|Depend on use case|Low|\n",
    "\n",
    "### Recommendations*\n",
    "\n",
    "In general, the final recommendation **depends heavily on your specific use case**. Ray Core-based solutions perform well in situations where the batch inference pipeline contains stages that are well-suited for straightforward parallelization. These applications can easily be divided into smaller, independent tasks that can be processed concurrently, without requiring significant communication or coordination between the different processing units.\n",
    "\n",
    "For most batch inference problems, however, it is advisable to **start by using Ray AIR's BatchPredictor** due to its lower barrier to entry, built-in optimizations, ability to abstract away the more tedious aspects of distribution, among many reasons discussed previously.\n",
    "\n",
    "**Keep in mind that these recommendations are meant to provide initial guidance and are not a definitive guide. If you have any questions or want to discuss your use case further, you can reach out on our Slack channel.*\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Scaling_inference/actor_inference.png\" width=\"90%\" loading=\"lazy\">|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Scaling_inference/air_batchpredictor.png\" width=\"90%\" loading=\"lazy\">|\n",
    "|:-:|:-:|\n",
    "|Ray Core-based approach with Ray Actors|Ray AIR-based approach with BatchPredictor|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe90b042-e31b-483e-8368-5d7b7aa1c761",
   "metadata": {},
   "source": [
    "# Connect with the Ray community\n",
    "\n",
    "You can learn and get more involved with the Ray community of developers and researchers:\n",
    "\n",
    "* [**Ray documentation**](https://docs.ray.io/en/latest)\n",
    "\n",
    "* [**Official Ray Website**](https://www.ray.io/)  \n",
    "Browse the ecosystem and use this site as a hub to get the information that you need to get going and building with Ray.\n",
    "\n",
    "* [**Join the Community on Slack**](https://forms.gle/9TSdDYUgxYs8SA9e8)  \n",
    "Find friends to discuss your new learnings in our Slack space.\n",
    "\n",
    "* [**Use the Discussion Board**](https://discuss.ray.io/)  \n",
    "Ask questions, follow topics, and view announcements on this community forum.\n",
    "\n",
    "* [**Join a Meetup Group**](https://www.meetup.com/Bay-Area-Ray-Meetup/)  \n",
    "Tune in on meet-ups to listen to compelling talks, get to know other users, and meet the team behind Ray.\n",
    "\n",
    "* [**Open an Issue**](https://github.com/ray-project/ray/issues/new/choose)  \n",
    "Ray is constantly evolving to improve developer experience. Submit feature requests, bug-reports, and get help via GitHub issues.\n",
    "\n",
    "* [**Become a Ray contributor**](https://docs.ray.io/en/latest/ray-contribute/getting-involved.html)  \n",
    "We welcome community contributions to improve our documentation and Ray framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5891f0e7-56f7-41c5-b130-6bbee6e878f8",
   "metadata": {},
   "source": [
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Generic/ray_logo.png\" width=\"20%\" loading=\"lazy\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "vscode": {
   "interpreter": {
    "hash": "567405a8058597909526349386224fe35dd047505a91307e44ed44be00113429"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
