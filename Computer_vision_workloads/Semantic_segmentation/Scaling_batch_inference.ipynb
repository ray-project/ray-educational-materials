{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcb9a269-765a-46ef-abb6-d60682d38b0d",
   "metadata": {},
   "source": [
    "# Scalable Batch Inference with Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b114120-a325-4597-bf7b-8310c9c8d776",
   "metadata": {},
   "source": [
    "<img src=\"../../_static/assets/Generic/ray_logo.png\" width=\"20%\" loading=\"lazy\">\n",
    "\n",
    "## About this notebook\n",
    "\n",
    "### Is it right for you?\n",
    "\n",
    "This module focuses on batch inference, presenting several approaches for scaling inference on Ray through hands-on examples. It is right for you if:\n",
    "\n",
    "* you observe performance bottlenecks when working on model (batch) inference problems\n",
    "* you want to scale or increase throughput of your existing batch inference pipelines\n",
    "* you wish to explore different architectures for batch inference with Ray Core and Ray AIR\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "For this notebook you should have:\n",
    "\n",
    "* practical Python and machine learning experience\n",
    "* familiarity with batch inference pattern in ML\n",
    "* familiarity with Ray and Ray AIR equivalent to completing these training modules:\n",
    "  * [Overview of Ray](https://github.com/ray-project/ray-educational-materials/blob/main/Introductory_modules/Overview_of_Ray.ipynb)\n",
    "  * [Introduction to Ray AIR](https://github.com/ray-project/ray-educational-materials/blob/main/Introductory_modules/Introduction_to_Ray_AIR.ipynb)\n",
    "  * [Ray Core](https://github.com/ray-project/ray-educational-materials/tree/main/Ray_Core)\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "Upon completion of this notebook, you will be able to:\n",
    "\n",
    "* understand common design patterns for batch inference\n",
    "* identify multiple approaches for scaling batch inference with Ray\n",
    "* compare the benefits and drawbacks of different inference architectures on Ray for different use cases\n",
    "\n",
    "### What will you do?\n",
    "\n",
    "* using a semantic segmentation task, encounter several batch inference implementation approaches using:\n",
    "  * Ray Tasks\n",
    "  * Ray Actors\n",
    "  * Ray ActorPool utility\n",
    "  * Ray AIR Datasets\n",
    "  * Ray AIR BatchPredictor\n",
    "* explore parallelized inference through hands-on coding exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a02709-0ed5-489a-b344-27005261eb72",
   "metadata": {},
   "source": [
    "## Part 1: Scalable batch inference design paterns with Ray\n",
    "\n",
    "The end goal for machine learning models is to generate performant predictions over a set of unseen data. In this module, you will approach parallelizing batch inference on using Ray Core's API as well as the high-level abstractions available in Ray AI Runtime.\n",
    "\n",
    "|<img src=\"../../_static/assets/Scaling_inference/example_ml_workflow.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Example of a machine learning workflow.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8becdd0-3f1e-4503-92b6-0e275e03ad75",
   "metadata": {},
   "source": [
    "### Stateless inference - Ray Tasks\n",
    "\n",
    "Loading complex models into memory can be expensive and sequential processing of requests limits speed. *Stateless inference* allows an ML system to handle high volume requests by:\n",
    "\n",
    "1. exporting the model's mathematical core into a language agnostic format\n",
    "2. restoring the architecture and weights of a trained model in a stateless function (i.e. Ray tasks)\n",
    "\n",
    "A Ray task is *stateless* because its output (e.g. predictions) is determined purely by its inputs (e.g. the trained model). Performing online inference involves loading the model for every request and synchronously serving results.\n",
    "\n",
    "|<img src=\"../../_static/assets/Scaling_inference/task_inference.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Stateless inference using Ray Tasks.|\n",
    "\n",
    "In the figure above, you perform batch inference by preprocessing your big dataset into batches that are assigned to workers via Ray tasks. Each task loads the trained model and outputs predictions on batches as they are assigned.\n",
    "\n",
    "**Code Snippet**:\n",
    "\n",
    "```python\n",
    "object_refs = [task.remote(input) for _ in range(10)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5687ea84-b693-4a97-8de4-26954a4c97c4",
   "metadata": {},
   "source": [
    "### Stateful inference with Ray Actors\n",
    "\n",
    "When your deployed model takes too long to generate immediate results, online prediction may not be the right approach. In addition, some situations require predictions to be generated over large volumes of data such as curating personalized playlists. You can use *batch inference*, which is an asynchronous method of batching observations for prediction in advance to process a high volume of samples efficiently.\n",
    "\n",
    "Setting up distributed batch inference with Ray involves:\n",
    "\n",
    "1. creating a number of replicas of your model; in Ray, these replicas are represented as Actors (i.e., stateful processes) that can be assigned to GPUs and hold instantiated model objects\n",
    "\n",
    "2. feeding data into these model replicas in parallel, and retrieve inference results\n",
    "\n",
    "|<img src=\"../../_static/assets/Scaling_inference/actor_inference.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Stateful inference using Ray Actors.|\n",
    "\n",
    "Much like stateless inference using Ray tasks, stateful inference replaces Ray tasks with Ray actors and leverages Ray's object store to avoid loading the model for every batch.\n",
    "\n",
    "**Code Snippet**:\n",
    "\n",
    "```python\n",
    "actors = [ActorCls.remote(input) for _ in range(10)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc92ba9-95c2-48b2-a115-064be421995d",
   "metadata": {},
   "source": [
    "### Stateful inference with Ray ActorsPool utility\n",
    "\n",
    "Ray provides a convenient [ActorPool utility](https://docs.ray.io/en/latest/ray-core/package-ref.html#ray-util-actorpool) which wraps the above list of actors to avoid futures management.\n",
    "\n",
    "|<img src=\"../../_static/assets/Scaling_inference/actor_pool.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Using Actor Pools for Batch Inference.|\n",
    "\n",
    "Building off of the stateful inference diagram, an Actor Pool wraps around the `n` actors so you do not have to manage idle actors and manually distribute workloads.\n",
    "\n",
    "**Code Snippet**:\n",
    "\n",
    "```python\n",
    "from ray.util.actor_pool import ActorPool\n",
    "actor_pool = ActorPool(actors)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c500d89-bdae-41fd-9b12-264c0839af90",
   "metadata": {},
   "source": [
    "### Batch inference with Ray AIR Datasets\n",
    "\n",
    "Ray Datasets allows for parallel reading and preprocessing of source data along with autoscaling of the ActorPool. As a part of Ray AIR, you specify what you want done through a set of declarative key-value arguments rather than concerning yourself with how to instruct Ray to scale.\n",
    "\n",
    "|<img src=\"../../_static/assets/Scaling_inference/ray_datasets.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Ray Datasets replace the 'Batch preprocessing' stage.|\n",
    "\n",
    "In Ray AIR, a trained model is loaded into a `Checkpoint` object (could be from training or tuning). An AIR `Predictor` loads model from the `Checkpoint` to perform inference. Then, using the preprocessed batches provided by Ray Datasets, you extract predictions off of the testing data.\n",
    "\n",
    "**Code Snippet**:\n",
    "\n",
    "```python\n",
    "batches = data.map_batches(\n",
    "              MyModel,\n",
    "              num_gpus=1,\n",
    "              batch_size-1024,\n",
    "              compute=ray.data.ActorPoolStrategy(min_size=10, max_size=50)\n",
    "          )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a4c211-95fe-4218-a10a-08c0cfeafc9c",
   "metadata": {},
   "source": [
    "### Batch inference with high-level API - BatchPredictor\n",
    "\n",
    "Ray AIR's [`BatchPredictor`](https://docs.ray.io/en/latest/ray-air/package-ref.html#batch-predictor) takes in a [`Checkpoint`](https://docs.ray.io/en/latest/ray-air/package-ref.html#checkpoint) which represents the saved model. This high-level abstraction offers simple and composable APIs that enable preprocessing data in batches with [BatchMapper](https://docs.ray.io/en/latest/ray-air/package-ref.html#generic-preprocessors) and instantiate a distributed predictor given checkpoint data.\n",
    "\n",
    "|<img src=\"../../_static/assets/Scaling_inference/air_batchpredictor.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Using Ray AIR's `BatchPredictor` for Batch Inference.|\n",
    "\n",
    "Finally, you can use an AIR `BatchPredictor` that takes both the `Checkpoint` and `Predictor` to replace the process of manually performing inference on a large dataset.\n",
    "\n",
    "**Code Snippet**:\n",
    "\n",
    "```python\n",
    "batch_predictor = BatchPredictor(\n",
    "                      Checkpoint,\n",
    "                      Predictor\n",
    "                  )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee761b0-7549-46ca-99bc-e3588a7a1deb",
   "metadata": {},
   "source": [
    "## Part 2: Data and model used in this notebook - vision transformers for semantic segmentation\n",
    "\n",
    "### SceneParse150 - MIT Scene Parsing Benchmark\n",
    "\n",
    "Image segmentation takes a scene and classifies image objects [into semantic categories](https://docs.google.com/spreadsheets/d/1se8YEtb2detS7OuPE86fXGyD269pMycAWe2mtKUj2W8/edit?usp=sharing) pixel-by-pixel. [MIT ADE20K Dataset](http://sceneparsing.csail.mit.edu/) (SceneParse150) provides the largest open source dataset for scene parsing, and in this notebook, you will be scaling inference on image regions depicted in these samples.\n",
    "\n",
    "|<img src=\"../../_static/assets/Scaling_inference/scene.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Test image on the left vs. predicted result on the right.[Source](https://github.com/CSAILVision/semantic-segmentation-pytorch) *Date accessed: November 10, 2022*|\n",
    "\n",
    "Dataset Highlights:\n",
    "\n",
    "* 20k annotated, scene-centric training images\n",
    "* 2k validation images\n",
    "* 150 total categories such as person, car, bed, sky, and more\n",
    "\n",
    "### SegFormer - modern transformer for vision tasks\n",
    "\n",
    "[SegFormer](https://arxiv.org/pdf/2105.15203.pdf) is a simple and powerful semantic segmentation method whose architecture consists of a hierarchical Transformer encoder and a lightweight All-MLP decoder. What sets SegFormer apart from previous approaches boils down to two key features:\n",
    "\n",
    "1. a novel hierarchically structured Transformer encoder which does not depend on positional encoding, avoiding interpolation when test resolution differs from training\n",
    "2. avoids complex decoders\n",
    "\n",
    "With demonstrated success on benchmarks such as Cityscapes and [MIT ADE20K Dataset](http://sceneparsing.csail.mit.edu/), you will use a pretrained version to perform inference on test images from the SceneParse 150 dataset.\n",
    "\n",
    "|<img src=\"../../_static/assets/Scaling_inference/segformer_architecture.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Segformer architecture taken from [original paper](https://arxiv.org/pdf/2105.15203.pdf). *Date accessed: November 10, 2022*|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c018d564-0e4a-453f-9f7a-74466c0273e8",
   "metadata": {},
   "source": [
    "## Part 3: Sequential batch inference implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a07b49f-ee7b-48c7-a4d0-bc4043ff24c9",
   "metadata": {},
   "source": [
    "|<img src=\"../../_static/assets/Scaling_inference/single_sequential_timeline.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Sequential inference on the single worker. Performance is limited to the single machine performance.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb5419c-26af-4b09-8fd2-e885aaa60021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e155a5b3-d37b-45d6-b9d2-74720b94b5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(201)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347f873c",
   "metadata": {},
   "source": [
    "Setting the seed to a constant value ensures that multiple runs of the notebook produce the same results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a7f1cb-4009-4848-b589-974f82285963",
   "metadata": {},
   "source": [
    "### Load pre-trained model from the HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e32feab-170b-4220-a263-bb678592d106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_labels\n",
    "from transformers import SegformerForSemanticSegmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4346c5a0-36b4-46ce-bdc0-3102543d2dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"nvidia/segformer-b0-finetuned-ade-512-512\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0987413-f1d7-4f97-a0a9-ba583b593631",
   "metadata": {},
   "source": [
    "There are five different segformers to choose from. The model chosen here is the smallest of the five. These models are pretrained and they are fine tuned on the MITADE20K dataset of 512x512 images. \n",
    "\n",
    "https://huggingface.co/nvidia/segformer-b0-finetuned-ade-512-512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c27154-9930-466e-b89f-45cbe487a690",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label, label2id = get_labels()\n",
    "print(f\"total labels: {len(id2label)}\")\n",
    "print(f\"example lables: {list(id2label.values())[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9354716-8ac2-4e4f-97e1-e48ae174f0d9",
   "metadata": {},
   "source": [
    "The function `get_labels` downloads from the `huggingface_hub` library the mappings `id2label` and `label2id` between the label IDs and the labels for the categories of objects in the images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab7f82a-93e7-44c3-ac0e-322ebcdfd443",
   "metadata": {},
   "source": [
    "#### Load SegFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aec6f8-22bf-41f9-bd7b-fe54889d762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    MODEL_NAME, id2label=id2label, label2id=label2id\n",
    ")\n",
    "print(f\"number of model parameters: {model.num_parameters()/(10**6):.2f} M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f3ef57",
   "metadata": {},
   "source": [
    "The model loaded here is the smallest of the five segformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41546abd-2f3d-4128-8176-19cd0604d6f4",
   "metadata": {},
   "source": [
    "#### Create feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f888ed08-cde2-4f4a-b05f-e467de4b7ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"reduce_labels\" is to drop background from loss compute: https://huggingface.co/docs/transformers/model_doc/segformer#segformer\n",
    "from transformers import SegformerFeatureExtractor\n",
    "\n",
    "feature_extractor = SegformerFeatureExtractor.from_pretrained(\n",
    "    MODEL_NAME, reduce_labels=True\n",
    ")\n",
    "feature_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9a23f3",
   "metadata": {},
   "source": [
    "Every huggingface model has an associated feature extractor that preprocesses the input features. The flag `reduce_labels` removes the background from the loss computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3109152b-3572-40f9-8fc6-b04ecce59896",
   "metadata": {},
   "source": [
    "### Prepare SceneParse150 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d58d4b-d13c-4c34-8e80-194c8f6bdae9",
   "metadata": {},
   "source": [
    "#### Load dataset from the HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70f31fe-e8a9-41ea-857c-e2e2b36503e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from Hugging Face\n",
    "from datasets import load_dataset\n",
    "\n",
    "DATASET_NAME = (\n",
    "    \"scene_parse_150\"  # name of the dataset on the HuggingFace's datasets repository.\n",
    ")\n",
    "\n",
    "# split here only for fast-debug, remove before real use.\n",
    "# ds = load_dataset(DATASET_NAME, split=\"train[:50]\")  # for dry run only\n",
    "dataset_dict = load_dataset(DATASET_NAME)\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6e66d8-2587-43c9-91bb-cdfc6f22b33d",
   "metadata": {},
   "source": [
    "This can take some time, because you download data - over 20k images to the local machine or cluster.\n",
    "\n",
    "The `load_dataset` utility loads the SceneParse150 dataset from Hugging Face's `datasets` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98814d17-d57d-4bdd-ba1c-8e64880f6361",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset_dict[\"train\"]\n",
    "\n",
    "print(f\"train_dataset\\n{train_dataset}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2a63ad-3bfd-453c-827e-6a0f664ac310",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Display example images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c22f28-f63c-42e8-ab43-1684aa692d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import display_example_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1054dfa-90a6-4569-bc0f-cb059f4b4893",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_example_images(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c6e51b",
   "metadata": {},
   "source": [
    "Each Hugging Face dataset comes with a `train_test_split` method that we're going to use next. We want 80% of the data to be training data, and 20% held back for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2394456",
   "metadata": {},
   "source": [
    "To get a feel for what this dataset consists of, let's print the first of it. Since the train-test split we did is randomized, the resulting image will be different every time you load the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85464a2-d42f-45cb-907f-c206eecfc999",
   "metadata": {},
   "source": [
    "### Run inference on few images and visualize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70115952-fdb0-40d3-87d0-a7903502a289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import visualize_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba82c68-f3cd-46ae-8b23-6fee43ccb3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, image, labels, device):\n",
    "    inputs = feature_extractor(\n",
    "        images=image, segmentation_maps=labels, return_tensors=\"pt\"\n",
    "    )\n",
    "    outputs = model(\n",
    "        pixel_values=inputs.pixel_values.to(device), labels=inputs.labels.to(device)\n",
    "    )\n",
    "    loss = outputs.loss.detach().cpu().numpy()\n",
    "\n",
    "    upsampled_logits = torch.nn.functional.interpolate(\n",
    "        outputs.logits.cpu(),\n",
    "        size=image.size[::-1],\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "    return upsampled_logits.argmax(dim=1)[0], loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810e036a",
   "metadata": {},
   "source": [
    "The `predict` function uses the input `model` to predict the label of each pixel in input image. The prediction takes the following steps:\n",
    "\n",
    "1. The input image is converted using `feature_extractor` into three 512x512 images representing the three color chanels of the input image. This is independent of the original size of the input image.\n",
    "2. The 512x512 images are then passed to the model, which then produces 150 128x128 images, one image for each available category. Each image is a mask representing the part of the image that belongs to that category.\n",
    "3. In order to display the predicted regions on top of the original image, the 150 128x128 images are upsampled to the size of the original.\n",
    "4. The 150 images are then collapsed into a single image using `argmax`, where each pixel has the label ID of the category predicted for that pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5500ec-cafc-4c27-a3ca-02ac093bf8c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Run inference on train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b9f0f1-be22-4910-a5a1-e6d861cf3a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "j = np.random.randint(train_dataset.num_rows)\n",
    "\n",
    "random_image = train_dataset[j][\"image\"]\n",
    "labels = train_dataset[j][\"annotation\"]\n",
    "\n",
    "segmentation, loss = predict(model=model, image=random_image, labels=labels, device=dev)\n",
    "\n",
    "visualize_predictions(image=random_image, predictions=segmentation, loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b62f05-26bc-46bf-941d-945bcdf0068e",
   "metadata": {},
   "source": [
    "Each time you run this code, a different image from the training set is passed to `predict`, and the categories assigned to each pixel of the image are displayed with a different color."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7243ec-d207-4b33-89ca-b80309c19c06",
   "metadata": {},
   "source": [
    "### Run sequential batch inference on data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021487a1-de67-4e76-96e9-998796638662",
   "metadata": {},
   "source": [
    "#### Run inference on a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ed4236-c68a-4622-a044-735bc2c9342f",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = np.random.randint(train_dataset.num_rows)\n",
    "\n",
    "random_image = train_dataset[j][\"image\"]\n",
    "labels = train_dataset[j][\"annotation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3902edad-6839-4b09-b3db-832bff0c8c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "segmentation, loss = predict(model=model, image=random_image, labels=labels, device=dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67a2244-089f-4af0-9d0f-7bd1c53820e6",
   "metadata": {},
   "source": [
    "Time how long it takes to run `predict` on a single image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373c05ba-799a-4eb6-a7f6-c879bcbdd336",
   "metadata": {},
   "source": [
    "##### Performance analysis\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong>Performance</strong>: time needed to run inference on the batch of data. Measured in seconds.\n",
    "</div>\n",
    "\n",
    "|Compute       |Performance  |\n",
    "|:-------------|:------------|\n",
    "|M1 MacBook Pro|0.45s        |\n",
    "|cluster 1 AWS |x.xxs        |\n",
    "|cluster 2 AWS |x.xxs        |\n",
    "\n",
    "Average time is approximately 0.45 seconds on the single image for SegFormer model with 3.7M parameters (b0 variant).\n",
    "\n",
    "\n",
    "*Results are not representative and are meant to provide you with an intuitive understanding of the performance.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa7df14-25a9-40ab-8bf0-05588af9883b",
   "metadata": {},
   "source": [
    "#### Run batch inference on 10 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6f6fad-d7be-40da-80ef-094f822bd556",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_image_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb4337e-28c2-4f5a-bf23-638137b2649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_IMAGES = 10\n",
    "\n",
    "image_indices = get_image_indices(dataset=train_dataset, n=N_IMAGES)\n",
    "image_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248512f7-da91-4a57-ada8-28e602a0e611",
   "metadata": {},
   "source": [
    "Get 10 random image IDs from the training data set to run inference on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a576c5-5af3-4791-a767-da2fe366f439",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for i in image_indices:\n",
    "    image = train_dataset[i][\"image\"]\n",
    "    labels = train_dataset[i][\"annotation\"]\n",
    "    segmentation, loss = predict(model=model, image=image, labels=labels, device=dev)\n",
    "    predictions.append((segmentation, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149e4dab",
   "metadata": {},
   "source": [
    "Time how long it takes to run `predict` in series on the 10 random images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5efa01f-c36e-4093-926f-24847a99ac24",
   "metadata": {},
   "source": [
    "##### Performance analysis\n",
    "\n",
    "Some experiments results:\n",
    "\n",
    "|Compute       |Performance  |\n",
    "|:-------------|:------------|\n",
    "|M1 MacBook Pro|4.7s         |\n",
    "|cluster 1 AWS |x.xxs        |\n",
    "|cluster 2 AWS |x.xxs        |\n",
    "\n",
    "Performance is a linear function of nuber of images in batch. Single image performance was 0.45s -> 10 images is 4.7s.\n",
    "\n",
    "*Results are not representative and are meant to provide you with an intuitive understanding of the performance.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e69cad-d947-42d0-9cad-8f0a2d4941c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Summary: sequential batch inference implementation\n",
    "\n",
    "|<img src=\"../../_static/assets/Scaling_inference/single_sequential_timeline.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Sequential inference on the single worker. Performance is limited to the single machine performance.|\n",
    "\n",
    "|Compute       |1 image|10 images|100 images|\n",
    "|:-------------|:------|:--------|:---------|\n",
    "|M1 MacBook Pro|0.45s  |4.7s     |53s       |\n",
    "|cluster 1 AWS |x.xxs  |.        |.         |\n",
    "|cluster 2 AWS |x.xxs  |.        |.         |\n",
    "\n",
    "#### Key Concepts\n",
    "\n",
    "#### Key API Elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33c74b3-9d0f-469f-83ec-7e5557104489",
   "metadata": {},
   "source": [
    "## Part 4: Stateless inference - Ray Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cba2f68-c762-439e-850b-3af83ba1eaae",
   "metadata": {},
   "source": [
    "|<img src=\"../../_static/assets/Scaling_inference/task_inference.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Stateless inference using Ray Tasks.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665bc8b6-878e-4fde-9054-2891acf83c1b",
   "metadata": {},
   "source": [
    "### Initialize Ray runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c97168",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "if ray.is_initialized:\n",
    "    ray.shutdown()\n",
    "\n",
    "cluster_info = ray.init()\n",
    "cluster_info.address_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc24d09d-3b63-4307-a47e-1a72b2e33c9a",
   "metadata": {},
   "source": [
    "### Put the modelÂ and feature extractor in the object store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abbe8d8-f00d-4ba8-a81f-aa0a5f480854",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ref = ray.put(model)\n",
    "feature_extractor_ref = ray.put(feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffc8a08-5629-42ae-94a0-1bdafad47c74",
   "metadata": {},
   "source": [
    "Place the model and feature extractor in the object store to avoid copying every time the model is passed to a remote function or method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946db436-ac8b-4ac9-8dca-e557dbd126fb",
   "metadata": {},
   "source": [
    "### Implement remote function for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797b9e38",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def inference_task(model, image, labels, device):\n",
    "    return predict(model=model, image=image, labels=labels, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e695c89f",
   "metadata": {},
   "source": [
    "The most naive version of parallelising prediction is to create Ray tasks that load the trained model internally when called. This way we can make the prediction task \"stateless\", but at the cost of incurring the overhead of loading the model every single time. This is akin to what serverless solutions like AWS Lambda would do, and this pattern could be worth it for tiny models, for which the application doesn't get bottle-necked by the model loading step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6170e984-f46a-416d-8f5d-9192f2f9a251",
   "metadata": {},
   "source": [
    "### Run batch inference on 100 images and assess scalability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572f87fa-ec0f-482e-9830-9bdc667c4a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_IMAGES = 100\n",
    "\n",
    "image_indices = get_image_indices(dataset=train_dataset, n=N_IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a5e447-c6e7-4a36-bfd0-4fd37d5b2011",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "prediction_refs = []\n",
    "for i in image_indices:\n",
    "    task_ref = inference_task.remote(\n",
    "        model=model_ref,\n",
    "        image=train_dataset[i][\"image\"],\n",
    "        labels=train_dataset[i][\"annotation\"],\n",
    "        device=dev,\n",
    "    )\n",
    "    prediction_refs.append(task_ref)\n",
    "\n",
    "predictions = ray.get(prediction_refs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc0dbf2",
   "metadata": {},
   "source": [
    "Each call to the remote function `inference_task.remote` returns immediately. Ray then schedules each task to execute in parallel using the available resources. Calling `ray.get` waits for all the predictions to finish and returns the final results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c89723-bf39-4a91-8900-74201b989940",
   "metadata": {},
   "source": [
    "#### Performance analysis\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong>Performance</strong>: time needed to run inference on the batch of data. Measured in seconds.\n",
    "</div>\n",
    "\n",
    "|Compute       |Performance  |\n",
    "|:-------------|:------------|\n",
    "|M1 MacBook Pro|13s          |\n",
    "|cluster 1 AWS |x.xxs        |\n",
    "|cluster 2 AWS |x.xxs        |\n",
    "\n",
    "Distributed batch inference yields approximately 4x performance gain, when compared to the sequential implementation.\n",
    "\n",
    "* Parallel: 13s.\n",
    "* Sequential: 53s.\n",
    "\n",
    "*Results are not representative and are meant to provide you with an intuitive understanding of the performance.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711cf20f-4cb2-4e16-ad66-bd273352c9d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Summary: stateless inference - Ray Tasks\n",
    "\n",
    "|Compute       |10 image|100 images|1k images|10k images|\n",
    "|:-------------|:-------|:---------|:--------|:---------|\n",
    "|M1 MacBook Pro|1.3s    |13s       |125s     |n.a.      |\n",
    "|cluster 1 AWS |x.xxs   |.         |.        |.         |\n",
    "|cluster 2 AWS |x.xxs   |.         |.        |.         |\n",
    "\n",
    "Average speed per prediction is 0.125s. That yields 4x performance speedup, when compared to the sequential approach, which is approximately 0.45s.\n",
    "\n",
    "#### Key Concepts\n",
    "\n",
    "#### Key API Elements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e2f9e2-945b-4b08-95ec-74b09b8b07d5",
   "metadata": {},
   "source": [
    "## Part 5: Stateful inference with Ray Actors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff1bb7d-b8f9-4a2d-89d2-01446e114525",
   "metadata": {},
   "source": [
    "|<img src=\"../../_static/assets/Scaling_inference/actor_inference.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Stateful batch inference using Ray Actors.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70563f89-c95c-4ee8-8374-0288d918d5fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Implement remote class for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4030c8a1",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class PredictionActor:\n",
    "    def __init__(self, model, feature_extractor):\n",
    "        self.model = model\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def predict(self, image, labels):\n",
    "        inputs = feature_extractor(\n",
    "            images=image, segmentation_maps=labels, return_tensors=\"pt\"\n",
    "        )\n",
    "        outputs = self.model(\n",
    "            pixel_values=inputs.pixel_values.to(self.device),\n",
    "            labels=inputs.labels.to(self.device),\n",
    "        )\n",
    "        loss = outputs.loss.detach().cpu().numpy()\n",
    "\n",
    "        upsampled_logits = torch.nn.functional.interpolate(\n",
    "            outputs.logits.cpu(),\n",
    "            size=image.size[::-1],\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "\n",
    "        return upsampled_logits.argmax(dim=1)[0], loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28da8dd3-08ff-47b5-8d29-be9e27a99f6b",
   "metadata": {},
   "source": [
    "Predict method is the same as in sequential implementation.\n",
    "\n",
    "The benefit of using actors over tasks is that actors allow keeping track of state. In this particular case, each instance of `PredictionActor` will hold its own copy of the model, to avoid having to load the model every time a call to `predict` is made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd911b90-2006-4fc9-bbbb-90e92ef79ee3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create list of Ray Actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f910cd41",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "N_ACTORS = 7\n",
    "\n",
    "idle_actors = []\n",
    "for i in range(N_ACTORS):\n",
    "    idle_actors.append(\n",
    "        PredictionActor.remote(model=model_ref, feature_extractor=feature_extractor_ref)\n",
    "    )\n",
    "\n",
    "idle_actors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b915bfba-7baf-4761-a99d-e6e36dfbfa0f",
   "metadata": {},
   "source": [
    "You named the list `idle_actors` as they are not doing anything yet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d39dc3-a2ac-45a6-aab6-6a26147dd9b3",
   "metadata": {},
   "source": [
    "### Run batch inference on 100 images with Ray Actors and assess scalability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a80f96-3561-4435-8661-1b989c22692e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_results_postprocessing(results, predictions):\n",
    "    predictions.append(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26d89d1",
   "metadata": {},
   "source": [
    "The purpose of `prediction_results_postprocessing` is to abstract away the final processing step. In this demo, the postprocessing step is a very simple one, but in practice it will likely be much more complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa802a4b-24cc-4716-bb94-a033b0394b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_IMAGES = 100\n",
    "preds = []\n",
    "future_to_actor_mapping = {}\n",
    "\n",
    "image_indices = get_image_indices(dataset=train_dataset, n=N_IMAGES)\n",
    "data = [\n",
    "    (train_dataset[i][\"image\"], train_dataset[i][\"annotation\"]) for i in image_indices\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb849ae-a56b-43a9-8af5-57bd776faac5",
   "metadata": {},
   "source": [
    "The variable `future_to_actor_mapping` will hold a mapping from futures to actors, to be able to determine which actors are idle by looking at the finished futures.\n",
    "\n",
    "The `data` variable is a list of image-annotation pairs, where the annotation contains the correct labeling of all the segments in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbce1966-c4be-406e-8df1-4ff1650b531b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "while data:\n",
    "    if idle_actors:\n",
    "        actor = idle_actors.pop()\n",
    "        image, labels = data.pop()\n",
    "        future = actor.predict.remote(image=image, labels=labels)\n",
    "        future_to_actor_mapping[future] = actor\n",
    "    else:\n",
    "        [ready], _ = ray.wait(list(future_to_actor_mapping.keys()), num_returns=1)\n",
    "        actor = future_to_actor_mapping.pop(ready)\n",
    "        idle_actors.append(actor)\n",
    "        prediction_results_postprocessing(ray.get(ready), preds)\n",
    "\n",
    "# Process any leftover results at the end.\n",
    "for future in future_to_actor_mapping.keys():\n",
    "    prediction_results_postprocessing(ray.get(future), preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462f5b7b",
   "metadata": {},
   "source": [
    "The `while` loop goes over all the image-annotation pairs, and if there is an idle actor, that actor is assigned the next image-annotation pair to work on. If no actors are idle, the loop waits until an actor finishes, and the assigns the next image-annotation pair that actor. The `future_to_actor_mapping` is used to keep track of what each actor is working on, so that when a task is finished we know which actor finished it.\n",
    "\n",
    "Finally, once all the data has been assigned, we wait for all remaining actors to finish their tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a7b0fd-b23e-4212-bfcd-483c6a604376",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df3c873-5617-43d9-8ac0-31319080de23",
   "metadata": {},
   "source": [
    "based on this: https://docs.ray.io/en/master/ray-core/patterns/limit-pending-tasks.html\n",
    "\n",
    "ray.wait() -> https://docs.ray.io/en/latest/ray-core/package-ref.html#ray-wait"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeef5f3",
   "metadata": {},
   "source": [
    "|<img src=\"../../_static/assets/Scaling_inference/sequential_timeline.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Timeline of sequential batch assignment spread across three workers.|\n",
    "\n",
    "|<img src=\"../../_static/assets/Scaling_inference/distributed_timeline.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Timeline of distributed bath inference where a scheduler orchestrates batch assignment as soon as a worker is available.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf9bc23-b565-48b0-8963-41bf3b2241ff",
   "metadata": {},
   "source": [
    "#### Performance analysis\n",
    "\n",
    "|Compute       |Performance  |\n",
    "|:-------------|:------------|\n",
    "|M1 MacBook Pro|15s          |\n",
    "|cluster 1 AWS |x.xxs        |\n",
    "|cluster 2 AWS |x.xxs        |\n",
    "\n",
    "Results for 7 actors\n",
    "\n",
    "*Results are not representative and are meant to provide you with an intuitive understanding of the performance.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e0209e-ca44-41d9-9c61-18150dc78285",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Summary: stateful inference with Ray Actors\n",
    "\n",
    "|Compute       |10 image|100 images|1k images|10k images|\n",
    "|:-------------|:-------|:---------|:--------|:---------|\n",
    "|M1 MacBook Pro|1.4s    |15s       |.        |n.a.      |\n",
    "|cluster 1 AWS |x.xxs   |.         |.        |.         |\n",
    "|cluster 2 AWS |x.xxs   |.         |.        |.         |\n",
    "\n",
    "#### Key Concepts\n",
    "\n",
    "#### Key API Elements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7036efa3-2dd1-4b55-82e3-da81a0a33f13",
   "metadata": {},
   "source": [
    "## Part 6: Stateful inference with Ray ActorPool utility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e914515e-6987-42d9-a29a-d9233db7971f",
   "metadata": {},
   "source": [
    "|<img src=\"../../_static/assets/Scaling_inference/actor_pool.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Using Actor Pools for Batch Inference.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6a75b0-648e-4806-9ae6-f49f9ce10ba0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create ActorPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9797d0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from ray.util.actor_pool import ActorPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756bce4f-a8ed-45c4-8f28-a44d5e4c16db",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ACTORS = 7\n",
    "\n",
    "actors = [\n",
    "    PredictionActor.remote(model=model_ref, feature_extractor=feature_extractor_ref)\n",
    "    for _ in range(N_ACTORS)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb97c5e3-0cec-41b2-ad44-0a5e4dda0b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_pool = ActorPool(actors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0c7732",
   "metadata": {},
   "source": [
    "Just as before, each actor is an instance `PredictionActor`, and `ActorPool` collectively wraps the actors to manage futures automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fa01e6-0b71-4d0f-8299-5331af4b7ba8",
   "metadata": {},
   "source": [
    "### Run batch inference on 100 images with ActorPool and assess scalability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50e7b08",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def actor_call(actor, data_item):\n",
    "    image, labels = data_item\n",
    "    return actor.predict.remote(image=image, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1e7ea3",
   "metadata": {},
   "source": [
    "`actor_call` returns an ObjectRef that computes the image segmentation prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0d3698-6901-4444-9e88-89a28d60cdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_IMAGES = 100\n",
    "preds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c3d855-5480-462c-811d-62a402fd2f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_indices = get_image_indices(dataset=train_dataset, n=N_IMAGES)\n",
    "data = [\n",
    "    (train_dataset[i][\"image\"], train_dataset[i][\"annotation\"]) for i in image_indices\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaacec5d-1e99-40bc-b160-642c2a32b554",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for result in actor_pool.map_unordered(actor_call, data):\n",
    "    prediction_results_postprocessing(result, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029149ee",
   "metadata": {},
   "source": [
    "`map_unordered` takes in:\n",
    "- `actor_call`: a function that takes `(actor, data_item)` as argument and returns an ObjectRef computing the result over the value. The actor will be considered busy until the ObjectRef completes.\n",
    "- `data`: a list of values that `actor_call(actor, data_item)` should be applied to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5730b12c-c5e2-4935-8b06-d0d01b36e3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744b251d-fda1-4b69-81f5-126b98b30354",
   "metadata": {},
   "source": [
    "#### Performance analysis\n",
    "\n",
    "|Compute       |Performance  |\n",
    "|:-------------|:------------|\n",
    "|M1 MacBook Pro|15s          |\n",
    "|cluster 1 AWS |x.xxs        |\n",
    "|cluster 2 AWS |x.xxs        |\n",
    "\n",
    "Results for 7 actors\n",
    "\n",
    "*Results are not representative and are meant to provide you with an intuitive understanding of the performance.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941f3d6d-42c8-4726-ab46-21527fd737e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Summary: stateful inference with Ray ActorPool utility\n",
    "\n",
    "|Compute       |10 image|100 images|1k images|10k images|\n",
    "|:-------------|:-------|:---------|:--------|:---------|\n",
    "|M1 MacBook Pro|1.4s    |15s       |.        |n.a.      |\n",
    "|cluster 1 AWS |x.xxs   |.         |.        |.         |\n",
    "|cluster 2 AWS |x.xxs   |.         |.        |.         |\n",
    "\n",
    "#### Key Concepts\n",
    "\n",
    "#### Key API Elements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7289f04c-337c-47e1-a415-a5c5d80b61cc",
   "metadata": {},
   "source": [
    "## Part 7: Batch inference with Ray AIR Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68550c41-7755-4863-900a-d8220d9d9b55",
   "metadata": {},
   "source": [
    "|<img src=\"../../_static/assets/Scaling_inference/ray_datasets.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Ray Datasets replace the 'Batch preprocessing' stage.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d050e876-1b42-41ee-b3f2-d0caa8a31fbc",
   "metadata": {},
   "source": [
    "### Create Ray dataset with 100 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44657fb2-407a-40e8-9e13-99b05674925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8248993a-0193-47da-aea7-8e04e473de9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_IMAGES = 100\n",
    "image_indices = get_image_indices(dataset=train_dataset, n=N_IMAGES)\n",
    "\n",
    "data = [\n",
    "    (train_dataset[i][\"image\"], train_dataset[i][\"annotation\"]) for i in image_indices\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1f4066-7d32-4cb6-813f-9a880263b2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ray.data.from_items(data)\n",
    "dataset.show(limit=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7e6f6a-1a06-4923-9691-d4dd8d352b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.take(limit=1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca4ba67-72f8-405c-b9c9-faccbe7900d1",
   "metadata": {},
   "source": [
    "### Implement class that computes predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54efdaa4-d0d6-473d-b647-0d7ee9a3e6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionClass:\n",
    "    def __init__(self, model, feature_extractor):\n",
    "        self.model = model\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        predictions_list = []\n",
    "\n",
    "        for data_item in batch:\n",
    "            image, labels = data_item\n",
    "\n",
    "            inputs = self.feature_extractor(\n",
    "                images=image, segmentation_maps=labels, return_tensors=\"pt\"\n",
    "            )\n",
    "            outputs = self.model(\n",
    "                pixel_values=inputs.pixel_values.to(self.device),\n",
    "                labels=inputs.labels.to(self.device),\n",
    "            )\n",
    "            loss = outputs.loss.detach().cpu().numpy()\n",
    "\n",
    "            upsampled_logits = torch.nn.functional.interpolate(\n",
    "                outputs.logits.cpu(),\n",
    "                size=image.size[::-1],\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=False,\n",
    "            )\n",
    "            upsampled_logits = upsampled_logits.argmax(dim=1)[0]\n",
    "\n",
    "            predictions_list.append(\n",
    "                {\"prediction\": upsampled_logits.detach().cpu().numpy(), \"loss\": loss}\n",
    "            )\n",
    "\n",
    "        return predictions_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6147a110-10f8-4ee1-ab83-48116ac0e4e3",
   "metadata": {},
   "source": [
    "`batch` - argument in __call__ is list.\n",
    "\n",
    "The return type must be one of:\n",
    "\n",
    "* `pandas.DataFrame`\n",
    "* `pyarrow.Table`\n",
    "* `numpy.ndarray`,\n",
    "* `Dict[str, numpy.ndarray]`\n",
    "* `list`\n",
    "\n",
    "https://docs.ray.io/en/latest/data/transforming-datasets.html#transform-datasets-writing-udfs\n",
    "\n",
    "https://docs.ray.io/en/latest/data/api/dataset.html#ray.data.Dataset.map_batches\n",
    "\n",
    "https://docs.ray.io/en/latest/data/transforming-datasets.html#batch-udf-output-types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb83f12c-d51f-4a6a-a95a-50d88ae1ba36",
   "metadata": {},
   "source": [
    "### Run batch inference on 100 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58424a37-642a-4906-86d4-f1e7b5a31058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data import ActorPoolStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daec098",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "results_dataset = dataset.map_batches(\n",
    "    PredictionClass,\n",
    "    batch_size=1,\n",
    "    num_gpus=0,\n",
    "    compute=ActorPoolStrategy(min_size=1, max_size=7),\n",
    "    fn_constructor_args=(model, feature_extractor),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ac047c-8885-4c09-abae-99d2023c78a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dataset.take(limit=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2824b4f4-94f3-4ac5-a06a-9bff6fed4086",
   "metadata": {},
   "source": [
    "don't forget to pass `fn_constructor_args` to construct PredictionClass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9a2656-0d64-44ec-9716-7b35a4cd51e6",
   "metadata": {},
   "source": [
    "What is ActorPoolStrategy?\n",
    "\n",
    "Try different `batch_size` values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34c05d8-7b3b-4524-9f03-7ec64166bdd4",
   "metadata": {},
   "source": [
    "### Summary: batch inference with Ray AIR Datasets\n",
    "\n",
    "|Compute       |10 image|100 images|1k images|10k images|\n",
    "|:-------------|:-------|:---------|:--------|:---------|\n",
    "|M1 MacBook Pro|1.3s    |18.9s     |125s     |n.a.      |\n",
    "|cluster 1 AWS |x.xxs   |.         |.        |.         |\n",
    "|cluster 2 AWS |x.xxs   |.         |.        |.         |\n",
    "\n",
    "#### Key Concepts\n",
    "\n",
    "#### Key API Elements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460ac28a-3739-4456-99a3-1579c1081db3",
   "metadata": {},
   "source": [
    "## Part 8: Ray AIR BatchPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9302f394-a33b-45e2-b527-6c74aea58f90",
   "metadata": {},
   "source": [
    "|<img src=\"../../_static/assets/Scaling_inference/air_batchpredictor.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Using Ray AIR's `BatchPredictor` for Batch Inference.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ef6e96-f29e-4db9-be57-0aee104a34d9",
   "metadata": {},
   "source": [
    "### Implement Predictor for image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ae3c1e-31f6-4388-86a8-10ba84860f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.predictor import Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3024c0-f651-44f4-93f6-8964918b3bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSegmentationPredictor(Predictor):\n",
    "    def __init__(self, model, feature_extractor):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def _predict_pandas(self, batch):\n",
    "        image, labels = batch[\"value\"][0]\n",
    "\n",
    "        inputs = self.feature_extractor(\n",
    "            images=image, segmentation_maps=labels, return_tensors=\"pt\"\n",
    "        )\n",
    "        outputs = self.model(\n",
    "            pixel_values=inputs.pixel_values.to(self.device),\n",
    "            labels=inputs.labels.to(self.device),\n",
    "        )\n",
    "        loss = outputs.loss.detach().cpu().numpy()\n",
    "\n",
    "        upsampled_logits = torch.nn.functional.interpolate(\n",
    "            outputs.logits.cpu(),\n",
    "            size=image.size[::-1],\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        upsampled_logits = upsampled_logits.argmax(dim=1)[0]\n",
    "\n",
    "        df = pd.DataFrame(columns=[\"prediction\", \"loss\"])\n",
    "        df.loc[0, \"prediction\"] = upsampled_logits.detach().cpu().numpy()\n",
    "        df.loc[0, \"loss\"] = loss\n",
    "\n",
    "        return df\n",
    "\n",
    "    @classmethod\n",
    "    def from_checkpoint(self, checkpoint, **kwargs):\n",
    "        checkpoint_data = checkpoint.to_dict()\n",
    "        return SemanticSegmentationPredictor(\n",
    "            model=checkpoint_data[\"model\"],\n",
    "            feature_extractor=checkpoint_data[\"feature_extractor\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09cd7a2-3825-49c4-a4f6-74ba71859ad1",
   "metadata": {},
   "source": [
    "batch in `_predict_pandas` is DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898ffc2c-c712-40d3-a290-fa9e3ebc1910",
   "metadata": {},
   "source": [
    "https://docs.ray.io/en/latest/ray-air/predictors.html#batch-prediction\n",
    "\n",
    "https://docs.ray.io/en/latest/ray-air/package-ref.html#predictor\n",
    "\n",
    "Ray AIR Predictors are a class that loads models from Checkpoint to perform inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0790eeb-3207-4f8b-b520-e3751f93cf2b",
   "metadata": {},
   "source": [
    "### Implement BatchPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b73b9a-b04b-4ded-8322-fbc4fa2620a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.air import Checkpoint\n",
    "from ray.train.batch_predictor import BatchPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3118f64",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "batch_predictor = BatchPredictor(\n",
    "    checkpoint=Checkpoint.from_dict(\n",
    "        {\"model\": model, \"feature_extractor\": feature_extractor}\n",
    "    ),\n",
    "    predictor_cls=SemanticSegmentationPredictor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0509bf8-d124-4a6c-9720-82bf01b03faa",
   "metadata": {},
   "source": [
    "### Run batch inference on 100 images and assess scalability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74717154-8664-46b6-b54e-ec6d687d644f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = batch_predictor.predict(data=dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ca79ea-bfbf-4f72-a9fc-1013d3bdc7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf1a41b-0e05-427f-8cf2-ec393484bf16",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Summary: BatchPredictor\n",
    "\n",
    "|Compute       |10 image|100 images|1k images|10k images|\n",
    "|:-------------|:-------|:---------|:--------|:---------|\n",
    "|M1 MacBook Pro|1.3s    |13s       |125s     |n.a.      |\n",
    "|cluster 1 AWS |x.xxs   |.         |.        |.         |\n",
    "|cluster 2 AWS |x.xxs   |.         |.        |.         |\n",
    "\n",
    "#### Key Concepts\n",
    "\n",
    "#### Key API Elements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86ade2c-5e33-46af-804a-a10dd0e2bfc8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 9: Architectures for scalable batch inference with Ray - recap\n",
    "\n",
    "|<img src=\"../../_static/assets/Scaling_inference/task_inference.png\" width=\"100%\" loading=\"lazy\">|<img src=\"../../_static/assets/Scaling_inference/actor_inference.png\" width=\"100%\" loading=\"lazy\">|<img src=\"../../_static/assets/Scaling_inference/actor_pool.png\" width=\"100%\" loading=\"lazy\">|<img src=\"../../_static/assets/Scaling_inference/ray_datasets.png\" width=\"100%\" loading=\"lazy\">|<img src=\"../../_static/assets/Scaling_inference/air_batchpredictor.png\" width=\"100%\" loading=\"lazy\">|\n",
    "|:-:|:-:|:-:|:-:|:-:|\n",
    "|Ray Tasks|Ray Actors|`ActorPool`|`Dataset.map_batches()`|`BatchPredictor`|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe90b042-e31b-483e-8368-5d7b7aa1c761",
   "metadata": {},
   "source": [
    "# Connect with the Ray community\n",
    "\n",
    "You can learn and get more involved with the Ray community of developers and researchers:\n",
    "\n",
    "* [Ray documentation](https://docs.ray.io/en/latest)\n",
    "* [Official Ray Website](https://www.ray.io/): Browse the ecosystem and use this site as a hub to get the information that you need to get going and building with Ray.\n",
    "* [Join the Community on Slack](https://forms.gle/9TSdDYUgxYs8SA9e8): Find friends to discuss your new learnings in our Slack space.\n",
    "* [Use the Discussion Board](https://discuss.ray.io/): Ask questions, follow topics, and view announcements on this community forum.\n",
    "* [Join a Meetup Group](https://www.meetup.com/Bay-Area-Ray-Meetup/): Tune in on meet-ups to listen to compelling talks, get to know other users, and meet the team behind Ray.\n",
    "* [Open an Issue](https://github.com/ray-project/ray/issues/new/choose): Ray is constantly evolving to improve developer experience. Submit feature requests, bug-reports, and get help via GitHub issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5891f0e7-56f7-41c5-b130-6bbee6e878f8",
   "metadata": {},
   "source": [
    "<img src=\"../../_static/assets/Generic/ray_logo.png\" width=\"20%\" loading=\"lazy\">"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
