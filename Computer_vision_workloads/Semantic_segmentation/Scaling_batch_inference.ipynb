{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcb9a269-765a-46ef-abb6-d60682d38b0d",
   "metadata": {},
   "source": [
    "# Scaling Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b114120-a325-4597-bf7b-8310c9c8d776",
   "metadata": {},
   "source": [
    "<img src=\"../../_static/assets/Generic/ray_logo.png\" width=\"20%\" loading=\"lazy\">\n",
    "\n",
    "## About this notebook\n",
    "\n",
    "### Is it right for you?\n",
    "\n",
    "This module focuses on the batch inference task. It presents common design patterns for running batch inference and present few approaches how to implement batch inference with Ray depending on your needs. It is right for you if:\n",
    "\n",
    "* you work with model (batch) inference problems and you observe performance bottlenecks\n",
    "* you want to scale or increase throughput of your existing batch inference pipelines\n",
    "* you wish to explore different architectures for batch inference with Ray Core and Ray AIR\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "For this notebook you should have:\n",
    "\n",
    "* practical Python and machine learning experience\n",
    "* familiarity with batch inference problem in ML\n",
    "* familiarity with Ray and Ray AIR. Equivalent to completing these training modules:\n",
    "  * [Overview of Ray](https://github.com/ray-project/ray-educational-materials/blob/main/Introductory_modules/Overview_of_Ray.ipynb)\n",
    "  * [Introduction to Ray AIR](https://github.com/ray-project/ray-educational-materials/blob/main/Introductory_modules/Introduction_to_Ray_AIR.ipynb)\n",
    "  * [Ray Core](https://github.com/ray-project/ray-educational-materials/tree/main/Ray_Core)\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "Upon completion of this notebook, you will know about:\n",
    "\n",
    "* batch inference patterns\n",
    "* how to implement scalable batch inference with Ray\n",
    "\n",
    "### What will you do?\n",
    "\n",
    "* learn about scaling inference with common design patterns\n",
    "* explore different architectures for predicting on semantic segmentation tasks\n",
    "* implement paralleized inference through hands-on coding exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a02709-0ed5-489a-b344-27005261eb72",
   "metadata": {},
   "source": [
    "## Part 1: (Ray) architectures for scalable batch inference\n",
    "\n",
    "The end goal for machine learning models is to generate performant predictions over a set of unseen data. In this module, you will approach parallelizing batch inference on using Ray Core's API as well as the high-level abstractions available in Ray AI Runtime.\n",
    "\n",
    "|<img src=\"../../_static/assets/Scaling_inference/ml_workflow.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Example of a machine learning workflow.|\n",
    "\n",
    "### Stateless inference - Ray Tasks\n",
    "\n",
    "Loading complex models into memory can be expensive and sequential processing of requests limits speed. *Stateless inference* allows an ML system to handle high volume requests by:\n",
    "\n",
    "1. exporting the model's mathematical core into a language agnostic format\n",
    "2. restoring the architecture and weights of a trained model in a stateless function (i.e. Ray tasks)\n",
    "\n",
    "A Ray task is *stateless* because its output (e.g. predictions) is determined purely by its inputs (e.g. the trained model). Performing online inference involves loading the model for every request and synchronously serving results.\n",
    "\n",
    "|<img src=\"../../_static/assets/Scaling_inference/task_inference.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Stateless inference using Ray Tasks.|\n",
    "\n",
    "In the figure above, you perform batch inference by preprocessing your big dataset into batches that are assigned to workers via Ray tasks. Each task loads the trained model and outputs predictions on batches as they are assigned.\n",
    "\n",
    "**Code Snippet**:\n",
    "\n",
    "```python\n",
    "object_refs = [task.remote(input) for _ in range(10)]\n",
    "```\n",
    "\n",
    "#### Stateful inference - Ray Actors\n",
    "\n",
    "When your deployed model takes too long to generate immediate results, online prediction may not be the right approach. In addition, some situations require predictions to be generated over large volumes of data such as curating personalized playlists. You can use *batch inference*, which is an asynchronous method of batching observations for prediction in advance to process a high volume of samples efficiently.\n",
    "\n",
    "Setting up distributed batch inference with Ray involves:\n",
    "\n",
    "1. creating a number of replicas of your model; in Ray, these replicas are represented as Actors (i.e., stateful processes) that can be assigned to GPUs and hold instantiated model objects\n",
    "\n",
    "2. feeding data into these model replicas in parallel, and retrieve inference results\n",
    "\n",
    "|<img src=\"../../_static/assets/Scaling_inference/actor_inference.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Stateful inference using Ray Actors.|\n",
    "\n",
    "Much like stateless inference using Ray tasks, stateful inference replaces Ray tasks with Ray actors and leverages Ray's object store to avoid loading the model for every batch.\n",
    "\n",
    "**Code Snippet**:\n",
    "\n",
    "```python\n",
    "actors = [ActorCls.remote(input) for _ in range(10)]\n",
    "```\n",
    "\n",
    "#### Ray ActorPool - Increment of the previous approach - utility lib.\n",
    "\n",
    "Ray provides a convenient [ActorPool utility](https://docs.ray.io/en/latest/ray-core/package-ref.html#ray-util-actorpool) which wraps the above list of actors to avoid futures management.\n",
    "\n",
    "|<img src=\"../../_static/assets/Scaling_inference/actor_pool.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Using Actor Pools for Batch Inference.|\n",
    "\n",
    "Building off of the stateful inference diagram, an Actor Pool wraps around the `n` actors so you do not have to manage idle actors and manually distribute workloads.\n",
    "\n",
    "**Code Snippet**:\n",
    "\n",
    "```python\n",
    "from ray.util.actor_pool import ActorPool\n",
    "actor_pool = ActorPool(actors)\n",
    "```\n",
    "\n",
    "#### Ray AIR Datasets\n",
    "\n",
    "Ray Datasets allows for parallel reading and preprocessing of source data along with autoscaling of the ActorPool. As a part of Ray AIR, you specify what you want done through a set of declarative key-value arguments rather than concerning yourself with how to instruct Ray to scale.\n",
    "\n",
    "|<img src=\"../../_static/assets/Scaling_inference/ray_datasets.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Ray Datasets replace the 'Batch preprocessing' stage.|\n",
    "\n",
    "In Ray AIR, a trained model is loaded into a `Checkpoint` object (could be from training or tuning). An AIR `Predictor` loads model from the `Checkpoint` to perform inference. Then, using the preprocessed batches provided by Ray Datasets, you extract predictions off of the testing data.\n",
    "\n",
    "**Code Snippet**:\n",
    "\n",
    "```python\n",
    "batches = data.map_batches(\n",
    "              MyModel,\n",
    "              num_gpus=1,\n",
    "              batch_size-1024,\n",
    "              compute=ray.data.ActorPoolStrategy(min_size=10, max_size=50)\n",
    "          )\n",
    "```\n",
    "\n",
    "#### Ray AIR BatchPredictor\n",
    "\n",
    "Ray AIR's [`BatchPredictor`](https://docs.ray.io/en/latest/ray-air/package-ref.html#batch-predictor) takes in a [`Checkpoint`](https://docs.ray.io/en/latest/ray-air/package-ref.html#checkpoint) which represents the saved model. This high-level abstraction offers simple and composable APIs that enable preprocessing data in batches with [BatchMapper](https://docs.ray.io/en/latest/ray-air/package-ref.html#generic-preprocessors) and instantiate a distributed predictor given checkpoint data.\n",
    "\n",
    "|<img src=\"../../_static/assets/Scaling_inference/air_batchpredictor.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Using Ray AIR's `BatchPredictor` for Batch Inference.|\n",
    "\n",
    "Finally, you can use an AIR `BatchPredictor` that takes both the `Checkpoint` and `Predictor` to replace the process of manually performing inference on a large dataset.\n",
    "\n",
    "**Code Snippet**:\n",
    "\n",
    "```python\n",
    "batch_predictor = BatchPredictor(\n",
    "                      Checkpoint,\n",
    "                      Predictor\n",
    "                  )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf547df-6663-47e4-99df-de54ac0cd5ef",
   "metadata": {},
   "source": [
    "## Part 2: Notes on data and model\n",
    "\n",
    "### Data\n",
    "\n",
    "Image segmentation takes a scene and classifies image objects [into semantic categories](https://docs.google.com/spreadsheets/d/1se8YEtb2detS7OuPE86fXGyD269pMycAWe2mtKUj2W8/edit?usp=sharing) pixel-by-pixel. [MIT ADE20K Dataset](http://sceneparsing.csail.mit.edu/) (SceneParse150) provides the largest open source dataset for scene parsing, and in this notebook, you will be scaling inference on image regions depicted in these samples.\n",
    "\n",
    "|<img src=\"../../_static/assets/Scaling_inference/scene.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Test image on the left vs. predicted result on the right.[Source](https://github.com/CSAILVision/semantic-segmentation-pytorch) *Date accessed: November 10, 2022*|\n",
    "\n",
    "**Dataset Highlights**\n",
    "\n",
    "- 20k annotated, scene-centric training images\n",
    "- 2k validation images\n",
    "- 150 total categories such as person, car, bed, sky, and more\n",
    "\n",
    "### Model\n",
    "\n",
    "[SegFormer](https://arxiv.org/pdf/2105.15203.pdf) is a simple and powerful semantic segmentation method whose architecture consists of a hierarchical Transformer encoder and a lightweight All-MLP decoder. What sets SegFormer apart from previous approaches boils down to two key features:\n",
    "\n",
    "1. a novel hierarchically structured Transformer encoder which does not depend on positional encoding, avoiding interpolation when test resolution differs from training\n",
    "2. avoids complex decoders\n",
    "\n",
    "With demonstrated success on benchmarks such as Cityscapes and [MIT ADE20K Dataset](http://sceneparsing.csail.mit.edu/), you will use a pretrained version to perform inference on test images from the SceneParse 150 dataset.\n",
    "\n",
    "|<img src=\"../../_static/assets/Scaling_inference/segformer_architecture.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Segformer architecture taken from [original paper](https://arxiv.org/pdf/2105.15203.pdf). *Date accessed: November 10, 2022*|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c018d564-0e4a-453f-9f7a-74466c0273e8",
   "metadata": {},
   "source": [
    "## Part 3: Sequential batch inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb5419c-26af-4b09-8fd2-e885aaa60021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e155a5b3-d37b-45d6-b9d2-74720b94b5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(201)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a7f1cb-4009-4848-b589-974f82285963",
   "metadata": {},
   "source": [
    "### Load pre-trained model from the HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e32feab-170b-4220-a263-bb678592d106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_labels\n",
    "from transformers import SegformerForSemanticSegmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4346c5a0-36b4-46ce-bdc0-3102543d2dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"nvidia/segformer-b0-finetuned-ade-512-512\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0987413-f1d7-4f97-a0a9-ba583b593631",
   "metadata": {},
   "source": [
    "model finetuned on the 512x512 dataset\n",
    "\n",
    "https://huggingface.co/nvidia/segformer-b0-finetuned-ade-512-512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c27154-9930-466e-b89f-45cbe487a690",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label, label2id = get_labels()\n",
    "print(f\"total labels: {len(id2label)}\")\n",
    "print(f\"example lables: {list(id2label.values())[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9354716-8ac2-4e4f-97e1-e48ae174f0d9",
   "metadata": {},
   "source": [
    "Next, we download the mappings from the Hub (using the huggingface_hub library) and create two dictionaries, namely id2label and label2id. We use these dictionaries to map (bidirectionally) between label IDs (int) and the labels (string) of the respective images. There are a total of 150 labels, or categories in for this dataset, and we print 10 of them at the end. Since we're parsing scenes and want to classify segments of natural images, you can see that the labels describe entities such as walls, floors, roads or grass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab7f82a-93e7-44c3-ac0e-322ebcdfd443",
   "metadata": {},
   "source": [
    "#### Load SegFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aec6f8-22bf-41f9-bd7b-fe54889d762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SegformerForSemanticSegmentation.from_pretrained(MODEL_NAME, id2label=id2label, label2id=label2id)\n",
    "print(f\"number of model parameters: {model.num_parameters()/(10**6):.2f} M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41546abd-2f3d-4128-8176-19cd0604d6f4",
   "metadata": {},
   "source": [
    "#### Create feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f888ed08-cde2-4f4a-b05f-e467de4b7ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"reduce_labels\" is to drop background from loss compute: https://huggingface.co/docs/transformers/model_doc/segformer#segformer\n",
    "from transformers import SegformerFeatureExtractor\n",
    "feature_extractor = SegformerFeatureExtractor.from_pretrained(MODEL_NAME, reduce_labels=True)\n",
    "feature_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3109152b-3572-40f9-8fc6-b04ecce59896",
   "metadata": {},
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d58d4b-d13c-4c34-8e80-194c8f6bdae9",
   "metadata": {},
   "source": [
    "#### Load dataset from the HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70f31fe-e8a9-41ea-857c-e2e2b36503e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from Hugging Face\n",
    "from datasets import load_dataset\n",
    "\n",
    "DATASET_NAME = \"scene_parse_150\" # name of the dataset on the HuggingFace's datasets repository.\n",
    "\n",
    "# split here only for fast-debug, remove before real use.\n",
    "# ds = load_dataset(DATASET_NAME, split=\"train[:50]\")  # for dry run only\n",
    "dataset_dict = load_dataset(DATASET_NAME)\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6e66d8-2587-43c9-91bb-cdfc6f22b33d",
   "metadata": {},
   "source": [
    "This can take some time, because you download data - over 20k images to the local machine or cluster.\n",
    "\n",
    "We start be downloading the SceneParse150 dataset using Hugging Face's `datasets` library. Specifically, we're going to leverage the `load_dataset` utility to reference this dataset as a string (`\"scene_parse_150\"`). Note that this can take a couple of minutes. We also specify a `split` argument so that we can access `train` and `test` data on the resulting dataset `ds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98814d17-d57d-4bdd-ba1c-8e64880f6361",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = dataset_dict[\"train\"]\n",
    "test_ds = dataset_dict[\"test\"]\n",
    "\n",
    "print(f\"train_ds\\n{train_ds}\\n\")\n",
    "print(f\"test_ds\\n{test_ds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2a63ad-3bfd-453c-827e-6a0f664ac310",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Display example images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c22f28-f63c-42e8-ab43-1684aa692d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import display_example_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1054dfa-90a6-4569-bc0f-cb059f4b4893",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_example_images(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c6e51b",
   "metadata": {},
   "source": [
    "Each Hugging Face dataset comes with a `train_test_split` method that we're going to use next. We want 80% of the data to be training data, and 20% held back for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2394456",
   "metadata": {},
   "source": [
    "To get a feel for what this dataset consists of, let's print the first of it. Since the train-test split we did is randomized, the resulting image will be different every time you load the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85464a2-d42f-45cb-907f-c206eecfc999",
   "metadata": {},
   "source": [
    "### Run inference on example images and visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70115952-fdb0-40d3-87d0-a7903502a289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import visualize_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba82c68-f3cd-46ae-8b23-6fee43ccb3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def predict(model, image, device=dev, labels=None):\n",
    "    if labels is not None:\n",
    "        inputs = feature_extractor(images=image, segmentation_maps=labels, return_tensors=\"pt\")\n",
    "        outputs = model(pixel_values=inputs.pixel_values.to(device), labels=inputs.labels.to(device))\n",
    "        loss = outputs.loss.cpu()\n",
    "    else:\n",
    "        inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "        outputs = model(pixel_values=inputs.pixel_values.to(device))\n",
    "        loss = None\n",
    "\n",
    "    upsampled_logits = torch.nn.functional.interpolate(\n",
    "        outputs.logits.cpu(),\n",
    "        size=image.size[::-1],\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "    return upsampled_logits.argmax(dim=1)[0], loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5500ec-cafc-4c27-a3ca-02ac093bf8c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Run inference on train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b9f0f1-be22-4910-a5a1-e6d861cf3a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = np.random.randint(train_ds.num_rows)\n",
    "\n",
    "random_image = train_ds[j][\"image\"]\n",
    "labels = train_ds[j][\"annotation\"]\n",
    "\n",
    "segmentation, loss = predict(model=model, image=random_image, labels=labels)\n",
    "\n",
    "visualize_predictions(image=random_image, predictions=segmentation, loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654d534d-2263-41b8-b921-7588d06c0ae1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Run inference on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07085791-5f01-433e-8fc9-c7dd7bccbc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = np.random.randint(test_ds.num_rows)\n",
    "\n",
    "random_image = test_ds[j][\"image\"]\n",
    "labels = test_ds[j][\"annotation\"]\n",
    "\n",
    "segmentation, _ = predict(model=model, image=random_image)\n",
    "\n",
    "visualize_predictions(image=random_image, predictions=segmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7243ec-d207-4b33-89ca-b80309c19c06",
   "metadata": {},
   "source": [
    "### Run sequential batch inference on data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b58232-7578-4c5c-854c-7c0c980c7d2a",
   "metadata": {},
   "source": [
    "|<img src=\"../../_static/assets/Scaling_inference/single_seq_timeline.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Timeline of batch inference on one worker.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ed4236-c68a-4622-a044-735bc2c9342f",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = np.random.randint(train_ds.num_rows)\n",
    "\n",
    "random_image = train_ds[j][\"image\"]\n",
    "labels = train_ds[j][\"annotation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021487a1-de67-4e76-96e9-998796638662",
   "metadata": {},
   "source": [
    "#### Single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3902edad-6839-4b09-b3db-832bff0c8c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "segmentation, loss = predict(model=model, image=random_image, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373c05ba-799a-4eb6-a7f6-c879bcbdd336",
   "metadata": {},
   "source": [
    "Note: wall time on the M1 MacBook Pro: 0.45s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa7df14-25a9-40ab-8bf0-05588af9883b",
   "metadata": {},
   "source": [
    "#### 10 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6f6fad-d7be-40da-80ef-094f822bd556",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_image_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb4337e-28c2-4f5a-bf23-638137b2649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "images_ids = get_image_ids(dataset=train_ds, n_ids=10)\n",
    "images_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a576c5-5af3-4791-a767-da2fe366f439",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for i in images_ids:\n",
    "    image = train_ds[i][\"image\"]\n",
    "    labels = train_ds[i][\"annotation\"]\n",
    "    segmentation, loss = predict(model=model, image=image, labels=labels)\n",
    "    predictions.append((segmentation, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5efa01f-c36e-4093-926f-24847a99ac24",
   "metadata": {},
   "source": [
    "Note: wall time on the M1 MacBook Pro: 4.7s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d907fd2a-9a44-4011-883b-21a2d7fc4cc5",
   "metadata": {},
   "source": [
    "#### 100 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94af6f27-8ceb-4dea-8be5-aa19501d274d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "images_ids = get_image_ids(dataset=train_ds, n_ids=100)\n",
    "images_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa1618b-c37c-4e4b-a2cb-a03c793292b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for i in images_ids:\n",
    "    image = train_ds[i][\"image\"]\n",
    "    labels = train_ds[i][\"annotation\"]\n",
    "    segmentation, loss = predict(model=model, image=image, labels=labels)\n",
    "    predictions.append((segmentation, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebe7e04-1177-4f6d-8d6c-f3cce867d22e",
   "metadata": {},
   "source": [
    "Note: wall time on the M1 MacBook Pro: 53s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33c74b3-9d0f-469f-83ec-7e5557104489",
   "metadata": {},
   "source": [
    "## Part 4: Stateless inference - Ray Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c97168",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "if ray.is_initialized:\n",
    "    ray.shutdown()\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abbe8d8-f00d-4ba8-a81f-aa0a5f480854",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ref = ray.put(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e695c89f",
   "metadata": {},
   "source": [
    "The most naive version of parallelising prediction is to create Ray tasks that load the trained model internally when called. This way we can make the prediction task \"stateless\", but at the cost of incurring the overhead of loading the model every single time. This is akin to what serverless solutions like AWS Lambda would do, and this pattern could be worth it for tiny models, for which the application doesn't get bottle-necked by the model loading step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946db436-ac8b-4ac9-8dca-e557dbd126fb",
   "metadata": {},
   "source": [
    "### Prepare remote function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797b9e38",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def prediction_task(model, image, labels):\n",
    "    return predict(model=model, image=image, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dedddda-42f6-4db6-8829-1ec65bcea9e3",
   "metadata": {},
   "source": [
    "### 10 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f8ced4-da67-4b91-9be0-46891aa07158",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_ids = get_image_ids(dataset=train_ds, n_ids=10)\n",
    "images_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c999e2b5-f18d-4a36-b7c9-9577553ffe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "prediction_refs = []\n",
    "for i in images_ids:\n",
    "    task_ref = prediction_task.remote(model=model_ref,\n",
    "                                      image=train_ds[i][\"image\"],\n",
    "                                      labels=train_ds[i][\"annotation\"])\n",
    "    prediction_refs.append(task_ref)\n",
    "\n",
    "print(*prediction_refs, sep=\"\\n\")\n",
    "\n",
    "predictions = ray.get(prediction_refs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cba2f68-c762-439e-850b-3af83ba1eaae",
   "metadata": {},
   "source": [
    "|<img src=\"../../_static/assets/Scaling_inference/task_inference.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Stateless inference using Ray Tasks.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fe57e0-14f9-4642-8a07-087c308c296e",
   "metadata": {},
   "source": [
    "### 100 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572f87fa-ec0f-482e-9830-9bdc667c4a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_ids = get_image_ids(dataset=train_ds, n_ids=100)\n",
    "print(len(images_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a5e447-c6e7-4a36-bfd0-4fd37d5b2011",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "prediction_refs = []\n",
    "for i in images_ids:\n",
    "    task_ref = prediction_task.remote(model=model_ref,\n",
    "                                      image=train_ds[i][\"image\"],\n",
    "                                      labels=train_ds[i][\"annotation\"])\n",
    "    prediction_refs.append(task_ref)\n",
    "\n",
    "predictions = ray.get(prediction_refs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c89723-bf39-4a91-8900-74201b989940",
   "metadata": {},
   "source": [
    "Note: wall time on the M1 MacBook Pro: 13s.\n",
    "\n",
    "Distribute batch inference yields **4x performance gain**\n",
    "\n",
    "* Parallel: 13s.\n",
    "* Sequential: 53s.\n",
    "\n",
    "*(experiment on the M1 MacBook Pro)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c638ed2a-e096-48f2-9aa5-793e9091a5c0",
   "metadata": {},
   "source": [
    "### 1000 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f6971b-f489-4941-9117-97507705f869",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_ids = get_image_ids(dataset=train_ds, n_ids=1000)\n",
    "len(images_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516154b8-8b39-41b8-8313-64fbd9d6ede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "prediction_refs = []\n",
    "for i in images_ids:\n",
    "    task_ref = prediction_task.remote(model=model_ref,\n",
    "                                      image=train_ds[i][\"image\"],\n",
    "                                      labels=train_ds[i][\"annotation\"])\n",
    "    prediction_refs.append(task_ref)\n",
    "\n",
    "predictions = ray.get(prediction_refs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905e0827-7b40-4c36-a704-1296ecb4f8bf",
   "metadata": {},
   "source": [
    "Note: wall time on the M1 MacBook Pro: 125s.\n",
    "\n",
    "Average speed per prediction is 0.125s. That yields 4x performance speedup, when compared to the sequential approach, which is approximately 0.45s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e2f9e2-945b-4b08-95ec-74b09b8b07d5",
   "metadata": {},
   "source": [
    "## Part 5: Stateful inference - Ray Actors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff1bb7d-b8f9-4a2d-89d2-01446e114525",
   "metadata": {},
   "source": [
    "|<img src=\"../../_static/assets/Scaling_inference/actor_inference.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Stateful inference using Ray Actors.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70563f89-c95c-4ee8-8374-0288d918d5fe",
   "metadata": {},
   "source": [
    "### Prepare Ray Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4030c8a1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class PredictionActor:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def predict(self, image, labels=None):\n",
    "        if labels is not None:\n",
    "            inputs = feature_extractor(images=image, segmentation_maps=labels, return_tensors=\"pt\")\n",
    "            outputs = self.model(pixel_values=inputs.pixel_values.to(self.device), labels=inputs.labels.to(self.device))\n",
    "            loss = outputs.loss.cpu()\n",
    "        else:\n",
    "            inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "            outputs = self.model(pixel_values=inputs.pixel_values.to(self.device))\n",
    "            loss = None\n",
    "\n",
    "        upsampled_logits = torch.nn.functional.interpolate(\n",
    "            outputs.logits.cpu(),\n",
    "            size=image.size[::-1],\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False)\n",
    "\n",
    "        return upsampled_logits.argmax(dim=1)[0], loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f910cd41",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "N_ACTORS = 3\n",
    "\n",
    "idle_actors = []\n",
    "for i in range(N_ACTORS):\n",
    "    idle_actors.append(PredictionActor.remote(model=model_ref))\n",
    "\n",
    "idle_actors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61df3db9-9711-478c-be8d-e41baaa1bb0e",
   "metadata": {},
   "source": [
    "### 100 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9043010c-64bf-446f-aaf3-ff1fdaea6ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463f5a88-e6e2-4a2e-84eb-120bf8f4aa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_ids = get_image_ids(dataset=train_ds, n_ids=100)\n",
    "data = [(train_ds[i][\"image\"], train_ds[i][\"annotation\"]) for i in images_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a80f96-3561-4435-8661-1b989c22692e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_results_postprocessing(results, predictions):\n",
    "    predictions.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206e0f4b-a0f8-4e10-a3cd-0358c2649170",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_to_actor_mapping = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbce1966-c4be-406e-8df1-4ff1650b531b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "while data:\n",
    "    if idle_actors:\n",
    "        actor = idle_actors.pop()\n",
    "        image, labels = data.pop()\n",
    "        future = actor.predict.remote(image=image, labels=labels)\n",
    "        future_to_actor_mapping[future] = actor\n",
    "    else:\n",
    "        [ready], _ = ray.wait(list(future_to_actor_mapping.keys()), num_returns=1)\n",
    "        actor = future_to_actor_mapping.pop(ready)\n",
    "        idle_actors.append(actor)\n",
    "        prediction_results_postprocessing(ray.get(ready), preds)\n",
    "\n",
    "# Process any leftover results at the end.\n",
    "for future in future_to_actor_mapping.keys():\n",
    "    prediction_results_postprocessing(ray.get(future), preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df3c873-5617-43d9-8ac0-31319080de23",
   "metadata": {},
   "source": [
    "based on this: https://docs.ray.io/en/master/ray-core/patterns/limit-pending-tasks.html\n",
    "\n",
    "ray.wait() -> https://docs.ray.io/en/latest/ray-core/package-ref.html#ray-wait"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeef5f3",
   "metadata": {},
   "source": [
    "|<img src=\"../../_static/assets/Scaling_inference/seq_timeline.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Timeline of sequential batch assignment spread across three workers.|\n",
    "\n",
    "|<img src=\"../../_static/assets/Scaling_inference/distrib_timeline.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Timeline of distributed bath inference where a scheduler orchestrates batch assignment as soon as a worker is available.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7036efa3-2dd1-4b55-82e3-da81a0a33f13",
   "metadata": {},
   "source": [
    "## Part 6: Stateful inference - Ray ActorPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9797d0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from ray.util.actor_pool import ActorPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756bce4f-a8ed-45c4-8f28-a44d5e4c16db",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ACTORS = 3\n",
    "\n",
    "actors = [PredictionActor.remote(model=model_ref) for _ in range(N_ACTORS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb97c5e3-0cec-41b2-ad44-0a5e4dda0b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_pool = ActorPool(actors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fa01e6-0b71-4d0f-8299-5331af4b7ba8",
   "metadata": {},
   "source": [
    "### 100 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0d3698-6901-4444-9e88-89a28d60cdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c3d855-5480-462c-811d-62a402fd2f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_ids = get_image_ids(dataset=train_ds, n_ids=100)\n",
    "data = [(train_ds[i][\"image\"], train_ds[i][\"annotation\"]) for i in images_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50e7b08",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def actor_call(actor, data_item):\n",
    "    image, labels = data_item\n",
    "    return actor.predict.remote(image=image, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaacec5d-1e99-40bc-b160-642c2a32b554",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for result in actor_pool.map_unordered(actor_call, data):\n",
    "    prediction_results_postprocessing(result, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5730b12c-c5e2-4935-8b06-d0d01b36e3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7289f04c-337c-47e1-a415-a5c5d80b61cc",
   "metadata": {},
   "source": [
    "## Part 7: Ray AIR Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a31a72",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class PredictionClass:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        prediction = predict(self.model, batch)\n",
    "        # TODO: for some reason we are not allowed to return arbitrary return values here, needs to be numpy, list, etc.\n",
    "        return prediction.cpu().detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daec098",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dataset = ray.data.from_items(data)\n",
    "dataset.show()\n",
    "\n",
    "# ToDo: pre-processing -> dataset of baches of images, not individual images. Probably is has to be batches of pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87397fbd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "results = dataset.map_batches(\n",
    "    PredictionClass,\n",
    "    batch_size=1,\n",
    "    num_gpus=0,\n",
    "    compute=ray.data.ActorPoolStrategy(min_size=1, max_size=5),\n",
    "    fn_constructor_args=(model_ref,)\n",
    ")\n",
    "\n",
    "results.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460ac28a-3739-4456-99a3-1579c1081db3",
   "metadata": {},
   "source": [
    "## Part 8: Ray AIR BatchPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3118f64",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from ray.air import Checkpoint\n",
    "from ray.train.predictor import Predictor\n",
    "from ray.train.batch_predictor import BatchPredictor\n",
    "import pandas as pd\n",
    "\n",
    "# this is a hack, don't use the same image in the predictor.\n",
    "image = original_test_ds[0][\"image\"]\n",
    "\n",
    "\n",
    "# https://docs.ray.io/en/latest/ray-air/predictors.html#batch-prediction\n",
    "# adapt it to batch prediction on images\n",
    "class CustomPredictor(Predictor):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        self.model = load_trained_model(model_name)\n",
    "\n",
    "    def _predict_pandas(self, batch):\n",
    "        # TODO: figure out how to make this run on pandas properly.\n",
    "        # ... why are we forced to use pandas, though?\n",
    "        prediction = predict(self.model, image)\n",
    "\n",
    "        # ToDo: can we work with numpy here?\n",
    "        # implement post processing to cast prediction to pandas\n",
    "\n",
    "        return pd.DataFrame(prediction)\n",
    "\n",
    "    @classmethod\n",
    "    def from_checkpoint(cls, checkpoint, **kwargs):\n",
    "        return CustomPredictor(checkpoint.to_dict()[\"model\"])\n",
    "\n",
    "predictor = BatchPredictor(\n",
    "    checkpoint=Checkpoint.from_dict({\"model\": MODEL}),\n",
    "    predictor_cls=CustomPredictor,\n",
    "    preprocessor=None,\n",
    ")\n",
    "\n",
    "results = predictor.predict(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
