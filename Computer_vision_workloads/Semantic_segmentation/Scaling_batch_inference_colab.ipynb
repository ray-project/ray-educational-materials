{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcb9a269-765a-46ef-abb6-d60682d38b0d",
   "metadata": {},
   "source": [
    "# Scalable Batch Inference with Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b114120-a325-4597-bf7b-8310c9c8d776",
   "metadata": {},
   "source": [
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Generic/ray_logo.png\" width=\"20%\" loading=\"lazy\">\n",
    "\n",
    "## About this notebook\n",
    "\n",
    "### Is this module right for you?\n",
    "\n",
    "This module presents several approaches for scaling batch inference on Ray. Through hands-on practice with inference on a computer vision task, you will implement and compare different inference architectures to better understand Ray AIR and Ray Core.\n",
    "\n",
    "To get the most out of this notebook, the following scenarios may apply to you:\n",
    "\n",
    "* You observe performance bottlenecks when working on batch inference problems in computer vision projects.\n",
    "* You want to scale or increase throughput of existing batch inference pipelines.\n",
    "* You wish to explore different architectures for scaling batch inference with Ray AIR and Ray Core.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "For this notebook you should satisfy the following requirements:\n",
    "\n",
    "* Practical Python and machine learning experience.\n",
    "* Familiarity with batch inference in ML.\n",
    "* Familiarity with Ray and Ray AIR equivalent to completing these training modules:\n",
    "  * [Overview of Ray](https://github.com/ray-project/ray-educational-materials/blob/main/Introductory_modules/Overview_of_Ray.ipynb)\n",
    "  * [Introduction to Ray AIR](https://github.com/ray-project/ray-educational-materials/blob/main/Introductory_modules/Introduction_to_Ray_AIR.ipynb)\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "* Understand common design patterns for distributed batch inference.\n",
    "* Implement scalable batch inference with Ray.\n",
    "* Extend each approach by tuning performance.\n",
    "* Compare scalable batch inference architectures on Ray to evaluate which is most relevant to your work.\n",
    "\n",
    "### What will you do?\n",
    "\n",
    "* Learn about three distributed batch inference design patterns with Ray.\n",
    "* Get to know the inference task.\n",
    "  * Semantic (image) segmentation using the SegFormer model.\n",
    "* Implement sequential inference.\n",
    "* Implement distributed inference patterns.\n",
    "  * Inference with Ray AIR Datasets and **BatchPredictor** abstractions.\n",
    "* Compare approaches to identify situations best fit for each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a02709-0ed5-489a-b344-27005261eb72",
   "metadata": {},
   "source": [
    "## Part 1: Ray design patterns for scaling batch inference\n",
    "\n",
    "The ultimate goal for machine learning models is often to generate predictions on a set of unseen data. In this notebook, you focus on the inference stage of the ML workflow and explore different approaches to scaling it.\n",
    "\n",
    "Ray Core and Ray AIR provide APIs that allow you to perform batch inference at scale, processing millions of examples and offering various performance tuning options.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Scaling_inference/example_ml_workflow.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|An example of a machine learning workflow that starts with reading raw data and preprocessing it. These steps are followed by training and tuning that produce a trained model. This model is then used for inference, often on large datasets.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2450181f",
   "metadata": {},
   "source": [
    "### What is (batch) inference?\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong>Batch inference</strong> (also known as offline inference): is the process of generating predictions on a large set or \"batch\" of data.\n",
    "</div>\n",
    "\n",
    "Unlike *online inference* where predictions are generated as each observation is produced, batch inference generates predictions over a large number of input data when immediate response is not required or feasible. \n",
    "\n",
    "For example, batch inference is relevant when generating weekly product recommendations using historical customer data or sales forecasting using time-aggregated observations.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Scaling_inference/batch_inference.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Batch inference is the process of applying a trained model to a batch of data to generate predictions.|\n",
    "\n",
    "In a non-distributed setting, inference executes sequentially. The model processes incoming batches of data one at a time, limiting performance to a single machine or GPU. Below, you will learn about three approaches for distributing batch inference on Ray."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c500d89-bdae-41fd-9b12-264c0839af90",
   "metadata": {},
   "source": [
    "### Batch inference using Ray AIR BatchPredictor\n",
    "\n",
    "Ray AIR [BatchPredictor](https://docs.ray.io/en/latest/ray-air/predictors.html#batch-prediction) is a utility for large-scale, distributed batch inference. `BatchPredictor` has out-of-the-box features:\n",
    "\n",
    "* supports various predictors like [TorchPredictor](https://docs.ray.io/en/latest/ray-air/api/doc/ray.train.torch.TorchPredictor.html#ray.train.torch.TorchPredictor), [HuggingFacePredictor](https://docs.ray.io/en/latest/ray-air/api/doc/ray.train.huggingface.HuggingFacePredictor.html#ray.train.huggingface.HuggingFacePredictor) or [XGBoostPredictor](https://docs.ray.io/en/latest/ray-air/api/doc/ray.train.xgboost.XGBoostPredictor.html#ray.train.xgboost.XGBoostPredictor))\n",
    "* it handles framework-native batch conversions\n",
    "* it has options to resume operations from AIR checkpoint to prediction, selection / keep columns, etc.\n",
    "\n",
    "`BatchPredictor` takes in two components:\n",
    "\n",
    "* **`Checkpoint`**. A trained model, could be from training or tuning step.\n",
    "* **`Predictor`**. A class that loads models from `Checkpoint` to perform inference; supports framework-specific predictors (e.g. TorchPredictor and TensorflowPredictor).\n",
    "\n",
    "Once instantiated, BatchPredictor can call `predict()` on a Ray Dataset. [Ray Datasets](https://docs.ray.io/en/latest/data/dataset.html#datasets) are the standard way to load and exchange data in Ray AIR. Datasets load and preprocess data for parallel compute, internally handling operations like batching, pipelining, autoscaling the actor pool, and memory management.\n",
    "\n",
    "\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Scaling_inference/air_batchpredictor.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Ray Datasets parallelize data loading, preprocessing, and batching. Ray AIR `BatchPredictor` takes both `Checkpoint` and `Predictor` objects to call `predict()` on a Ray Dataset for distributed batch inference.|\n",
    "\n",
    "These high-level abstractions automate the challenging aspects of scaling batch inference in exchange for less direct control over the way Ray distributes.\n",
    "\n",
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Scaling_inference/code_batchpredictor.png\" width=\"70%\" loading=\"lazy\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee761b0-7549-46ca-99bc-e3588a7a1deb",
   "metadata": {},
   "source": [
    "## Part 2: Batch inference example using computer vision transformers\n",
    "\n",
    "To demonstrate the three design patterns introduced in the previous section, you will apply each approach on a computer vision task: semantic segmentation.\n",
    "\n",
    "Semantic segmentation, similar to object detection, involves assigning labels to objects in a scene pixel-by-pixel. In this hands-on example, you will run batch inference on image data by using a pretrained model to generate predictions.\n",
    "\n",
    "### Data\n",
    "\n",
    "#### MIT ADE20K - scene parsing benchmark\n",
    "\n",
    "The [MIT ADE20K Dataset](http://sceneparsing.csail.mit.edu/) (also known as \"SceneParse150\") provides the largest open source dataset for scene parsing. It is often used as a standard for assessing semantic segmentation model performance due to its high-quality annontations. For this example, you will use the unlabeled test data to implement different batch inference architectures.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Scaling_inference/scene.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Unannotated scene image from MITADE20K on the left. Pixel-by-pixel predictions on the right. [*Date accessed: November 10, 2022*](https://github.com/CSAILVision/semantic-segmentation-pytorch)|\n",
    "\n",
    "Dataset highlights\n",
    "\n",
    "* 20k annotated, scene-centric training images\n",
    "* 3.3k unlabeled test images\n",
    "* 150 [semantic categories](https://docs.google.com/spreadsheets/d/1se8YEtb2detS7OuPE86fXGyD269pMycAWe2mtKUj2W8/edit?usp=sharing) (such as person, car, bed, sky, etc.)\n",
    "\n",
    "### Model\n",
    "\n",
    "#### SegFormer - transformer-based framework for semantic segmentation\n",
    "\n",
    "[SegFormer](https://arxiv.org/pdf/2105.15203.pdf) is an effective semantic segmentation method based on a *transformer* architecture. [Transformers](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) are a type of deep learning architecture that process sequential data via a series of self-attention layers and then transform them via a feedforward neural network.\n",
    "\n",
    "What sets SegFormer apart from previous transformer-based approaches are two key features:\n",
    "\n",
    "1. A hierarchically structured transformer encoder which does not depend on positional encoding that avoids interpolation when training and testing resolutions differ.\n",
    "2. A lightweight MLP layer that avoids complex decoders.\n",
    "\n",
    "You will use a pretrained SegFormer model finetuned on [MITADE20K](http://sceneparsing.csail.mit.edu/) to perform batch inference.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Scaling_inference/segformer_architecture.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|SegFormer architecture showcasing the hierarical transformer encoder and all-MLP decoder. [*Date accessed: November 10, 2022*](https://arxiv.org/pdf/2105.15203.pdf).|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c018d564-0e4a-453f-9f7a-74466c0273e8",
   "metadata": {},
   "source": [
    "## Part 3: Sequential batch inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a07b49f-ee7b-48c7-a4d0-bc4043ff24c9",
   "metadata": {},
   "source": [
    "In order to establish familiarity with this batch inference task, you will implement a basic approach with one worker that generates predictions on batches sequentially. To get set up, the semantic segmentation example requires the following steps:\n",
    "\n",
    "1. Load the pretrained [SegFormer](https://huggingface.co/nvidia/segformer-b0-finetuned-ade-512-512) model.\n",
    "2. Load the [feature extractor](https://huggingface.co/docs/transformers/v4.16.2/en/model_doc/segformer#transformers.SegformerFeatureExtractor) (preprocessor for scene data).\n",
    "3. Load [SceneParse150](https://huggingface.co/datasets/scene_parse_150) dataset.\n",
    "4. Run batch inference on images from the test set.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Scaling_inference/single_sequential_timeline.png\" width=\"90%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Timeline of sequential batch inference using a single worker. Tasks can vary in runtime due variations in complexity, data size, and more. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1346830a-dce8-45a7-826e-c990eb3370ea",
   "metadata": {},
   "source": [
    "### Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79166c72-be43-4874-a68a-d5bb7b851b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U ray==2.3.0 transformers==4.26.1 torch==1.13.1 datasets==2.10.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f83049",
   "metadata": {},
   "source": [
    "### Set up necessary imports and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb5419c-26af-4b09-8fd2-e885aaa60021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Union\n",
    "from datasets.arrow_dataset import Dataset\n",
    "from huggingface_hub import hf_hub_download\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from PIL.JpegImagePlugin import JpegImageFile\n",
    "\n",
    "# Set the seed to a fixed value for reproducibility.\n",
    "torch.manual_seed(201)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245bf0ab",
   "metadata": {},
   "source": [
    "### Load the model components from the HuggingFace Hub\n",
    "\n",
    "From the [Hugging Face Hub](https://huggingface.co/docs/hub/index), retrieve the pretrained SegFormer model by specifying the model name and [label files](https://huggingface.co/datasets/huggingface/label-files/blob/main/ade20k-id2label.json) which map indices to semantic categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff44bdc",
   "metadata": {},
   "source": [
    "#### Load label mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a995aa5f-f77f-4683-a6ae-4a157eead902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/datasets/huggingface/label-files/blob/main/ade20k-id2label.json\n",
    "def get_labels():\n",
    "    repo_id = \"huggingface/label-files\"\n",
    "    filename = \"ade20k-id2label.json\"\n",
    "    id2label = json.load(\n",
    "        open(\n",
    "            hf_hub_download(repo_id=repo_id, filename=filename, repo_type=\"dataset\"),\n",
    "            \"r\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    id2label = {int(k): v for k, v in id2label.items()}\n",
    "    label2id = {v: k for k, v in id2label.items()}\n",
    "    return id2label, label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4346c5a0-36b4-46ce-bdc0-3102543d2dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label, label2id = get_labels()\n",
    "\n",
    "print(f\"Total number of labels: {len(id2label)}\")\n",
    "print(f\"Example labels: {list(id2label.values())[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9354716-8ac2-4e4f-97e1-e48ae174f0d9",
   "metadata": {},
   "source": [
    "The utility function `get_labels` fetches two dictionary mappings from [Hugging Face](https://huggingface.co/datasets/huggingface/label-files/blob/main/ade20k-id2label.json), `id2label` and `label2id`, which are used to convert between numerical and string labels for the 150 available [semantic categories](https://docs.google.com/spreadsheets/d/1se8YEtb2detS7OuPE86fXGyD269pMycAWe2mtKUj2W8/edit#gid=0) of objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d57314e",
   "metadata": {},
   "source": [
    "#### Load SegFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046f3990-3c6f-4ae4-9186-fa9fb1800f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SegformerForSemanticSegmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aec6f8-22bf-41f9-bd7b-fe54889d762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"nvidia/segformer-b0-finetuned-ade-512-512\"\n",
    "\n",
    "segformer = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    MODEL_NAME, id2label=id2label, label2id=label2id\n",
    ")\n",
    "\n",
    "print(f\"Number of model parameters: {segformer.num_parameters()/(10**6):.2f} M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f3ef57",
   "metadata": {},
   "source": [
    "The [Hugging Face Hub](https://huggingface.co/nvidia/segformer-b0-finetuned-ade-512-512) makes available many variations on SegFormer. Here, you specify a version finetuned on the MITADE20K (SceneParse150) dataset on images with a 512 x 512 resolution.\n",
    "\n",
    "Note: This \"b0\" model is the smallest, with [other options](https://huggingface.co/nvidia/segformer-b5-finetuned-ade-640-640) ranging up to and including \"b5\". Keep this in mind as something to experiment with when comparing different batch inference architectures later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41546abd-2f3d-4128-8176-19cd0604d6f4",
   "metadata": {},
   "source": [
    "#### Load the feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51be71f-1c3f-45fb-b9b2-af3393bfadc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SegformerFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f888ed08-cde2-4f4a-b05f-e467de4b7ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "segformer_feature_extractor = SegformerFeatureExtractor.from_pretrained(\n",
    "    MODEL_NAME, reduce_labels=True\n",
    ")\n",
    "segformer_feature_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9a23f3",
   "metadata": {},
   "source": [
    "[Feature extractors](https://huggingface.co/docs/transformers/main_classes/feature_extractor) preprocess input features (e.g. image data) by normalizing, resizing, padding, and converting raw images into the shape expected by SegFormer.\n",
    "\n",
    "The [`reduce_labels`](https://huggingface.co/docs/transformers/model_doc/segformer#segformer) flag ensures that the background of an image (anything that is not explicitly an object) isn't included when computing loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3109152b-3572-40f9-8fc6-b04ecce59896",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d58d4b-d13c-4c34-8e80-194c8f6bdae9",
   "metadata": {},
   "source": [
    "#### Set up necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e93db0-6218-4460-8f8c-14703a57f29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24ed45d-8700-40de-99ac-2bd4c5999974",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_DATA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25fc198-d3f4-47a0-9c16-9054a342c4f7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "  <strong>SMALL_DATA</strong>: a flag to download a subset (160 images) of the available test data. Defaults to True. Set to False (recommended) to work with the full test data (3352 images).\n",
    "</div>\n",
    "\n",
    "If you set `SMALL_DATA` to `False`, expect it to take some time (depending on your connection download speed) because you are downloading all test images to your local machine or cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3066409",
   "metadata": {},
   "source": [
    "#### Load SceneParse150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70f31fe-e8a9-41ea-857c-e2e2b36503e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"scene_parse_150\"\n",
    "\n",
    "# Load data from the Hugging Face datasets repository.\n",
    "if SMALL_DATA:\n",
    "    train_dataset = load_dataset(DATASET_NAME, split=\"train[:10]\")\n",
    "    test_dataset = load_dataset(DATASET_NAME, split=\"test[:160]\")\n",
    "else:\n",
    "    train_dataset = load_dataset(DATASET_NAME, split=\"train[:10]\")\n",
    "    test_dataset = load_dataset(DATASET_NAME, split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a16ce0",
   "metadata": {},
   "source": [
    "The two datasets serve different purposes:\n",
    "\n",
    "* **`train_dataset`**  \n",
    "    * Retrieve a small sample of images for visualization purposes only. Training samples include ground-truth, annotated image regions. Full training dataset contains 20210 images.\n",
    "* **`test_dataset`**  \n",
    "    * Used for batch inference purposes. Test samples do not contain ground-truth labels. Full test dataset contains 3352 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf19f5d-52cf-4d15-a958-e143bcf711c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff095514-c527-430c-a08e-b6d6eb12d44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_image_to_rgb(data_item):\n",
    "    if data_item[\"image\"].mode != \"RGB\":\n",
    "        data_item[\"image\"] = data_item[\"image\"].convert(mode=\"RGB\")\n",
    "\n",
    "    return data_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadba2c1-eb5e-42f9-8de7-bb3311a04786",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_dataset.map(convert_image_to_rgb)\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6e66d8-2587-43c9-91bb-cdfc6f22b33d",
   "metadata": {},
   "source": [
    "Each sample contains three components:\n",
    "* **`image`** \n",
    "    * The PIL image.\n",
    "* **`annotation`**  \n",
    "    * Human annotations of image regions (annotation mask is `None` in testing set).\n",
    "* **`category`**  \n",
    "    * Category of the scene generally (e.g. driveway, voting booth, dairy_outdoor)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2a63ad-3bfd-453c-827e-6a0f664ac310",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Display example images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fea9c83-a90a-4530-b539-66959ef1914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A colormap for visualizing segmentation results.\n",
    "# https://github.com/tensorflow/models/blob/3f1ca33afe3c1631b733ea7e40c294273b9e406d/research/deeplab/utils/get_dataset_colormap.py#L51\n",
    "# (date accessed: Nov 9th, 2022)\n",
    "ade_palette = np.array(\n",
    "    [\n",
    "        [0, 0, 0],\n",
    "        [120, 120, 120],\n",
    "        [180, 120, 120],\n",
    "        [6, 230, 230],\n",
    "        [80, 50, 50],\n",
    "        [4, 200, 3],\n",
    "        [120, 120, 80],\n",
    "        [140, 140, 140],\n",
    "        [204, 5, 255],\n",
    "        [230, 230, 230],\n",
    "        [4, 250, 7],\n",
    "        [224, 5, 255],\n",
    "        [235, 255, 7],\n",
    "        [150, 5, 61],\n",
    "        [120, 120, 70],\n",
    "        [8, 255, 51],\n",
    "        [255, 6, 82],\n",
    "        [143, 255, 140],\n",
    "        [204, 255, 4],\n",
    "        [255, 51, 7],\n",
    "        [204, 70, 3],\n",
    "        [0, 102, 200],\n",
    "        [61, 230, 250],\n",
    "        [255, 6, 51],\n",
    "        [11, 102, 255],\n",
    "        [255, 7, 71],\n",
    "        [255, 9, 224],\n",
    "        [9, 7, 230],\n",
    "        [220, 220, 220],\n",
    "        [255, 9, 92],\n",
    "        [112, 9, 255],\n",
    "        [8, 255, 214],\n",
    "        [7, 255, 224],\n",
    "        [255, 184, 6],\n",
    "        [10, 255, 71],\n",
    "        [255, 41, 10],\n",
    "        [7, 255, 255],\n",
    "        [224, 255, 8],\n",
    "        [102, 8, 255],\n",
    "        [255, 61, 6],\n",
    "        [255, 194, 7],\n",
    "        [255, 122, 8],\n",
    "        [0, 255, 20],\n",
    "        [255, 8, 41],\n",
    "        [255, 5, 153],\n",
    "        [6, 51, 255],\n",
    "        [235, 12, 255],\n",
    "        [160, 150, 20],\n",
    "        [0, 163, 255],\n",
    "        [140, 140, 140],\n",
    "        [250, 10, 15],\n",
    "        [20, 255, 0],\n",
    "        [31, 255, 0],\n",
    "        [255, 31, 0],\n",
    "        [255, 224, 0],\n",
    "        [153, 255, 0],\n",
    "        [0, 0, 255],\n",
    "        [255, 71, 0],\n",
    "        [0, 235, 255],\n",
    "        [0, 173, 255],\n",
    "        [31, 0, 255],\n",
    "        [11, 200, 200],\n",
    "        [255, 82, 0],\n",
    "        [0, 255, 245],\n",
    "        [0, 61, 255],\n",
    "        [0, 255, 112],\n",
    "        [0, 255, 133],\n",
    "        [255, 0, 0],\n",
    "        [255, 163, 0],\n",
    "        [255, 102, 0],\n",
    "        [194, 255, 0],\n",
    "        [0, 143, 255],\n",
    "        [51, 255, 0],\n",
    "        [0, 82, 255],\n",
    "        [0, 255, 41],\n",
    "        [0, 255, 173],\n",
    "        [10, 0, 255],\n",
    "        [173, 255, 0],\n",
    "        [0, 255, 153],\n",
    "        [255, 92, 0],\n",
    "        [255, 0, 255],\n",
    "        [255, 0, 245],\n",
    "        [255, 0, 102],\n",
    "        [255, 173, 0],\n",
    "        [255, 0, 20],\n",
    "        [255, 184, 184],\n",
    "        [0, 31, 255],\n",
    "        [0, 255, 61],\n",
    "        [0, 71, 255],\n",
    "        [255, 0, 204],\n",
    "        [0, 255, 194],\n",
    "        [0, 255, 82],\n",
    "        [0, 10, 255],\n",
    "        [0, 112, 255],\n",
    "        [51, 0, 255],\n",
    "        [0, 194, 255],\n",
    "        [0, 122, 255],\n",
    "        [0, 255, 163],\n",
    "        [255, 153, 0],\n",
    "        [0, 255, 10],\n",
    "        [255, 112, 0],\n",
    "        [143, 255, 0],\n",
    "        [82, 0, 255],\n",
    "        [163, 255, 0],\n",
    "        [255, 235, 0],\n",
    "        [8, 184, 170],\n",
    "        [133, 0, 255],\n",
    "        [0, 255, 92],\n",
    "        [184, 0, 255],\n",
    "        [255, 0, 31],\n",
    "        [0, 184, 255],\n",
    "        [0, 214, 255],\n",
    "        [255, 0, 112],\n",
    "        [92, 255, 0],\n",
    "        [0, 224, 255],\n",
    "        [112, 224, 255],\n",
    "        [70, 184, 160],\n",
    "        [163, 0, 255],\n",
    "        [153, 0, 255],\n",
    "        [71, 255, 0],\n",
    "        [255, 0, 163],\n",
    "        [255, 204, 0],\n",
    "        [255, 0, 143],\n",
    "        [0, 255, 235],\n",
    "        [133, 255, 0],\n",
    "        [255, 0, 235],\n",
    "        [245, 0, 255],\n",
    "        [255, 0, 122],\n",
    "        [255, 245, 0],\n",
    "        [10, 190, 212],\n",
    "        [214, 255, 0],\n",
    "        [0, 204, 255],\n",
    "        [20, 0, 255],\n",
    "        [255, 255, 0],\n",
    "        [0, 153, 255],\n",
    "        [0, 41, 255],\n",
    "        [0, 255, 204],\n",
    "        [41, 0, 255],\n",
    "        [41, 255, 0],\n",
    "        [173, 0, 255],\n",
    "        [0, 245, 255],\n",
    "        [71, 0, 255],\n",
    "        [122, 0, 255],\n",
    "        [0, 255, 184],\n",
    "        [0, 92, 255],\n",
    "        [184, 255, 0],\n",
    "        [0, 133, 255],\n",
    "        [255, 214, 0],\n",
    "        [25, 194, 194],\n",
    "        [102, 255, 0],\n",
    "        [92, 0, 255],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f42a14d-d001-4269-abf0-e81f06bd6b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_pixels_with_segmentation(\n",
    "    image: JpegImageFile, segmentation_maps: Union[torch.Tensor, np.array]\n",
    "):\n",
    "    segmentation_maps = np.array(segmentation_maps)\n",
    "    color_segments = np.zeros(\n",
    "        (segmentation_maps.shape[0], segmentation_maps.shape[1], 3), dtype=np.uint8\n",
    "    )\n",
    "    for label, color in enumerate(ade_palette):\n",
    "        color_segments[segmentation_maps == label, :] = color\n",
    "    color_segments = color_segments[..., ::-1]  # convert to BGR\n",
    "    pixels_with_segmentation = np.array(image) * 0.5 + color_segments * 0.5\n",
    "    return pixels_with_segmentation.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e06816f-0925-400f-a8af-1c06eac75aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_example_images(dataset: Dataset, n: int = 2):\n",
    "    fig, axes = plt.subplots(nrows=n, ncols=n, figsize=(10, 10))\n",
    "    fig.set_tight_layout(True)\n",
    "    for i, j in enumerate(\n",
    "        np.random.choice(dataset.num_rows, size=(n * n), replace=False)\n",
    "    ):\n",
    "        image_with_pixels = prepare_pixels_with_segmentation(\n",
    "            image=dataset[int(j)][\"image\"],\n",
    "            segmentation_maps=np.array(dataset[int(j)][\"annotation\"]),\n",
    "        )\n",
    "        axes[int(i / n), i % n].imshow(image_with_pixels)\n",
    "        axes[int(i / n), i % n].axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1054dfa-90a6-4569-bc0f-cb059f4b4893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try running this multiple times!\n",
    "display_example_images(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7243ec-d207-4b33-89ca-b80309c19c06",
   "metadata": {},
   "source": [
    "### Run sequential inference on 1 batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eccd1dc",
   "metadata": {},
   "source": [
    "#### Define inference logic\n",
    "\n",
    "This `predict` function forms the basis for the inference step, and you will reuse variations of this function multiple times throughout each approach for batch inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba82c68-f3cd-46ae-8b23-6fee43ccb3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    model: SegformerForSemanticSegmentation,\n",
    "    feature_extractor: SegformerFeatureExtractor,\n",
    "    images: list[JpegImageFile],\n",
    ") -> list[np.array]:\n",
    "    # Set the device on which PyTorch will run.\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)  # Move the model to specified device.\n",
    "    model.eval()  # Set the model in evaluation mode on test data.\n",
    "\n",
    "    # The feature extractor processes raw images.\n",
    "    inputs = feature_extractor(images=images, return_tensors=\"pt\")\n",
    "\n",
    "    # The model is applied to input images in the inference step.\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=inputs.pixel_values.to(device))\n",
    "\n",
    "    # Post-process the output for display.\n",
    "    image_sizes = [image.size[::-1] for image in images]\n",
    "    segmentation_maps_postprocessed = (\n",
    "        feature_extractor.post_process_semantic_segmentation(\n",
    "            outputs=outputs, target_sizes=image_sizes\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Return list of segmentation maps detached from the computation graph.\n",
    "    return [j.detach().cpu().numpy() for j in segmentation_maps_postprocessed]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336d6a94-8399-4616-8221-e1f5c357baf2",
   "metadata": {},
   "source": [
    "#### Prepare 1 batch of 16 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70115952-fdb0-40d3-87d0-a7903502a289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_indices(dataset: Dataset, n: int):\n",
    "    image_indices = np.random.choice(dataset.num_rows, size=n, replace=False)\n",
    "    return [int(i) for i in image_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930f4caf-8964-4170-8bde-b126a5a0ae57",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "# Get BATCH_SIZE randomly shuffled image IDs from the test dataset.\n",
    "image_indices = get_image_indices(dataset=test_dataset, n=BATCH_SIZE)\n",
    "image_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3302db4-6b93-4ef6-96b2-2bd316023044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of images by extracting images from random indices sampled from the test data.\n",
    "batch = [test_dataset[i][\"image\"] for i in image_indices]\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d33e48",
   "metadata": {},
   "source": [
    "#### Run batch inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b9f0f1-be22-4910-a5a1-e6d861cf3a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_maps = predict(\n",
    "    model=segformer,\n",
    "    feature_extractor=segformer_feature_extractor,\n",
    "    images=batch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b7394d-eef4-498e-bee4-38214df85406",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_maps[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e378ad0",
   "metadata": {},
   "source": [
    "Performing batch inference outputs a list of segmentation maps. Each element in the segmentation map array represents the [semantic category](https://docs.google.com/spreadsheets/d/1se8YEtb2detS7OuPE86fXGyD269pMycAWe2mtKUj2W8/edit#gid=0) of the corresponding pixel in the input image.\n",
    "\n",
    "Together, you can visualize these predicted segmentation maps by overlaying them onto the original image to see defined regions of objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee752e3a-d615-4773-a932-83018aee119e",
   "metadata": {},
   "source": [
    "#### Visualize example predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7d6a72-ed3e-4548-9b04-8490ee25f322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(image: JpegImageFile, segmentation_maps: torch.Tensor):\n",
    "    pxs = prepare_pixels_with_segmentation(\n",
    "        image=image, segmentation_maps=segmentation_maps\n",
    "    )\n",
    "    plt.imshow(pxs)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99ef21c-879e-46b3-b97f-1c00efcae05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(image=batch[0], segmentation_maps=segmentation_maps[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52be8531-ad94-4fde-b13d-a864f3020a37",
   "metadata": {},
   "source": [
    "### Run sequential inference on 10 batches\n",
    "\n",
    "Next, you will test the scalability and performance of the sequential batch inference approach by increasing the number of batches from 1 to 10. This will allow you to observe and verify that this approach can limit performance when scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93faafea-ddec-4148-a60a-a89d22b12aa8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Prepare batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e17bf3-9c04-47eb-a7c5-604a0ffbb3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "N_BATCHES = 10\n",
    "\n",
    "# Get BATCH_SIZE * N_BATCHES randomly shuffled image IDs from the test dataset.\n",
    "image_indices = get_image_indices(dataset=test_dataset, n=BATCH_SIZE * N_BATCHES)\n",
    "\n",
    "# Split indices into N_BATCHES\n",
    "image_indices_grouped = np.split(np.asarray(image_indices), N_BATCHES)\n",
    "image_indices_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f6004f-1cd5-46c9-89ad-708bbfe67312",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = []\n",
    "\n",
    "# Create a list of images for each batch of indices sampled from the test dataset.\n",
    "for image_idx in image_indices_grouped:\n",
    "    batch = [test_dataset[int(i)][\"image\"] for i in image_idx]\n",
    "    batches.append(batch)\n",
    "\n",
    "batches[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7907202d-bc99-40d0-aa15-e8200a206426",
   "metadata": {},
   "source": [
    "#### Run batch inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ca1763-8d9d-46aa-90c0-29b4b2be3c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9154c7c3-8fae-44f9-9e4c-bca3b62b7328",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in batches:\n",
    "    segmentation_maps = predict(\n",
    "        model=segformer,\n",
    "        feature_extractor=segformer_feature_extractor,\n",
    "        images=batch,\n",
    "    )\n",
    "    predictions.append(segmentation_maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24d6603-50ee-4ed7-b17e-411fb5ca92b1",
   "metadata": {},
   "source": [
    "Notice that increasing the number of batches by 10 leads to approximately a 10x increase in runtime/ This is the expected result for a sequential approach, which scales linearly with the number of batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d425f7-d23c-41d1-adee-3cc9509e190f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the resulting segmentation maps array.\n",
    "predictions[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e69cad-d947-42d0-9cad-8f0a2d4941c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Summary: Sequential batch inference\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Scaling_inference/single_sequential_timeline.png\" width=\"90%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Timeline of sequential batch inference using a single worker. Tasks can vary in runtime due variations in complexity, data size, and more. |\n",
    "\n",
    "#### Key concepts\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong>Batch inference</strong> (also known as offline inference): is the process of generating predictions on a large set or \"batch\" of data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7289f04c-337c-47e1-a415-a5c5d80b61cc",
   "metadata": {},
   "source": [
    "## Part 4: Distributed batch inference with Ray AIR\n",
    "\n",
    "These high-level APIs automate the challenging aspects of parallelizing and distributing batch inference tasks, allowing you to focus on the inference logic. \n",
    "\n",
    "There are four main abstractions that work together to optimize this process:\n",
    "\n",
    "* [**`Datasets`**](https://docs.ray.io/en/latest/data/dataset.html)  \n",
    "    * These are used to parallelize data loading, preprocessing, and exchanging data in Ray AIR.\n",
    "* [**`Checkpoint`**](https://docs.ray.io/en/latest/tune/tutorials/tune-checkpoints.html)  \n",
    "    * `Checkpoint` objects represent saved models created during training or tuning and provide a common interface for restoring the model's state for tasks such as inference.\n",
    "* [**`Predictor`**](https://docs.ray.io/en/latest/ray-air/predictors.html)  \n",
    "    * Ray AIR `Predictors` are a class that load models from `Checkpoint` to perform inference and can be used by `BatchPredictor` to do large-scale inference.\n",
    "* [**`BatchPredictor`**](https://docs.ray.io/en/latest/ray-air/predictors.html#batch-prediction)  \n",
    "    * Ray AIR `BatchPredictor` utility takes in a `Checkpoint` and a `Predictor` class and executes large-scale distributed batch prediction on a Ray Dataset when calling `predict()`.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Scaling_inference/air_batchpredictor.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Ray Datasets parallelize data loading, preprocessing, and batching. Ray AIR `BatchPredictor` takes both `Checkpoint` and `Predictor` objects to call `predict()` on a Ray Dataset for distributed batch inference.|\n",
    "\n",
    "Ray handles operations such as batching, pipelining, actor pool autoscaling, and memory management internally, so you can benefit from the scalability and ease of use of Ray AIR without needing to worry about the details of task distribution. Using these abstractions does come with some trade-offs, as you have less control over how Ray distributes the workload."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665bc8b6-878e-4fde-9054-2891acf83c1b",
   "metadata": {},
   "source": [
    "### Initialize Ray runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce928af7-2037-4b65-9dd1-6d67b5335e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c97168",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d050e876-1b42-41ee-b3f2-d0caa8a31fbc",
   "metadata": {},
   "source": [
    "### Create a Ray Dataset with 160 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ccdeb8-4c31-4752-8f16-9f13e3cf3366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get BATCH_SIZE * N_BATCHES randomly shuffled image IDs from the test dataset.\n",
    "image_indices = get_image_indices(dataset=test_dataset, n=BATCH_SIZE * N_BATCHES)\n",
    "\n",
    "# Create a list of images for the indices sampled from the test dataset.\n",
    "data = [test_dataset[i][\"image\"] for i in image_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1f4066-7d32-4cb6-813f-9a880263b2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Ray Dataset from the list of images to use in Ray AIR.\n",
    "dataset = ray.data.from_items(data)\n",
    "dataset.show(limit=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ef6e96-f29e-4db9-be57-0aee104a34d9",
   "metadata": {},
   "source": [
    "### Define a custom Predictor for image data\n",
    "\n",
    "`BatchPredictor` takes in a `Checkpoint` (which will be constructed from the SegFormer model and feature extractor) and a `Predictor`. Ray AIR supports multiple framework-specific [`Predictors`](https://docs.ray.io/en/latest/ray-air/package-ref.html#predictor) such as TorchPredictor and TensorflowPredictor while also allowing for the ability to implement a [custom](https://docs.ray.io/en/latest/ray-air/predictors.html#developer-guide-implementing-your-own-predictor) one. \n",
    "\n",
    "Here, you will implement a custom `SemanticSegmentationPredictor`, with the same replicas and core `predict()` logic as before, but with some modifications to fit the `BatchPredictor` pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ae3c1e-31f6-4388-86a8-10ba84860f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.air import Checkpoint\n",
    "from ray.train.predictor import Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3024c0-f651-44f4-93f6-8964918b3bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSegmentationPredictor(Predictor):\n",
    "    # The constructor method initializes the class to load/cache the model and feature extractor.\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: SegformerForSemanticSegmentation,\n",
    "        feature_extractor: SegformerFeatureExtractor,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    # This is the same logic as the `predict()` function defined in Part 3,\n",
    "    # only with pandas DataFrames as inputs and outputs.\n",
    "    def _predict_pandas(self, batch: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Set the device on which PyTorch will run.\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "        # The feature extractor processes raw images.\n",
    "        batch = [batch[\"value\"][0]]\n",
    "        inputs = self.feature_extractor(images=batch, return_tensors=\"pt\")\n",
    "\n",
    "        # The model is applied to input images in the inference step.\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(pixel_values=inputs.pixel_values.to(device))\n",
    "\n",
    "        # Post-process the output for display.\n",
    "        image_sizes = [image.size[::-1] for image in batch]\n",
    "        segmentation_maps_postprocessed = (\n",
    "            self.feature_extractor.post_process_semantic_segmentation(\n",
    "                outputs=outputs, target_sizes=image_sizes\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Post-process the list of segmentation maps into a pandas DataFrame\n",
    "        df = pd.DataFrame(columns=[\"segmentation_maps\"])\n",
    "        df.loc[0, \"segmentation_maps\"] = segmentation_maps_postprocessed\n",
    "\n",
    "        return df\n",
    "\n",
    "    # Creates an instance of SemanticSegmentationPredictor using the model and\n",
    "    # feature extractor contained in the Checkpoint.\n",
    "    @classmethod\n",
    "    def from_checkpoint(\n",
    "        self, checkpoint: Checkpoint, **kwargs\n",
    "    ) -> \"SemanticSegmentationPredictor\":\n",
    "        checkpoint_data = checkpoint.to_dict()\n",
    "        return SemanticSegmentationPredictor(\n",
    "            model=checkpoint_data[\"model\"],\n",
    "            feature_extractor=checkpoint_data[\"feature_extractor\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0790eeb-3207-4f8b-b520-e3751f93cf2b",
   "metadata": {},
   "source": [
    "### Create a BatchPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b73b9a-b04b-4ded-8322-fbc4fa2620a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.batch_predictor import BatchPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3118f64",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Construct a BatchPredictor using the SegFormer model and feature extractor along with an instance\n",
    "# of the custom SemanticSegmentationPredictor class.\n",
    "batch_predictor = BatchPredictor(\n",
    "    checkpoint=Checkpoint.from_dict(\n",
    "        {\"model\": segformer, \"feature_extractor\": segformer_feature_extractor}\n",
    "    ),\n",
    "    predictor_cls=SemanticSegmentationPredictor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0509bf8-d124-4a6c-9720-82bf01b03faa",
   "metadata": {},
   "source": [
    "### Run parallel batch inference on a Ray Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74717154-8664-46b6-b54e-ec6d687d644f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dataset = batch_predictor.predict(data=dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01247c69-c2f4-4563-a8d2-71b4e5b7b364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the resulting segmentation maps in this DataFrame.\n",
    "predictions_dataset.take(limit=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3aae2c-491f-41a9-bd67-09395f398890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terminate processes started by ray.init().\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf1a41b-0e05-427f-8cf2-ec393484bf16",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Summary: Distributed batch inference with Ray AIR\n",
    "\n",
    "#### Key API elements\n",
    "\n",
    "* [**`Datasets`**](https://docs.ray.io/en/latest/data/dataset.html)  \n",
    "    * These are used to parallelize data loading, preprocessing, and exchanging data in Ray AIR.\n",
    "* [**`Checkpoint`**](https://docs.ray.io/en/latest/tune/tutorials/tune-checkpoints.html)  \n",
    "    * `Checkpoint` objects represent saved models created during training or tuning and provide a common interface for restoring the model's state for tasks such as inference.\n",
    "* [**`Predictor`**](https://docs.ray.io/en/latest/ray-air/predictors.html)  \n",
    "    * Ray AIR `Predictors` are a class that load models from `Checkpoint` to perform inference and can be used by `BatchPredictor` to do large-scale inference.\n",
    "* [**`BatchPredictor`**](https://docs.ray.io/en/latest/ray-air/predictors.html#batch-prediction)  \n",
    "    * Ray AIR `BatchPredictor` utility takes in a `Checkpoint` and a `Predictor` class and executes large-scale distributed batch prediction on a Ray Dataset when calling `predict()`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8a0b7a-3cff-4aea-9b92-c28200560728",
   "metadata": {},
   "source": [
    "#### Shutdown Ray runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe90b042-e31b-483e-8368-5d7b7aa1c761",
   "metadata": {},
   "source": [
    "# Connect with the Ray community\n",
    "\n",
    "You can learn and get more involved with the Ray community of developers and researchers:\n",
    "\n",
    "* [**Ray documentation**](https://docs.ray.io/en/latest)\n",
    "\n",
    "* [**Official Ray Website**](https://www.ray.io/)  \n",
    "Browse the ecosystem and use this site as a hub to get the information that you need to get going and building with Ray.\n",
    "\n",
    "* [**Join the Community on Slack**](https://forms.gle/9TSdDYUgxYs8SA9e8)  \n",
    "Find friends to discuss your new learnings in our Slack space.\n",
    "\n",
    "* [**Use the Discussion Board**](https://discuss.ray.io/)  \n",
    "Ask questions, follow topics, and view announcements on this community forum.\n",
    "\n",
    "* [**Join a Meetup Group**](https://www.meetup.com/Bay-Area-Ray-Meetup/)  \n",
    "Tune in on meet-ups to listen to compelling talks, get to know other users, and meet the team behind Ray.\n",
    "\n",
    "* [**Open an Issue**](https://github.com/ray-project/ray/issues/new/choose)  \n",
    "Ray is constantly evolving to improve developer experience. Submit feature requests, bug-reports, and get help via GitHub issues.\n",
    "\n",
    "* [**Become a Ray contributor**](https://docs.ray.io/en/latest/ray-contribute/getting-involved.html)  \n",
    "We welcome community contributions to improve our documentation and Ray framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5891f0e7-56f7-41c5-b130-6bbee6e878f8",
   "metadata": {},
   "source": [
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Generic/ray_logo.png\" width=\"20%\" loading=\"lazy\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "vscode": {
   "interpreter": {
    "hash": "567405a8058597909526349386224fe35dd047505a91307e44ed44be00113429"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
