{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcb9a269-765a-46ef-abb6-d60682d38b0d",
   "metadata": {},
   "source": [
    "# Scaling Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b114120-a325-4597-bf7b-8310c9c8d776",
   "metadata": {},
   "source": [
    "<img src=\"../../_static/assets/Generic/ray_logo.png\" width=\"20%\" loading=\"lazy\">\n",
    "\n",
    "## About this notebook\n",
    "\n",
    "### Is it right for you?\n",
    "\n",
    "This is an introductory notebook that gives a broad overview of the Ray project. It is right for you if:\n",
    "\n",
    "* you work with model inference problem or you want to scale your existing inference pipelines\n",
    "* <ToDo\\>\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "For this notebook you should have:\n",
    "\n",
    "* practical Python and machine learning experience\n",
    "* familiarity with Ray and Ray AIR. Equivalent to completing these training modules:\n",
    "  * [Overview of Ray](https://github.com/ray-project/ray-educational-materials/blob/main/Introductory_modules/Overview_of_Ray.ipynb)\n",
    "  * [Introduction to Ray AIR](https://github.com/ray-project/ray-educational-materials/blob/main/Introductory_modules/Introduction_to_Ray_AIR.ipynb)\n",
    "  * [Ray Core](https://github.com/ray-project/ray-educational-materials/tree/main/Ray_Core)\n",
    "* familiarity with batch inference problem in ML\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "Upon completion of this notebook, you will know about:\n",
    "\n",
    "* Inference patterns\n",
    "* Architectures how to scale inference with Ray\n",
    "\n",
    "### What will you do?\n",
    "\n",
    "<ToDo/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a02709-0ed5-489a-b344-27005261eb72",
   "metadata": {},
   "source": [
    "## Part 1: Patterns and architectures for scalable inference\n",
    "\n",
    "The end goal for machine learning models is to generate performant predictions over a set of unseen data. In this module, you will approach parallelizing inference on using Ray Core's API as well as the high-level abstractions available in Ray AI Runtime.\n",
    "\n",
    "|<img src=\"../../_static/assets/Scaling_inference/ml_pipeline.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Typical end-to-end machine learning pipeline.|\n",
    "\n",
    "### Patterns for resilient serving\n",
    "\n",
    "**Stateless serving function with Ray tasks**\n",
    "\n",
    "In production environments, you prioritize lower latency, however, loading complex models into memory can be expensive and sequential processing of requests limits speed. *Stateless serving* allows an ML system to handle high volume requests by:\n",
    "\n",
    "1. exporting the model's mathematical core into a language agnostic format\n",
    "2. restoring the architecture and weights of a trained model in a stateless function (i.e. Ray tasks)\n",
    "3. deploying Ray tasks into a framework that provides a REST endpoint\n",
    "\n",
    "This way, the end client doesn't need to know anything about machine learning to be able to autoscale the web endpoints or manage the web application.\n",
    "\n",
    "**Batch serving with Ray actors**\n",
    "\n",
    " When your deployed model takes too long to generate immediate results, online prediction may not be the right approach. In addition, some situations require predictions to be generated over large volumes of data such as curating personalized playlists. You can use *batch inference*, which is an asynchronous method of batching observations for prediction in advance to process a high volume of samples efficiently.\n",
    "\n",
    "Setting up distributed batch inference with Ray involves:\n",
    "\n",
    "1. creating a number of replicas of your model; in Ray, these replicas are represented as Actors (i.e., stateful processes) that can be assigned to GPUs and hold instantiated model objects\n",
    "\n",
    "2. feeding data into these model replicas in parallel, and retrieve inference results\n",
    "\n",
    "### Ray architectures for scalable inference\n",
    "\n",
    "#### Stateless inference - Ray Task\n",
    "\n",
    "A Ray task is *stateless* because its output (e.g. predictions) is determined purely by its inputs (e.g. the trained model). Performing online inference involves loading the model for every request and synchronously serving results.\n",
    "\n",
    "#### Stateful inference - Ray Actors\n",
    "\n",
    "Stateless inference may work alright for smaller models, at times you want to avoid loading the model from memory every time. By placing the trained model in the Ray object store, then launching a number of Actors, you can implement distributed dispatch of tasks and process results from the actors in a streaming way.\n",
    "\n",
    "|<img src=\"../../_static/assets/Scaling_inference/actors.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Using Ray Actors for Batch Inference.|\n",
    "\n",
    "#### Ray ActorPool - Increment of the previous approach - utility lib.\n",
    "\n",
    "Ray provides a convenient [ActorPool utility](https://docs.ray.io/en/latest/ray-core/package-ref.html#ray-util-actorpool) which wraps the above list of actors to avoid futures management.\n",
    "\n",
    "|<img src=\"../../_static/assets/Scaling_inference/actor_pool.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Using Actor Pools for Batch Inference.|\n",
    "\n",
    "#### Ray AIR Datasets\n",
    "\n",
    "Ray Datasets allows for parallel reading and preprocessing of source data along with autoscaling of the ActorPool. As a part of Ray AIR, you specify what you want done through a set of declarative key-value arguments rather than concerning yourself with how to instruct Ray to scale.\n",
    "\n",
    "#### Ray AIR BatchPredictor\n",
    "\n",
    "Ray AIR's [`BatchPredictor`](https://docs.ray.io/en/latest/ray-air/package-ref.html#batch-predictor) takes in a [`Checkpoint`](https://docs.ray.io/en/latest/ray-air/package-ref.html#checkpoint) which represents the saved model. This high-level abstraction offers simple and composable APIs that enable preprocessing data in batches with [BatchMapper](https://docs.ray.io/en/latest/ray-air/package-ref.html#generic-preprocessors) and instantiate a distributed predictor given checkpoint data.\n",
    "\n",
    "|<img src=\"../../_static/assets/Scaling_inference/air_batchpredictor.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Using Ray AIR for Batch Inference.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf547df-6663-47e4-99df-de54ac0cd5ef",
   "metadata": {},
   "source": [
    "## Part 2: Notes on data and model\n",
    "\n",
    "### Data\n",
    "\n",
    "Image segmentation takes a scene and classifies image objects [into semantic categories](https://docs.google.com/spreadsheets/d/1se8YEtb2detS7OuPE86fXGyD269pMycAWe2mtKUj2W8/edit?usp=sharing) pixel-by-pixel. [MIT ADE20K Dataset](http://sceneparsing.csail.mit.edu/) provides the largest open source dataset for scene parsing, and in this notebook, you will be scaling inference on image regions depicted in these samples.\n",
    "\n",
    "|<img src=\"../../_static/assets/Scaling_inference/scene.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|[From left to right: test image, ground truth, predicted result.](https://github.com/CSAILVision/semantic-segmentation-pytorch) *Date accessed: November 10, 2022*|\n",
    "\n",
    "[**Dataset Highlights**](https://arxiv.org/pdf/1608.05442.pdf)\n",
    "\n",
    "- 20k annotated, scene-centric training images\n",
    "- 2k validation images\n",
    "- 150 total categories such as person, car, bed, sky, and more\n",
    "\n",
    "### Model\n",
    "\n",
    "[SegFormer Paper](https://arxiv.org/pdf/2105.15203.pdf)\n",
    "\n",
    "SegFormer consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great results on semantic segmentation benchmarks such as ADE20K and Cityscapes. The hierarchical Transformer is first pre-trained on ImageNet-1k, after which a decode head is added and fine-tuned altogether on a downstream dataset.\n",
    "\n",
    "The abstract from the paper is the following:\n",
    "\n",
    "\"We present SegFormer, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with lightweight multilayer perception (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding, thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from different layers, and thus combining both local attention and global attention to render powerful representations. We show that this simple and lightweight design is the key to efficient segmentation on Transformers. We scale our approach up to obtain a series of models from SegFormer-B0 to SegFormer-B5, reaching significantly better performance and efficiency than previous counterparts. For example, SegFormer-B4 achieves 50.3% mIoU on ADE20K with 64M parameters, being 5x smaller and 2.2% better than the previous best method. Our best model, SegFormer-B5, achieves 84.0% mIoU on Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C.\"\n",
    "\n",
    "|<img src=\"../../_static/assets/Scaling_inference/segformer_architecture.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Segformer architecture taken from [original paper](https://arxiv.org/pdf/2105.15203.pdf). *Date accessed: November 10, 2022*|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c018d564-0e4a-453f-9f7a-74466c0273e8",
   "metadata": {},
   "source": [
    "## Part 3: Vanilla implementation\n",
    "\n",
    "*(This implementation is inspired by the [Semantic segmentation](https://huggingface.co/docs/transformers/tasks/semantic_segmentation) guide. Date accessed: Nov 4th, 2022.)*\n",
    "\n",
    "We start be downloading the SceneParse150 dataset using Hugging Face's `datasets` library. Specifically, we're going to leverage the `load_dataset` utility to reference this dataset as a string (`\"scene_parse_150\"`). Note that this can take a couple of minutes. We also specify a `split` argument so that we can access `train` and `test` data on the resulting dataset `ds`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70f31fe-e8a9-41ea-857c-e2e2b36503e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from Hugging Face\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"scene_parse_150\", split=\"train[:50]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c6e51b",
   "metadata": {},
   "source": [
    "Each Hugging Face dataset comes with a `train_test_split` method that we're going to use next. We want 80% of the data to be training data, and 20% held back for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98814d17-d57d-4bdd-ba1c-8e64880f6361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make tr/test splits\n",
    "split_ds = ds.train_test_split(test_size=0.2)\n",
    "train_ds = split_ds[\"train\"]\n",
    "test_ds = split_ds[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2394456",
   "metadata": {},
   "source": [
    "To get a feel for what this dataset consists of, let's print the first of it. Since the train-test split we did is randomized, the resulting image will be different every time you load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab351fc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_ds[0][\"image\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96f9007",
   "metadata": {},
   "source": [
    "Next, we download the mappings from the Hub (using the `huggingface_hub` library) and create two dictionaries, namely `id2label` and `label2id`. We use these dictionaries to map (bidirectionally) between label IDs (int) and the labels (string) of the respective images. There are a total of 150 labels, or categories in for this dataset, and we print 10 of them at the end. Since we're parsing scenes and want to classify segments of natural images, you can see that the labels describe entities such as walls, floors, roads or grass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e32feab-170b-4220-a263-bb678592d106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "repo_id = \"huggingface/label-files\"\n",
    "filename = \"ade20k-id2label.json\"\n",
    "id2label = json.load(open(hf_hub_download(repo_id=repo_id, filename=filename, repo_type=\"dataset\"), \"r\"))\n",
    "\n",
    "id2label = {int(k): v for k, v in id2label.items()}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "num_labels = len(id2label)\n",
    "print(list(id2label.values())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0da5ecf-8a1e-4588-b010-f4a21aad0ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing and augmentations\n",
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"nvidia/mit-b0\", reduce_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1da5188",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## Augmentations\n",
    "from torchvision.transforms import ColorJitter\n",
    "\n",
    "jitter = ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dfdb96-0f12-4a96-abfd-cf7d8373f2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and validate transforms\n",
    "def train_transforms(example_batch):\n",
    "    images = [jitter(x) for x in example_batch[\"image\"]]\n",
    "    labels = [x for x in example_batch[\"annotation\"]]\n",
    "    inputs = feature_extractor(images, labels)\n",
    "    return inputs\n",
    "\n",
    "def val_transforms(example_batch):\n",
    "    images = [x for x in example_batch[\"image\"]]\n",
    "    labels = [x for x in example_batch[\"annotation\"]]\n",
    "    inputs = feature_extractor(images, labels)\n",
    "    return inputs\n",
    "\n",
    "# Inputs are pixel values - but we still need original images\n",
    "# ToDo: check if transform can return images, labels, inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e44f917-3de0-47b2-a3dd-3392a773d4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run transform on data\n",
    "from copy import deepcopy\n",
    "\n",
    "original_train_ds = deepcopy(train_ds)\n",
    "original_test_ds = deepcopy(test_ds)\n",
    "\n",
    "train_ds.set_transform(train_transforms)\n",
    "test_ds.set_transform(val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87778f10-37df-4c3d-b473-1d3a1d44ade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pretrained model\n",
    "from transformers import AutoModelForSemanticSegmentation\n",
    "\n",
    "pretrained_model_name = \"nvidia/mit-b0\"\n",
    "model = AutoModelForSemanticSegmentation.from_pretrained(\n",
    "    pretrained_model_name, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da131bf-a4ae-4e68-9b0c-fd72eccb094a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate TrainingArguments\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"segformer-b0-scene-parse-150\",\n",
    "    learning_rate=6e-5,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    save_total_limit=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=20,\n",
    "    eval_steps=20,\n",
    "    logging_steps=1,\n",
    "    eval_accumulation_steps=5,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12533b17-54a0-495c-9319-642372f77025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval metric from Evaluate lib.\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"mean_iou\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8e0986-93af-44d9-bbe3-afa65caa88e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metrics\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    with torch.no_grad():\n",
    "        logits, labels = eval_pred\n",
    "        logits_tensor = torch.from_numpy(logits)\n",
    "        logits_tensor = torch.nn.functional.interpolate(\n",
    "            logits_tensor,\n",
    "            size=labels.shape[-2:],\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        ).argmax(dim=1)\n",
    "\n",
    "        pred_labels = logits_tensor.detach().cpu().numpy()\n",
    "        metrics = metric.compute(\n",
    "            predictions=pred_labels,\n",
    "            references=labels,\n",
    "            num_labels=num_labels,\n",
    "            ignore_index=255,\n",
    "            reduce_labels=False,\n",
    "        )\n",
    "        for key, value in metrics.items():\n",
    "            if type(value) is np.ndarray:\n",
    "                metrics[key] = value.tolist()\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1a07cb-f838-4db6-815c-5c4eca4d905b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Trainer\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3622810-bf77-4ff5-a7ca-9bec0da81d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model training (TODO: optionally load a pre-trained model here instead, e.g. to save time).\n",
    "# ToDo: shall we put model on S3 or HF?\n",
    "\n",
    "run_training = False\n",
    "\n",
    "if run_training:\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b2d3f3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if run_training:\n",
    "    MODEL = trainer.save_model(\".\")\n",
    "else:\n",
    "    MODEL = pretrained_model_name\n",
    "\n",
    "def load_trained_model(model_path=MODEL):\n",
    "    return AutoModelForSemanticSegmentation.from_pretrained(\n",
    "        model_path, id2label=id2label, label2id=label2id\n",
    "    )\n",
    "\n",
    "model = load_trained_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d059bb-0e4a-4b82-8f85-aca7db10b57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "image = original_test_ds[0][\"image\"]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f787be-9f78-44a2-8209-993799036f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo - pixel values\n",
    "pixels = test_ds[0][\"pixel_values\"]\n",
    "pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9443bc-584d-4b4a-b616-56e6c3bb4649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this and following cells are re-usable functions.\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # use GPU if available, otherwise use a CPU\n",
    "\n",
    "def encode_pixels(image):\n",
    "    encoding = feature_extractor(image, return_tensors=\"pt\")\n",
    "    pixel_values = encoding.pixel_values.to(device)\n",
    "    return pixel_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b926356e-338b-4848-92a5-8315346d1561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logits(model, pixel_values):\n",
    "    outputs = model(pixel_values=pixel_values)\n",
    "    return outputs.logits.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e14197-a2c2-4a01-b81b-f50ed41550a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale img\n",
    "def upsample_logits(logits):\n",
    "    upsampled_logits = torch.nn.functional.interpolate(\n",
    "        logits,\n",
    "        size=image.size[::-1],\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "\n",
    "    return upsampled_logits.argmax(dim=1)[0]\n",
    "\n",
    "def predict(model, image):\n",
    "    pixel_values = encode_pixels(image)\n",
    "    logits = compute_logits(model, pixel_values)\n",
    "    return upsample_logits(logits)\n",
    "\n",
    "prediction = predict(model, image)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df44fc1b-2045-46f8-9686-0a49e6e2b55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize pred\n",
    "# palette was copied from here: https://github.com/tensorflow/models/blob/3f1ca33afe3c1631b733ea7e40c294273b9e406d/research/deeplab/utils/get_dataset_colormap.py#L51\n",
    "# (accessed: nov 9, 2022)\n",
    "\n",
    "from palette import create_ade20k_label_colormap as ade_palette\n",
    "\n",
    "# ToDo: figure out how to visualzie grid of images -> results preview for learners\n",
    "# TODO: this only seems to work for the 0th image\n",
    "# ToDo: make sure that both pred and visualization works here\n",
    "def prepare_for_visualisation(image, prediction):\n",
    "    color_seg = np.zeros((prediction.shape[0], prediction.shape[1], 3), dtype=np.uint8)\n",
    "    palette = np.array(ade_palette())\n",
    "    for label, color in enumerate(palette):\n",
    "        color_seg[prediction == label, :] = color\n",
    "    color_seg = color_seg[..., ::-1]  # convert to BGR\n",
    "\n",
    "    img = np.array(image) * 0.5 + color_seg * 0.5  # plot the image with the segmentation map\n",
    "    img = img.astype(np.uint8)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31b780b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# image is now Jpeg object\n",
    "img = prepare_for_visualisation(image, prediction)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33c74b3-9d0f-469f-83ec-7e5557104489",
   "metadata": {},
   "source": [
    "## Part 4: Stateless inference - Ray Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c97168",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e695c89f",
   "metadata": {},
   "source": [
    "The most naive version of parallelising prediction is to create Ray tasks that load the trained model internally when called. This way we can make the prediction task \"stateless\", but at the cost of incurring the overhead of loading the model every single time. This is akin to what serverless solutions like AWS Lambda would do, and this pattern could be worth it for tiny models, for which the application doesn't get bottle-necked by the model loading step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797b9e38",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def prediction_task(image):\n",
    "    model = load_trained_model()\n",
    "    return predict(model, image)\n",
    "\n",
    "prediction_refs = [prediction_task.remote(original_test_ds[i][\"image\"]) for i in range(10)]\n",
    "predictions = ray.get(prediction_refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c778447b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "img = prepare_for_visualisation(original_test_ds[0][\"image\"], predictions[0])\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e2f9e2-945b-4b08-95ec-74b09b8b07d5",
   "metadata": {},
   "source": [
    "## Part 5: Stateful inference - Ray Actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4030c8a1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "@ray.remote\n",
    "class PredictionActor:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def predict(self, image):\n",
    "        return  predict(self.model, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f910cd41",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model = load_trained_model()\n",
    "model_ref = ray.put(model)\n",
    "actors = [PredictionActor.remote(model_ref) for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540e7f6a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def process_result(prediction):\n",
    "    print(f\"Got prediction of shape: {prediction.shape}\")\n",
    "\n",
    "data = [original_test_ds[i][\"image\"] for i in range(10)]\n",
    "\n",
    "# todo: check if copy is needed here\n",
    "idle_actors = actors.copy()\n",
    "future_to_actor = {}\n",
    "\n",
    "# based on a pattern, where you want to wait 1,2,4,8,... seconds for the results\n",
    "# based on this: https://docs.ray.io/en/master/ray-core/patterns/limit-pending-tasks.html\n",
    "while data:\n",
    "    if idle_actors:\n",
    "        actor = idle_actors.pop()\n",
    "        future = actor.predict.remote(data.pop())\n",
    "        future_to_actor[future] = actor\n",
    "    else:\n",
    "        # https://docs.ray.io/en/latest/ray-core/package-ref.html#ray-wait\n",
    "        [ready], _ = ray.wait(list(future_to_actor.keys()), num_returns=1) # num_returns=1 makes sure that you return one item in the ready list\n",
    "        actor = future_to_actor.pop(ready)\n",
    "        idle_actors.append(actor)\n",
    "        process_result(ray.get(ready))\n",
    "\n",
    "# Process any leftover results at the end.\n",
    "for future in future_to_actor.keys():\n",
    "    process_result(ray.get(future))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7036efa3-2dd1-4b55-82e3-da81a0a33f13",
   "metadata": {},
   "source": [
    "## Part 6: Stateful inference - Ray ActorPool - Increment of the previous approach - utility lib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9797d0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from ray.util.actor_pool import ActorPool\n",
    "actor_pool = ActorPool(actors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50e7b08",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data = [original_test_ds[i][\"image\"] for i in range(10)]\n",
    "\n",
    "\n",
    "def actor_call(actor, data_item):\n",
    "    return actor.predict.remote(data_item)\n",
    "\n",
    "for result in actor_pool.map_unordered(actor_call, data):\n",
    "    process_result(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7289f04c-337c-47e1-a415-a5c5d80b61cc",
   "metadata": {},
   "source": [
    "## Part 7: Ray AIR Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a31a72",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class PredictionClass:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        prediction = predict(self.model, batch)\n",
    "        # TODO: for some reason we are not allowed to return arbitrary return values here, needs to be numpy, list, etc.\n",
    "        return prediction.cpu().detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daec098",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dataset = ray.data.from_items(data)\n",
    "dataset.show()\n",
    "\n",
    "# ToDo: pre-processing -> dataset of baches of images, not individual images. Probably is has to be batches of pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87397fbd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "results = dataset.map_batches(\n",
    "    PredictionClass,\n",
    "    batch_size=1,\n",
    "    num_gpus=0,\n",
    "    compute=ray.data.ActorPoolStrategy(min_size=1, max_size=5),\n",
    "    fn_constructor_args=(model_ref,)\n",
    ")\n",
    "\n",
    "results.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460ac28a-3739-4456-99a3-1579c1081db3",
   "metadata": {},
   "source": [
    "## Part 8: Ray AIR BatchPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3118f64",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from ray.air import Checkpoint\n",
    "from ray.train.predictor import Predictor\n",
    "from ray.train.batch_predictor import BatchPredictor\n",
    "import pandas as pd\n",
    "\n",
    "# this is a hack, don't use the same image in the predictor.\n",
    "image = original_test_ds[0][\"image\"]\n",
    "\n",
    "\n",
    "# https://docs.ray.io/en/latest/ray-air/predictors.html#batch-prediction\n",
    "# adapt it to batch prediction on images\n",
    "class CustomPredictor(Predictor):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        self.model = load_trained_model(model_name)\n",
    "\n",
    "    def _predict_pandas(self, batch):\n",
    "        # TODO: figure out how to make this run on pandas properly.\n",
    "        # ... why are we forced to use pandas, though?\n",
    "        prediction = predict(self.model, image)\n",
    "\n",
    "        # ToDo: can we work with numpy here?\n",
    "        # implement post processing to cast prediction to pandas\n",
    "\n",
    "        return pd.DataFrame(prediction)\n",
    "\n",
    "    @classmethod\n",
    "    def from_checkpoint(cls, checkpoint, **kwargs):\n",
    "        return CustomPredictor(checkpoint.to_dict()[\"model\"])\n",
    "\n",
    "predictor = BatchPredictor(\n",
    "    checkpoint=Checkpoint.from_dict({\"model\": MODEL}),\n",
    "    predictor_cls=CustomPredictor,\n",
    "    preprocessor=None,\n",
    ")\n",
    "\n",
    "results = predictor.predict(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
