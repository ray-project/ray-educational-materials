{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcb9a269-765a-46ef-abb6-d60682d38b0d",
   "metadata": {},
   "source": [
    "# Scaling Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b114120-a325-4597-bf7b-8310c9c8d776",
   "metadata": {},
   "source": [
    "<img src=\"../../_static/assets/Generic/ray_logo.png\" width=\"20%\" loading=\"lazy\">\n",
    "\n",
    "## About this notebook\n",
    "\n",
    "### Is it right for you?\n",
    "\n",
    "This is an introductory notebook that gives a broad overview of the Ray project. It is right for you if:\n",
    "\n",
    "* you work with model inference problem or you want to scale your existing inference pipelines\n",
    "* <ToDo\\>\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "For this notebook you should have:\n",
    "\n",
    "* practical Python and machine learning experience\n",
    "* familiarity with Ray and Ray AIR. Equivalent to completing these training modules:\n",
    "  * [Overview of Ray](https://github.com/ray-project/ray-educational-materials/blob/main/Introductory_modules/Overview_of_Ray.ipynb)\n",
    "  * [Introduction to Ray AIR](https://github.com/ray-project/ray-educational-materials/blob/main/Introductory_modules/Introduction_to_Ray_AIR.ipynb)\n",
    "  * [Ray Core](https://github.com/ray-project/ray-educational-materials/tree/main/Ray_Core)\n",
    "* familiarity with batch inference problem in ML\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "Upon completion of this notebook, you will know about:\n",
    "\n",
    "* Inference patterns\n",
    "* Architectures how to scale inference with Ray\n",
    "\n",
    "### What will you do?\n",
    "\n",
    "<ToDo/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a02709-0ed5-489a-b344-27005261eb72",
   "metadata": {},
   "source": [
    "## Part 1: Patterns and architectures for scalable inference\n",
    "\n",
    "<ToDo, part 1>\n",
    "\n",
    "ML model lifecycle - diagram with focus\n",
    "\n",
    "### Patterns for resilient serving\n",
    "\n",
    "Book chapters\n",
    "* Stateless Serving Function - implement with Ray Tasks\n",
    "* Batch Serving - implement with Actors\n",
    "\n",
    "### Ray architectures for scalable inference\n",
    "\n",
    "* Stateless inference - Ray Task\n",
    "* Stateful inference - Ray Actors\n",
    "* Ray ActorPool - Increment of the previous approach - utility lib.\n",
    "* Ray AIR Datasets\n",
    "* Ray AIR BatchPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf547df-6663-47e4-99df-de54ac0cd5ef",
   "metadata": {},
   "source": [
    "## Part 2: Notes on data and model\n",
    "\n",
    "### Data\n",
    "\n",
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c018d564-0e4a-453f-9f7a-74466c0273e8",
   "metadata": {},
   "source": [
    "## Part 3: Vanilla implementation\n",
    "\n",
    "*(This implementation is inspired by the [Semantic segmentation](https://huggingface.co/docs/transformers/tasks/semantic_segmentation) guide. Date accessed: Nov 4th, 2022.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70f31fe-e8a9-41ea-857c-e2e2b36503e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from Hugging Face\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"scene_parse_150\", split=\"train[:50]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98814d17-d57d-4bdd-ba1c-8e64880f6361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make tr/test splits\n",
    "ds = ds.train_test_split(test_size=0.2)\n",
    "train_ds = ds[\"train\"]\n",
    "test_ds = ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559ad8bf-3b38-4328-9445-ee959540697e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print one example\n",
    "train_ds[0][\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e32feab-170b-4220-a263-bb678592d106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the mappings from the Hub and create the id2label and label2id dictionaries\n",
    "\n",
    "import json\n",
    "from huggingface_hub import hf_hub_url, hf_hub_download\n",
    "\n",
    "repo_id = \"huggingface/label-files\"\n",
    "filename = \"ade20k-id2label.json\"\n",
    "id2label = json.load(open(hf_hub_download(repo_id=repo_id, filename=filename, repo_type=\"dataset\"), \"r\"))\n",
    "\n",
    "id2label = {int(k): v for k, v in id2label.items()}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "num_labels = len(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0da5ecf-8a1e-4588-b010-f4a21aad0ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing and augmentations\n",
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"nvidia/mit-b0\", reduce_labels=True)\n",
    "\n",
    "## Augmentations\n",
    "from torchvision.transforms import ColorJitter\n",
    "\n",
    "jitter = ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dfdb96-0f12-4a96-abfd-cf7d8373f2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and validate transforms\n",
    "def train_transforms(example_batch):\n",
    "    images = [jitter(x) for x in example_batch[\"image\"]]\n",
    "    labels = [x for x in example_batch[\"annotation\"]]\n",
    "    inputs = feature_extractor(images, labels)\n",
    "    return inputs\n",
    "\n",
    "def val_transforms(example_batch):\n",
    "    images = [x for x in example_batch[\"image\"]]\n",
    "    labels = [x for x in example_batch[\"annotation\"]]\n",
    "    inputs = feature_extractor(images, labels)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e44f917-3de0-47b2-a3dd-3392a773d4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run transform on data\n",
    "train_ds.set_transform(train_transforms)\n",
    "test_ds.set_transform(val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87778f10-37df-4c3d-b473-1d3a1d44ade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pretrained model\n",
    "from transformers import AutoModelForSemanticSegmentation\n",
    "\n",
    "pretrained_model_name = \"nvidia/mit-b0\"\n",
    "model = AutoModelForSemanticSegmentation.from_pretrained(\n",
    "    pretrained_model_name, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da131bf-a4ae-4e68-9b0c-fd72eccb094a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate TrainingArguments\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"segformer-b0-scene-parse-150\",\n",
    "    learning_rate=6e-5,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    save_total_limit=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=20,\n",
    "    eval_steps=20,\n",
    "    logging_steps=1,\n",
    "    eval_accumulation_steps=5,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12533b17-54a0-495c-9319-642372f77025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval metric from Evaluate lib.\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"mean_iou\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8e0986-93af-44d9-bbe3-afa65caa88e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metrics\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    with torch.no_grad():\n",
    "        logits, labels = eval_pred\n",
    "        logits_tensor = torch.from_numpy(logits)\n",
    "        logits_tensor = torch.nn.functional.interpolate(\n",
    "            logits_tensor,\n",
    "            size=labels.shape[-2:],\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        ).argmax(dim=1)\n",
    "\n",
    "        pred_labels = logits_tensor.detach().cpu().numpy()\n",
    "        metrics = metric.compute(\n",
    "            predictions=pred_labels,\n",
    "            references=labels,\n",
    "            num_labels=num_labels,\n",
    "            ignore_index=255,\n",
    "            reduce_labels=False,\n",
    "        )\n",
    "        for key, value in metrics.items():\n",
    "            if type(value) is np.ndarray:\n",
    "                metrics[key] = value.tolist()\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1a07cb-f838-4db6-815c-5c4eca4d905b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Trainer\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3622810-bf77-4ff5-a7ca-9bec0da81d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d059bb-0e4a-4b82-8f85-aca7db10b57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "# check example image from test\n",
    "image = test_ds[0][\"image\"]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9443bc-584d-4b4a-b616-56e6c3bb4649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process img\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # use GPU if available, otherwise use a CPU\n",
    "encoding = feature_extractor(image, return_tensors=\"pt\")\n",
    "pixel_values = encoding.pixel_values.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b926356e-338b-4848-92a5-8315346d1561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference - get logits\n",
    "outputs = model(pixel_values=pixel_values)\n",
    "logits = outputs.logits.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e14197-a2c2-4a01-b81b-f50ed41550a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale img\n",
    "upsampled_logits = torch.nn.functional.interpolate(\n",
    "    logits,\n",
    "    size=image.size[::-1],\n",
    "    mode=\"bilinear\",\n",
    "    align_corners=False,\n",
    ")\n",
    "\n",
    "pred_seg = upsampled_logits.argmax(dim=1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df44fc1b-2045-46f8-9686-0a49e6e2b55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize pred\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "color_seg = np.zeros((pred_seg.shape[0], pred_seg.shape[1], 3), dtype=np.uint8)\n",
    "palette = np.array(ade_palette())\n",
    "for label, color in enumerate(palette):\n",
    "    color_seg[pred_seg == label, :] = color\n",
    "color_seg = color_seg[..., ::-1]  # convert to BGR\n",
    "\n",
    "img = np.array(image) * 0.5 + color_seg * 0.5  # plot the image with the segmentation map\n",
    "img = img.astype(np.uint8)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efe3937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize pred\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "color_seg = np.zeros((pred_seg.shape[0], pred_seg.shape[1], 3), dtype=np.uint8)\n",
    "palette = np.array(ade_palette())\n",
    "for label, color in enumerate(palette):\n",
    "    color_seg[pred_seg == label, :] = color\n",
    "color_seg = color_seg[..., ::-1]  # convert to BGR\n",
    "\n",
    "img = np.array(image) * 0.5 + color_seg * 0.5  # plot the image with the segmentation map\n",
    "img = img.astype(np.uint8)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d391b2-82da-449c-a616-61d00f6153c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef935c7b-5caa-40b2-b6f6-a23887ba0bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfd64fe-1a93-4148-9b0a-6de0cb16ae0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f84d6c-7673-41c5-9609-a370151c9e25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dd130c-07e1-47af-9282-b131f5927507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d33c74b3-9d0f-469f-83ec-7e5557104489",
   "metadata": {},
   "source": [
    "## Part 4: Stateless inference - Ray Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e2f9e2-945b-4b08-95ec-74b09b8b07d5",
   "metadata": {},
   "source": [
    "## Part 5: Stateful inference - Ray Actors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7036efa3-2dd1-4b55-82e3-da81a0a33f13",
   "metadata": {},
   "source": [
    "## Part 6: Stateful inference - Ray ActorPool - Increment of the previous approach - utility lib."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7289f04c-337c-47e1-a415-a5c5d80b61cc",
   "metadata": {},
   "source": [
    "## Part 7: Ray AIR Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460ac28a-3739-4456-99a3-1579c1081db3",
   "metadata": {},
   "source": [
    "## Part 8: Ray AIR BatchPredictor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
