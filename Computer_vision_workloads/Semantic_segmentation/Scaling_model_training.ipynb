{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling model training\n",
    "\n",
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Generic/ray_logo.png\" width=\"20%\" loading=\"lazy\">\n",
    "\n",
    "## About this notebook\n",
    "\n",
    "### Is this module right for you?\n",
    "\n",
    "This module guides you through distributed model training with Ray. Through fine-tuning a transformer for a computer vision task, ML practitioners will learn how to scale training workloads using deep learning models on large datasets.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "For this notebook, you should satisfy the following minimum requirements:\n",
    "-   Practical Python knowledge.\n",
    "-   Familiarity with training deep learning models.\n",
    "-   Experience with Ray equivalent to completing the following training modules:\n",
    "    -   [Overview of Ray](https://github.com/ray-project/ray-educational-materials/blob/main/Introductory_modules/Overview_of_Ray.ipynb)\n",
    "    -   [Introduction to Ray AIR](https://github.com/ray-project/ray-educational-materials/blob/main/Introductory_modules/Introduction_to_Ray_AIR.ipynb)\n",
    "    \n",
    "### Learning objectives\n",
    "\n",
    "-   Understand the challenges associated with distributing model training across multiple GPUs.\n",
    "-   Implement the data parallelism design pattern using Ray Datasets.\n",
    "-   Fine-tune a transformer model on an image dataset using Ray Train.\n",
    "-   Evaluate the trained model by performing inference on the test set.\n",
    "\n",
    "### What will you do?\n",
    "\n",
    "-   Distributed model training overview\n",
    "    -   Learn about why training large machine learning models requires a distributed solution.\n",
    "    -   Refresh your knowledge of the data parallelism design pattern.\n",
    "-   Example: Fine-tuning a model for image segmentation.\n",
    "    -   Background\n",
    "        -   Data - MITADE20K benchmark dataset of scene images.\n",
    "        -   Model - Segformer transformer for semantic segmentation.\n",
    "    -   Getting started\n",
    "        -   Start Ray cluster and set-up environment.\n",
    "    -   Data ingest\n",
    "        -   Batch and transform raw data into training inputs using Ray Data.\n",
    "    -   Distributed training\n",
    "        -   Fine-tune transformer model on benchmark dataset using Ray Train.\n",
    "    -   Evaluation\n",
    "        -   Perform inference on the test set to assess performance using Ray AIR's BatchPredictor.\n",
    "-   Conclusion\n",
    "    -   Summarize the distributed training approach as well as the Ray components at each stage of the pipeline."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed model training\n",
    "\n",
    "As the development of machine learning models advances, their [size continues to balloon](https://epochai.org/blog/machine-learning-model-sizes-and-the-parameter-gap). Training these large neural networks can take a prohibitively long time and requires an increasingly [massive amount of compute](https://www.hyro.ai/glossary/gpt-3#:~:text=To%20be%20exact%2C%20GPT%2D3,amount%20of%20time%20is%20unimaginable.).\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/ai_compute_annotated.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|OpenAI's blog[\"AI and compute\"](https://openai.com/research/ai-and-compute) reports that the amount of compute needed to train the largest models has roughly doubled every 3.5 months since 2012, with no signs of this trend slowing down. Annotated original chart with trend lines overlaid.|\n",
    "\n",
    "Distributing this workload presents unique challenges involved with orchestrating multiple machines to produce one computationally synchronized result. This problem only compounds when working with heterogeneous resources, multiple tuning experiments, or a model that can't fit on a single GPU. To address these issues, machine learning practitioners have developed a variety of techniques to parallelize training across nodes, one of which is data parallelism.\n",
    "\n",
    "### Data parallelism\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong>Data parallelism:</a></strong> a design pattern that trains replicas of the model on different subsets of a large dataset, periodically synchronizing weights to produce a fully trained result. This method requires that a model's parameters, or weights, are able to fit on a single GPU's memory.\n",
    "</div>\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Scaling_model_training/data_parallelism.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|A large dataset is sharded across multiple worker nodes each containing a model copy. Gradients calculated on independent nodes are continuously synchronized with others to produce a final trained model.|\n",
    "\n",
    "Ray Train provides distributed data parallel training capabilities. Its integration with Ray AIR also allows for convenient parallelization of data ingestion and pre-processing, hyperparameter tuning, batch inference, and serving. This provides a unified compute layer for the machine learning pipeline, eliminating the need to stitch together independent scaling solutions at each stage. In the next section, you will implement this design pattern using a transformer model and scene images to accomplish a computer vision task.\n",
    "\n",
    "Note: There are other techniques for distributed training such as model parallelism which divides the model itself across multiple GPUs. However, this module will focus on implementing data parallelism."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background on semantic segmentation\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong>Semantic segmentation:</a></strong> a computer vision task that assigns labels to object regions in a scene, pixel-by-pixel. Similar to object detection, this approach involves dividing an image into multiple semantic categories such as couch, person, car, or sky.\n",
    "</div>\n",
    "\n",
    "In this hands-on example, you will implement the data parallelism design pattern by fine-tuning a pretrained transformer model on scene image data.\n",
    "\n",
    "### Data\n",
    "\n",
    "#### MIT ADE20K - scene parsing benchmark\n",
    "\n",
    "The [MIT ADE20K Dataset](http://sceneparsing.csail.mit.edu/) (also known as \"SceneParse150\") provides the largest open source dataset for scene parsing. It is often used as a standard for assessing semantic segmentation model performance due to its high-quality annotations.\n",
    "\n",
    "You will use the training set for fine-tuning and the unlabeled test set for evaluation.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Scaling_inference/scene.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Test image on the left vs. predicted result. ([Source](https://github.com/CSAILVision/semantic-segmentation-pytorch))|\n",
    "\n",
    "Dataset highlights\n",
    "\n",
    "-   20k annotated, scene-centric training images\n",
    "-   3.3k unlabeled test images\n",
    "-   150 [semantic categories](https://docs.google.com/spreadsheets/d/1se8YEtb2detS7OuPE86fXGyD269pMycAWe2mtKUj2W8/edit?usp=sharing) (such as person, car, bed, sky, etc.)\n",
    "\n",
    "### Model\n",
    "\n",
    "#### SegFormer - transformer-based framework for semantic segmentation\n",
    "\n",
    "[SegFormer](https://arxiv.org/pdf/2105.15203.pdf) is an effective semantic segmentation method based on a transformer architecture. [Transformers](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) are a type of deep learning architecture that process sequential data via a series of self-attention layers and then transform them via a feedforward neural network.\n",
    "\n",
    "What sets SegFormer apart from previous transformer-based approaches are two key features:\n",
    "\n",
    "1.  A hierarchically structured transformer encoder which does not depend on positional encoding that avoids interpolation when training and testing resolutions differ.\n",
    "2.  A lightweight MLP layer that avoids complex decoders.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Scaling_inference/segformer_architecture.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Segformer architecture illustrated in the [original paper](https://arxiv.org/pdf/2105.15203.pdf).|\n",
    "\n",
    "You will use a general, pre-trained SegFormer model to fine-tune on [MITADE20K](http://sceneparsing.csail.mit.edu/) image data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data ingest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You have successfully gained experience in using Ray Train to fine-tune a vision transformer model for semantic segmentation.In the upcoming module, you will be able to build on this example by conducting a series of hyperparameter tuning experiments using Ray Tune.\n",
    "\n",
    "### Summary\n",
    "\n",
    "-   Distributed model training\n",
    "    -   Training and fine-tuning large neural networks requires a massive amount of compute, so the only solution is to distribute this workload.\n",
    "    -   Data parallelism offers a pattern for sharding a large dataset across multiple machines for training and gradient synchronization.\n",
    "    -   This orchestration and maintenance is challenging, and Ray AIR offers a unified compute solution to scale this workload that integrates well with other stages in the pipeline.\n",
    "-   Fine-tuning Segformer on MITADE20K\n",
    "    -   Data ingest\n",
    "        -   Ray Data can be used to ingest and preprocess training images. These same transformations can be applied during tuning, inference, and serving.\n",
    "    -   Distributed training\n",
    "        -   Ray Train can fine-tune a transformer model, in this case implementing the data parallel design pattern by running PyTorch's [Distributed Data Parallel](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html) as the backend.\n",
    "    -   Evaluation\n",
    "        -   You used Ray AIR's BatchPredictor to assess performance of the fine-tuned model by running inference.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect with the Ray community\n",
    "\n",
    "You can learn and get more involved with the Ray community of developers and researchers:\n",
    "\n",
    "* [**Ray documentation**](https://docs.ray.io/en/latest)\n",
    "\n",
    "* [**Official Ray site**](https://www.ray.io/)  \n",
    "Browse the ecosystem and use this site as a hub to get the information that you need to get going and building with Ray.\n",
    "\n",
    "* [**Join the community on Slack**](https://forms.gle/9TSdDYUgxYs8SA9e8)  \n",
    "Find friends to discuss your new learnings in our Slack space.\n",
    "\n",
    "* [**Use the discussion board**](https://discuss.ray.io/)  \n",
    "Ask questions, follow topics, and view announcements on this community forum.\n",
    "\n",
    "* [**Join a meetup group**](https://www.meetup.com/Bay-Area-Ray-Meetup/)  \n",
    "Tune in on meet-ups to listen to compelling talks, get to know other users, and meet the team behind Ray.\n",
    "\n",
    "* [**Open an issue**](https://github.com/ray-project/ray/issues/new/choose)  \n",
    "Ray is constantly evolving to improve developer experience. Submit feature requests, bug-reports, and get help via GitHub issues.\n",
    "\n",
    "* [**Become a Ray contributor**](https://docs.ray.io/en/latest/ray-contribute/getting-involved.html)  \n",
    "We welcome community contributions to improve our documentation and Ray framework.\n",
    "\n",
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Generic/ray_logo.png\" width=\"20%\" loading=\"lazy\">"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
