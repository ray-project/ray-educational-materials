{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Ray AI Runtime (AIR)\n",
    "---\n",
    "(*Suggested Time to Complete: 30 minutes*)\n",
    "\n",
    "![Ray AIR](images/Ray_AIR.png)\n",
    "*Figure 1*\n",
    "\n",
    "Ray AI Runtime (AIR) is a unified set of libraries built on top of Ray for distributed data processing, model training, tuning, model serving, and reinforcement learning, all in Python. AIR provides simple scalable machine learning for individual workloads and end-to-end workflows, bringing together an ecosystem of integrations.\n",
    "\n",
    "Before we lay out each library and their unique jobs to be done, let's take a moment to motivate Ray AIR by taking a high-level view of the data science and machine learning workflow.\n",
    "\n",
    "Developing a machine learning system is an iterative and often cyclical process that roughly touches on the following stages:\n",
    "\n",
    "1. Business Needs: work with stakeholders to identify business requirements and align on which metric to optimize\n",
    "2. Data Collection: source and collect data and labels\n",
    "3. Feature Engineering: turn raw data into usable material\n",
    "4. Model Training: the learning part of machine learning\n",
    "5. Model Evaluation: try to improve upon your baseline model with hyperparameter tuning or more feature engineering or even a more relevant set of data\n",
    "6. Deployment: deploy your solution to production and/or serve your model to the end user\n",
    "\n",
    "Each of the five main native libraries that Ray AIR wraps tackles a specific piece of the ML specific tasks outlined above, and because this abstraction layer is built on top of Ray Core, it is distributed by nature.\n",
    "\n",
    "1. [Ray Data](https://docs.ray.io/en/latest/data/dataset.html): load and manipulate data\n",
    "2. [Ray Train](https://docs.ray.io/en/latest/train/train.html): scales model training for popular ML frameworks\n",
    "3. [Ray Tune](https://docs.ray.io/en/latest/tune/index.html): scales experiment execution and hyperparameter tuning\n",
    "4. [Ray Serve](https://docs.ray.io/en/latest/serve/index.html): scales model serving\n",
    "5. [Ray RLlib](https://docs.ray.io/en/latest/rllib/index.html): for distributed reinforment learning workloads\n",
    "\n",
    "Let's contextualize Ray Data, Train, Tune, and Serve with a common ML pipeline and discuss how each library intersects with the distinct steps we need to distribute this end-to-end example. Towards the end, we will examine a reinforment learning specific workload for RLlib.\n",
    "\n",
    "**Learning Objectives**\n",
    "1. Understand the high-level data science libraries that compose Ray AIR: Data, Train, Tune, Serve, and RLlib\n",
    "2. Understand how to use Ray AIR as a unified toolkit to write an end-to-end ML application in Python\n",
    "3. Practice key concepts from each stage of the ML pipeline\n",
    "    - Data - use out-of-the-box preprocessors\n",
    "    - Train - use AIR Trainers for supported ML frameworks\n",
    "    - Tune - use AIR Tuners for hyperparameter search\n",
    "    - BatchPredictor - use AIR `BatchPredictor` to load model from best checkpoint for batch inference\n",
    "    - Serve - use `PredictorDeployment` for online inference\n",
    "    - RLlib - distribute RL workloads with RLlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Example: Breast Cancer Biopsy Classification\n",
    "***\n",
    "To illustrate Ray AIR's capabilities, we will walk through a practical, end-to-end example of building a typical machine learning pipeline using Ray Data, Train, Tune, and Serve. We will build a binary classification model based off the [Wisconsin Breast Cancer Dataset](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)) which contains 569 samples of malignant and benign biopsies. Our workflow will start by loading data, performing some basic preprocessing, train the model with XGBoost, tune hyperparameters, perform batch inference, and then finally deploy our application online.\n",
    "\n",
    "<!-- As we encounter each stage, we will formally introduce the Ray library that does the heavy lifting. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ray Data\n",
    "***\n",
    "First up, we want to load in the breast cancer dataset and transform its raw data into features that will be input to our machine learning model. Ray Datasets are the standard way to load and exchange data in Ray libraries and applications. They provide basic distributed data transformations such as map, filter, and repartition and are compatible with a variety of file formats, data sources, and distributed frameworks.\n",
    "\n",
    "**Key Concepts**\n",
    "\n",
    "`Dataset`: The standard way to load and exchange data in Ray AIR. In AIR, Datasets are used extensively for data loading, preprocessing, and batch inference.\n",
    "\n",
    "`Preprocessors`: Preprocessors are primitives that can be used to transform input data into features. Preprocessors operate on Datasets, which makes them scalable and compatible with a variety of datasources and dataframe libraries. A Preprocessor is fitted during Training, and applied at runtime in both Training and Serving on data batches in the same way. AIR comes with a collection of built-in preprocessors, and you can also define your own with simple templates which you can read more about in our [User Guide](https://docs.ray.io/en/latest/ray-air/preprocessors.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Relevant Packages + Starting Ray\n",
    "To start, we'll import Ray (check out our [installation instructions](https://docs.ray.io/en/latest/ray-overview/installation.html)) and start a Ray cluster on our machine that can utilize all the cores available to you as workers. We use `ray.is_initialized` to ensure that we only have one Ray cluster active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, os, random, warnings\n",
    "import pandas as pd\n",
    "import ray\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "\n",
    "if ray.is_initialized:\n",
    "    ray.shutdown()\n",
    "\n",
    "ray.init(logging_level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Ray Dataset\n",
    "Here, we read in the data from an S3 `.csv` datasource. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ray.data.read_csv(\"s3://anonymous@air-example-data/breast_cancer.csv\")\n",
    "\n",
    "# split data into training and test subsets\n",
    "train_dataset, valid_dataset = dataset.train_test_split(test_size=0.3)\n",
    "test_dataset = valid_dataset.drop_columns([\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**👾 Coding Excercise 👾**\n",
    "\n",
    "There exist many [`Dataset` API elements](https://docs.ray.io/en/latest/data/api/dataset.html#) available for common transformations and operations. Using the above as a reference:\n",
    "1. inspect the schema\n",
    "2. visualize the first five samples\n",
    "3. evaluate whether this dataset contains extreme outliers; if so, drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "In the code below, we define a `StandardScaler` preprocessor because we wish to normalize the *approximately normal* `mean_radius` and `mean_texture` columns.\n",
    "\n",
    "What's nice about a Ray AIR `Preprocessor` is that it is automatically incorporated into an end-to-end application built with AIR.\n",
    "\n",
    "- During Training: `Preprocessor` is passed into a `Trainer` to `fit` and `transform` input `Dataset`s.\n",
    "- During Tuning: each `Trial` will isntantiate its own copy of the `Preprocessor` and the fitting and transofrmation logic will occur once per `Trial`\n",
    "- During Checkpoint: the `Preprocessor` is saved in the `Checkpoint` is if was passed into the `Trainer`\n",
    "- During Predicting: if the `Checkpoint` contains a `Preprocessor`, then it will be used to call `transform_batch` on input batches prior to performing inference\n",
    "\n",
    "This kind of heavy lifting behind the scenes is one of the major advantages of Ray AIR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.preprocessors import StandardScaler\n",
    "\n",
    "# create a preprocessor to scale some columns\n",
    "preprocessor = StandardScaler(columns=[\"mean radius\", \"mean texture\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ray Train\n",
    "***\n",
    "Following data preprocessing, we can move forward with defining our model and fitting it on our training set. Ray Train is a lightweight library for distributed deep learning that allows you to easily supercharge your distributed training on Ray.\n",
    "\n",
    "**Key Concept**\n",
    "\n",
    "`Trainer`: Trainers are wrapper classes around third-party training frameworks such as XGBoost, Pytorch, and Tensorflow. They are built to help integrate with core Ray Actors (for distribution), Ray Tune, and Ray Datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define AIR `Trainer`\n",
    "\n",
    "In Ray AIR, `Trainer`s provide a way to scale out training with popular machine learning frameworks. As part of Ray Train, `Trainer`s enable users to run distributed multi-node training with fault tolerance. Fully integrated with the Ray ecosystem, Trainers leverage Ray Data to enable scalable preprocessing and performant distributed data ingestion. Also, Trainers can be composed with `Tuner`s for distributed hyperparameter tuning. After executing training, `Trainer`s output the trained model in the form of a `Checkpoint`, which can be used for batch or online prediction inference.\n",
    "\n",
    "There are three broad categories of Trainers that AIR offers:\n",
    "\n",
    "- Deep Learning Trainers (Pytorch, Tensorflow, Horovod)\n",
    "- Tree-based Trainers (XGBoost, LightGBM)\n",
    "- Other ML frameworks (HuggingFace, Scikit-Learn, RLlib)\n",
    "\n",
    "In the example below, we will use an `XGBoostTrainer`to perform binary classification our breast cancer biopsy samples. To construct a `Trainer`, you extent drom the `BaseTrainer` interface by providing:\n",
    "\n",
    "- a `scaling_config` which specifies how many parallel training workers and what type of resources (CPUs/GPUs) to use per worker during training.\n",
    "- a `run_config` which configures a variety of runtime parameters such as fault tolerance, logging, and callbacks\n",
    "- a collection of datasets and a preprocessor for the provided datasets which configures preprocessing and the datasets to ingest from\n",
    "- `resume_from_checkpoint` which is a checkpoint path to resume from, should your training run be interrupted\n",
    "\n",
    "In summary, the steps for our example are:\n",
    "\n",
    "1. define the parallelism for Ray compute\n",
    "2. define the XGBoost parameters for training\n",
    "3. supply the preprocessor for fitting and transforming dataset during training and validation\n",
    "4. provide the datasets for training and validation\n",
    "5. invoke `trainer.fit` to fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.air.config import ScalingConfig\n",
    "from ray.train.xgboost import XGBoostTrainer\n",
    "\n",
    "trainer = XGBoostTrainer(\n",
    "    scaling_config=ScalingConfig(\n",
    "        # Number of workers to use for data parallelism.\n",
    "        num_workers=2,\n",
    "        # Whether to use GPU acceleration.\n",
    "        use_gpu=False),\n",
    "    label_column=\"target\",\n",
    "    num_boost_round=20,\n",
    "    params={\n",
    "        # XGBoost specific params\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": [\"logloss\", \"error\"],\n",
    "    },\n",
    "    # our train and validation dataset and preprocessor\n",
    "    datasets={\"train\": train_dataset, \"valid\": valid_dataset},\n",
    "    preprocessor=preprocessor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer objects produce a Result object after calling `.fit()`. These objects contain training metrics as well as checkpoints to retrieve the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the trainer\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ray Tune\n",
    "***\n",
    "Ray Tune is a Python library for fast hyperparameter tuning at scale. Easily distribute your trial runs to quickly find the best hyperparameters.\n",
    "\n",
    "**Key Concept**\n",
    "\n",
    "`Tuner`: Tuners offer scalable hyperparameter tuning as part of Ray Tune. Tuners can work seamlessly with any Trainer but also can support arbitrary training functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use AIR `Tuner` for Hyperparameter Search\n",
    "What if you want to do hyperparameter optimization during training and use the best config for the model? Well, you can then use Tuner and supply your training function, `Trainer`, as part of the argument, along with other Tuner configuration.\n",
    "\n",
    "1. define the hyperparameter space\n",
    "2. define `TuneConfig` for number of trials and parallelism\n",
    "3. invoke `tuner.fit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "\n",
    "param_space = {\"params\": {\"max_depth\": tune.randint(1, 9)}}\n",
    "metric = \"train-logloss\"\n",
    "our_mode=\"min\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.tuner import Tuner, TuneConfig\n",
    "from ray.air.config import RunConfig\n",
    "\n",
    "tuner = Tuner(\n",
    "    trainer,\n",
    "    param_space=param_space,\n",
    "    tune_config=TuneConfig(num_samples=5, metric=metric, mode=our_mode),\n",
    ")\n",
    "# Execute tuning.\n",
    "result_grid = tuner.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ray AIR Checkpoints\n",
    "***\n",
    "The AIR trainers, tuners, and custom pretrained models generate Checkpoints. An AIR Checkpoint is a format for models that are used across different components of the Ray AI Runtime. This common format allows easy interoperability among AIR components and seamless integration with external supported machine learning frameworks.\n",
    "\n",
    "**Key Concept**\n",
    "\n",
    "`Checkpoints`: The AIR trainers, tuners, and custom pretrained model generate a framework-specific Checkpoint object. Checkpoints are a common interface for models that are used across different AIR components and libraries.\n",
    "\n",
    "`BatchPredictor`: loads the best model from a checkpoint to perform batch inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use AIR `BatchPredictor` for Batch Prediction\n",
    "Once you have trained and tuned your model, create a batch prdictor from best model using the `best_result.checkpoint` and do batch inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.batch_predictor import BatchPredictor\n",
    "from ray.train.xgboost import XGBoostPredictor\n",
    "\n",
    "batch_predictor = BatchPredictor.from_checkpoint(best_result.checkpoint, XGBoostPredictor)\n",
    "\n",
    "predicted_probabilities = batch_predictor.predict(test_dataset)\n",
    "print(\"PREDICTED PROBABILITIES\")\n",
    "predicted_probabilities.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Ray Serve\n",
    "***\n",
    "Ray Serve lets you serve machine learning models in real-time or batch using a simple Python API. Serve individual models or create composite model pipelines, where you can independently deploy, update, and scale individual components.\n",
    "\n",
    "**Key Concept**\n",
    "\n",
    "`Deployments`: Deploy the model as an inference service by using Ray Serve and the `PredictorDeployment` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `PredictorDeployment` for Online Inference\n",
    "Deploy the best model as an inference service by using Ray Serve and the `PredictorDeployment` class. After deploying the service, you can send requests to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import serve\n",
    "from fastapi import Request\n",
    "from ray.serve import PredictorDeployment\n",
    "from ray.serve.http_adapters import pandas_read_json\n",
    "\n",
    "serve.run(\n",
    "    PredictorDeployment.options(name=\"XGBoostService\", num_replicas=2, route_prefix=\"/rayair\").bind(\n",
    "        XGBoostPredictor, result.checkpoint, http_adapter=pandas_read_json\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "sample_input = test_dataset.take(1)\n",
    "sample_input = dict(sample_input[0])\n",
    "\n",
    "output = requests.post(\"http://localhost:8000/rayair\", json=[sample_input]).json()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "You've now just created a Ray Dataset, preprocessed some features, built a model with XGBoost, searched a hyperparameter space for the best configuration, loaded the best model from a checkpoint to perform batch inference, and served that model for online inference. Through this end-to-end example, you explored how to use Ray AIR to distribute an entire ML pipeline.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- `Datasets`\n",
    "- `Preprocessors`\n",
    "- `Trainers`\n",
    "- `Tuner`\n",
    "- `Checkpoints`\n",
    "- `BatchPredictor`\n",
    "- `Deployments`\n",
    "\n",
    "### Next Up\n",
    "\n",
    "Now that you've seen how you can use Ray AIR's unified toolkit to scale an end-to-end machine learning application, let's see how we can use it to scale individual workloads. In the next section we will cover a reinforcement learning example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Example\n",
    "words go here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray RLLib\n",
    "RLlib is the industry-standard reinforcement learning Python framework built on Ray. Designed for quick interation and a fast path to production, it includes 25+ latest algorithms that are all implemented to run at scale and in multi-agent mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "#### Key Concepts\n",
    "#### Key API Elements in This Section\n",
    "#### Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Resources\n",
    "---\n",
    "If you would like to practice your new skills further with some in-depth examples beyond the embedded coding excercises, take a look at this list of suggested problems:\n",
    "- Watch the Ray Summit Talk on [Introduction to Ray AIR](https://github.com/ray-project/hackathon5-algo)\n",
    "- Check out the [Ray AIR Documentation](https://docs.ray.io/en/latest/ray-air/getting-started.html)\n",
    "- Understand its [Components and APIs](https://docs.ray.io/en/latest/ray-air/package-ref.html)\n",
    "- Ray AIR [User Guides](https://docs.ray.io/en/latest/ray-air/user-guides.html) and [Examples](https://docs.ray.io/en/latest/ray-air/examples/index.html)\n",
    "\n",
    "\n",
    "# Next Steps\n",
    "---\n",
    "🎉 Congratulations! You have completed the tutorial on an Introduction to Ray AI Runtime! We dicussed each library in Ray AIR (Data, Train, Tune, Serve, RLLib) and saw some example machine learning workloads to be done with each. In the next module, we will introduce the ecosystem of integrated libraries runs on Ray Core's distributed execution engine, and with Ray Clusters, you can deploy your workloads on AWS, GCP, Azure, or on Kubernetes.\n",
    "\n",
    "From here, you can learn and get more involved with our active community of developers and researchers by checking out the following resources:\n",
    "- 💻 [Official Ray Website](https://www.ray.io/): Browse the ecosystem and use this site as a hub to get the information that you need to get going and building with Ray.\n",
    "- 💬 [Join the Community on Slack](https://forms.gle/9TSdDYUgxYs8SA9e8): Find friends to discuss your new learnings in our Slack space.\n",
    "- 📣 [Use the Discussion Board](https://discuss.ray.io/): Ask questions, follow topics, and view announcements on this community forum.\n",
    "- 🙋‍♀️ [Join a Meetup Group](https://www.meetup.com/Bay-Area-Ray-Meetup/): Tune in on meet-ups to listen to compelling talks, get to know other users, and meet the team behind Ray.\n",
    "- 🪲 [Open an Issue](https://github.com/ray-project/ray/issues/new/choose): Ray is constantly evolving to improve developer experience. Submit feature requests, bug-reports, and get help via GitHub issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "567405a8058597909526349386224fe35dd047505a91307e44ed44be00113429"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
