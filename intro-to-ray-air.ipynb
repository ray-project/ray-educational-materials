{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Ray AI Runtime (AIR)\n",
    "---\n",
    "(*Suggested Time to Complete: 30 minutes*)\n",
    "\n",
    "![Ray AIR](images/Ray_AIR.png)\n",
    "*Figure 1*\n",
    "\n",
    "Ray AI Runtime (AIR) is a unified set of libraries built on top of Ray for distributed data processing, model training, tuning, model serving, and reinforcement learning, all in Python. AIR provides simple scalable machine learning for individual workloads and end-to-end workflows, bringing together an ecosystem of integrations.\n",
    "\n",
    "Before we lay out each library and their unique jobs to be done, let's take a moment to motivate Ray AIR by taking a high-level view of the data science and machine learning workflow.\n",
    "\n",
    "Developing a machine learning system is an iterative and often cyclical process that roughly touches on the following stages:\n",
    "\n",
    "1. Business Needs: work with stakeholders to identify business requirements and align on which metric to optimize\n",
    "2. Data Collection: source and collect data and labels\n",
    "3. Feature Engineering: turn raw data into usable material\n",
    "4. Model Training: the learning part of machine learning\n",
    "5. Model Evaluation: try to improve upon your baseline model with hyperparameter tuning or more feature engineering or even a more relevant set of data\n",
    "6. Deployment: deploy your solution to production and/or serve your model to the end user\n",
    "\n",
    "Each of the five main native libraries that Ray AIR wraps tackles a specific piece of the ML specific tasks outlined above, and because this abstraction layer is built on top of Ray Core, it is distributed by nature.\n",
    "\n",
    "1. Ray Data: load and manipulate data\n",
    "2. Ray Train: scales model training for popular ML frameworks\n",
    "3. Ray Tune: scales experiment execution and hyperparameter tuning\n",
    "4. Ray Serve: scales model serving\n",
    "5. Ray RLlib: for distributed reinforment learning workloads\n",
    "\n",
    "Let's contextualize Ray Data, Train, Rune, and Serve with a common ML pipeline and discuss how each library intersects with the distinct steps we need to distribute this end-to-end example. Towards the end, we will examine a reinforment learning specific workload for RLlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray AIR End-to-End XGBoost Example\n",
    "- Talk about dataset, task, the outline each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "# Load data.\n",
    "dataset = ray.data.read_csv(\"s3://anonymous@air-example-data/breast_cancer.csv\")\n",
    "\n",
    "# Split data into train and validation.\n",
    "train_dataset, valid_dataset = dataset.train_test_split(test_size=0.3)\n",
    "\n",
    "# Create a test dataset by dropping the target column.\n",
    "test_dataset = valid_dataset.drop_columns(cols=[\"target\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a preprocessor to scale some columns.\n",
    "from ray.data.preprocessors import StandardScaler\n",
    "\n",
    "preprocessor = StandardScaler(columns=[\"mean radius\", \"mean texture\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.air.config import ScalingConfig\n",
    "from ray.train.xgboost import XGBoostTrainer\n",
    "\n",
    "trainer = XGBoostTrainer(\n",
    "    scaling_config=ScalingConfig(\n",
    "        # Number of workers to use for data parallelism.\n",
    "        num_workers=2,\n",
    "        # Whether to use GPU acceleration.\n",
    "        use_gpu=False,\n",
    "    ),\n",
    "    label_column=\"target\",\n",
    "    num_boost_round=20,\n",
    "    params={\n",
    "        # XGBoost specific params\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        # \"tree_method\": \"gpu_hist\",  # uncomment this to use GPUs.\n",
    "        \"eval_metric\": [\"logloss\", \"error\"],\n",
    "    },\n",
    "    datasets={\"train\": train_dataset, \"valid\": valid_dataset},\n",
    "    preprocessor=preprocessor,\n",
    ")\n",
    "result = trainer.fit()\n",
    "print(result.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "\n",
    "param_space = {\"params\": {\"max_depth\": tune.randint(1, 9)}}\n",
    "metric = \"train-logloss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.tuner import Tuner, TuneConfig\n",
    "from ray.air.config import RunConfig\n",
    "\n",
    "tuner = Tuner(\n",
    "    trainer,\n",
    "    param_space=param_space,\n",
    "    tune_config=TuneConfig(num_samples=5, metric=metric, mode=\"min\"),\n",
    ")\n",
    "# Execute tuning.\n",
    "result_grid = tuner.fit()\n",
    "\n",
    "# Fetch the best result.\n",
    "best_result = result_grid.get_best_result()\n",
    "print(\"Best Result:\", best_result)\n",
    "# Best Result: Result(metrics={'loss': 0.278409322102863, ...})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.batch_predictor import BatchPredictor\n",
    "from ray.train.xgboost import XGBoostPredictor\n",
    "\n",
    "# You can also create a checkpoint from a trained model using\n",
    "# `XGBoostCheckpoint.from_model`.\n",
    "checkpoint = best_result.checkpoint\n",
    "\n",
    "batch_predictor = BatchPredictor.from_checkpoint(checkpoint, XGBoostPredictor)\n",
    "\n",
    "predicted_probabilities = batch_predictor.predict(test_dataset)\n",
    "predicted_probabilities.show()\n",
    "# {'predictions': 0.9970690608024597}\n",
    "# {'predictions': 0.9943051934242249}\n",
    "# {'predictions': 0.00334902573376894}\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Data\n",
    "***\n",
    "Ray Datasets are the standard way to load and exchange data in Ray libraries and applications. They provide basic distributed data transformations such as map, filter, and repartition, and are compatible with a variety of file formats, data sources, and distributed frameworks.\n",
    "\n",
    "**Datasets**: Ray Datasets are the standard way to load and exchange data in Ray AIR. In AIR, Datasets are used extensively for data loading, preprocessing, and batch inference.\n",
    "\n",
    "**Preprocessors**: Preprocessors are primitives that can be used to transform input data into features. Preprocessors operate on Datasets, which makes them scalable and compatible with a variety of datasources and dataframe libraries.\n",
    "\n",
    "A Preprocessor is fitted during Training, and applied at runtime in both Training and Serving on data batches in the same way. AIR comess with a collection of built-in prepreocessors, and you can also define your own with simple templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "#### Key Concepts\n",
    "- Datasets\n",
    "- Preprocessors\n",
    "\n",
    "#### Key API Elements in This Section\n",
    "#### Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Train\n",
    "Ray Train is a lightweight library for distributed deep learning that allows you to easily supercharge your distributed PyTorch and TensorFlow training on Ray.\n",
    "\n",
    "**Trainer**: Trainers are wrapper classes around third-party training frameworks such as XGBoost and Pytoch. They are built to help integrate with core Ray actors (for distribution), Ray Tune, and Ray Datasets.\n",
    "\n",
    "**Checkpoints**: The AIR trainers, tuners, and custom pretrained model generate a framework-specific Checkpoint object. Checkpoints are a common interface for models that are used across different AIR components and libraries.\n",
    "\n",
    "**Batch Predictor**: You can take a checkpoint and do batch inference using the BatchPredictor object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "#### Key Concepts\n",
    "#### Key API Elements in This Section\n",
    "#### Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Tune\n",
    "Ray Tune is a Python library for fast hyperparameter tuning at scale. Easily distribute your trial runs to quickly find the best hyperparameters.\n",
    "\n",
    "**Tuner**: Tuners offer scalable hyperparameter tuning as part of Ray Tune. Tuners can work seamlessly with any Trainer but also can support arbitrary training functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "#### Key Concepts\n",
    "#### Key API Elements in This Section\n",
    "#### Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Serve\n",
    "Ray Serve lets you serve machine learning models in real-time or batch using a simple Python API. Serve individual models or create composite model pipelines, where you can independently deploy, update, and scale individual components.\n",
    "\n",
    "**Deployments**: Deploy the model as an inference service by using Ray Serve and the `PredictorDeployment` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "#### Key Concepts\n",
    "#### Key API Elements in This Section\n",
    "#### Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray RLLib\n",
    "RLlib is the industry-standard reinforcement learning Python framework built on Ray. Designed for quick interation and a fast path to production, it includes 25+ latest algorithms that are all implemented to run at scale and in multi-agent mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "#### Key Concepts\n",
    "#### Key API Elements in This Section\n",
    "#### Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Resources\n",
    "---\n",
    "If you would like to practice your new skills further with some in-depth examples beyond the embedded coding excercises, take a look at this list of suggested problems:\n",
    "- Watch the Ray Summit Talk on [Introduction to Ray AIR](https://github.com/ray-project/hackathon5-algo)\n",
    "- Check out the [Ray AIR Documentation](https://docs.ray.io/en/latest/ray-air/getting-started.html)\n",
    "- Understand its [Components and APIs](https://docs.ray.io/en/latest/ray-air/package-ref.html)\n",
    "- Ray AIR [User Guides](https://docs.ray.io/en/latest/ray-air/user-guides.html) and [Examples](https://docs.ray.io/en/latest/ray-air/examples/index.html)\n",
    "\n",
    "\n",
    "# Next Steps\n",
    "---\n",
    "üéâ Congratulations! You have completed the tutorial on an Introduction to Ray AI Runtime! We dicussed each library in Ray AIR (Data, Train, Tune, Serve, RLLib) and saw some example machine learning workloads to be done with each. In the next module, we will introduce the ecosystem of integrated libraries runs on Ray Core's distributed execution engine, and with Ray Clusters, you can deploy your workloads on AWS, GCP, Azure, or on Kubernetes.\n",
    "\n",
    "From here, you can learn and get more involved with our active community of developers and researchers by checking out the following resources:\n",
    "- üíª [Official Ray Website](https://www.ray.io/): Browse the ecosystem and use this site as a hub to get the information that you need to get going and building with Ray.\n",
    "- üí¨ [Join the Community on Slack](https://forms.gle/9TSdDYUgxYs8SA9e8): Find friends to discuss your new learnings in our Slack space.\n",
    "- üì£ [Use the Discussion Board](https://discuss.ray.io/): Ask questions, follow topics, and view announcements on this community forum.\n",
    "- üôã‚Äç‚ôÄÔ∏è [Join a Meetup Group](https://www.meetup.com/Bay-Area-Ray-Meetup/): Tune in on meet-ups to listen to compelling talks, get to know other users, and meet the team behind Ray.\n",
    "- ü™≤ [Open an Issue](https://github.com/ray-project/ray/issues/new/choose): Ray is constantly evolving to improve developer experience. Submit feature requests, bug-reports, and get help via GitHub issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "567405a8058597909526349386224fe35dd047505a91307e44ed44be00113429"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
