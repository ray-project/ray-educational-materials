{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Ray AI Runtime (AIR)\n",
    "---\n",
    "*Suggested Time to Complete: 30 minutes*\n",
    "\n",
    "![Ray AIR](intro-to-ray-images/Ray_AIR.png)\n",
    "*Figure 5*\n",
    "\n",
    "Ray AI Runtime (AIR) is a unified set of libraries for distributed data processing, model training, tuning, reinforcement learning, and model serving, all in Python. Before we lay out each library and their unique jobs to be done, let's take a moment to motivate Ray AIR by taking a high-level view of the data science and machine learning workflow.\n",
    "\n",
    "Developing a machine learning system is an iterative and often cyclical process that roughly touches on the following stages:\n",
    "1. Business Needs: work with stakeholders to identify business requirements and align on which metric to optimize\n",
    "2. Data Collection: source and collect data and labels\n",
    "3. Feature Engineering: turn raw data into usable material\n",
    "4. Model Training: the learning part of machine learning\n",
    "5. Model Evaluation: try to improve upon your baseline model with hyperparameter tuning or more feature engineering or even a more relevant set of data\n",
    "6. Deployment: deploy your solution to production and/or serve your model to the end user\n",
    "\n",
    "Each of the five main native libraries that Ray AIR wraps tackles a specific piece of the ML specific tasks outlined above and because this abstraction layer is built on top of Ray Core, it is distributed by nature.\n",
    "\n",
    "1. Ray Data: load and manipulate data\n",
    "2. Ray Train: scales model training for popular ML frameworks\n",
    "3. Ray Tune: scales experiment execution and hyperparameter tuning\n",
    "4. Ray Serve: scales model serving\n",
    "5. Ray RLlib: for distributed reinforment learning workloads\n",
    "\n",
    "Let's examine each library in further detail and run through an illustrative example for each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Data\n",
    "***\n",
    "Ray Datasets are the standard way to load and exchange data in Ray libraries and applications. They provide basic distributed data transformations such as map, filter, and repartition, and are compatible with a variety of file formats, data sources, and distributed frameworks.\n",
    "\n",
    "**Datasets**: Ray Datasets are the standard way to load and exchange data in Ray AIR. In AIR, Datasets are used extensively for data loading, preprocessing, and batch inference.\n",
    "\n",
    "**Preprocessors**: Preprocessors are primitives that can be used to transform input data into features. Preprocessors operate on Datasets, which makes them scalable and compatible with a variety of datasources and dataframe libraries.\n",
    "\n",
    "A Preprocessor is fitted during Training, and applied at runtime in both Training and Serving on data batches in the same way. AIR comess with a collection of built-in prepreocessors, and you can also define your own with simple templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataset of Python objects.\n",
    "ds = ray.data.range(10000)\n",
    "# -> Dataset(num_blocks=200, num_rows=10000, schema=<class 'int'>)\n",
    "\n",
    "ds.take(5)\n",
    "# -> [0, 1, 2, 3, 4]\n",
    "\n",
    "ds.count()\n",
    "# -> 10000\n",
    "\n",
    "# Create a Dataset of Arrow records.\n",
    "ds = ray.data.from_items([{\"col1\": i, \"col2\": str(i)} for i in range(10000)])\n",
    "# -> Dataset(num_blocks=200, num_rows=10000, schema={col1: int64, col2: string})\n",
    "\n",
    "ds.show(5)\n",
    "# -> {'col1': 0, 'col2': '0'}\n",
    "# -> {'col1': 1, 'col2': '1'}\n",
    "# -> {'col1': 2, 'col2': '2'}\n",
    "# -> {'col1': 3, 'col2': '3'}\n",
    "# -> {'col1': 4, 'col2': '4'}\n",
    "\n",
    "ds.schema()\n",
    "# -> col1: int64\n",
    "# -> col2: string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "#### Key Concepts\n",
    "- Datasets\n",
    "- Preprocessors\n",
    "\n",
    "#### Key API Elements in This Section\n",
    "#### Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Train\n",
    "Ray Train is a lightweight library for distributed deep learning that allows you to easily supercharge your distributed PyTorch and TensorFlow training on Ray.\n",
    "\n",
    "**Trainer**: Trainers are wrapper classes around third-party training frameworks such as XGBoost and Pytoch. They are built to help integrate with core Ray actors (for distribution), Ray Tune, and Ray Datasets.\n",
    "\n",
    "**Checkpoints**: The AIR trainers, tuners, and custom pretrained model generate a framework-specific Checkpoint object. Checkpoints are a common interface for models that are used across different AIR components and libraries.\n",
    "\n",
    "**Batch Predictor**: You can take a checkpoint and do batch inference using the BatchPredictor object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.train as train\n",
    "from ray.train import Trainer\n",
    "import torch\n",
    "\n",
    "def train_func():\n",
    "    # Setup model.\n",
    "    model = torch.nn.Linear(1, 1)\n",
    "    model = train.torch.prepare_model(model)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "    # Setup data.\n",
    "    input = torch.randn(1000, 1)\n",
    "    labels = input * 2\n",
    "    dataset = torch.utils.data.TensorDataset(input, labels)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=32)\n",
    "    dataloader = train.torch.prepare_data_loader(dataloader)\n",
    "\n",
    "    # Train.\n",
    "    for _ in range(5):\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return model.state_dict()\n",
    "\n",
    "trainer = Trainer(backend=\"torch\", num_workers=4)\n",
    "trainer.start()\n",
    "results = trainer.run(train_func)\n",
    "trainer.shutdown()\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "#### Key Concepts\n",
    "#### Key API Elements in This Section\n",
    "#### Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Tune\n",
    "Ray Tune is a Python library for fast hyperparameter tuning at scale. Easily distribute your trial runs to quickly find the best hyperparameters.\n",
    "\n",
    "**Tuner**: Tuners offer scalable hyperparameter tuning as part of Ray Tune. Tuners can work seamlessly with any Trainer but also can support arbitrary training functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    " \n",
    "def objective(step, alpha, beta):\n",
    "   return (0.1 + alpha * step / 100)**(-1) + beta * 0.1\n",
    " \n",
    "def training_function(config):\n",
    "   # Hyperparameters\n",
    "   alpha, beta = config[\"alpha\"], config[\"beta\"]\n",
    "   for step in range(10):\n",
    "       # Iterative training function - can be any arbitrary training procedure.\n",
    "       intermediate_score = objective(step, alpha, beta)\n",
    "       # Feed the score back back to Tune.\n",
    "       tune.report(mean_loss=intermediate_score)\n",
    " \n",
    "analysis = tune.run(\n",
    "   training_function,\n",
    "   config={\n",
    "       \"alpha\": tune.grid_search([0.001, 0.01, 0.1]),\n",
    "       \"beta\": tune.choice([1, 2, 3])\n",
    "   })\n",
    " \n",
    "print(\"Best config: \", analysis.get_best_config(\n",
    "   metric=\"mean_loss\", mode=\"min\"))\n",
    " \n",
    "# Get a dataframe for analyzing trial results.\n",
    "df = analysis.results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "#### Key Concepts\n",
    "#### Key API Elements in This Section\n",
    "#### Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Serve\n",
    "Ray Serve lets you serve machine learning models in real-time or batch using a simple Python API. Serve individual models or create composite model pipelines, where you can independently deploy, update, and scale individual components.\n",
    "\n",
    "**Deployments**: Deploy the model as an inference service by using Ray Serve and the `PredictorDeployment` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from ray import serve\n",
    "\n",
    "@serve.deployment(route_prefix=\"/iris\")\n",
    "class BoostingModel:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.label_list = iris_dataset[\"target_names\"].tolist()\n",
    "\n",
    "    async def __call__(self, request):\n",
    "        payload = await request.json()\n",
    "        print(f\"Received flask request with data {payload}\")\n",
    "\n",
    "        prediction = self.model.predict([payload[\"vector\"]])[0]\n",
    "        human_name = self.label_list[prediction]\n",
    "        return {\"result\": human_name}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Train model.\n",
    "    iris_dataset = load_iris()\n",
    "    model = GradientBoostingClassifier()\n",
    "    model.fit(iris_dataset[\"data\"], iris_dataset[\"target\"])\n",
    "\n",
    "    # Deploy model\n",
    "    serve.run(BoostingModel.bind(model))\n",
    "\n",
    "    # Query model\n",
    "    sample_request_input = {\"vector\": [1.2, 1.0, 1.1, 0.9]}\n",
    "    response = requests.get(\"http://localhost:8000/iris\", json=sample_request_input)\n",
    "    print(response.text)\n",
    "    \n",
    "    # prints\n",
    "    # Result:\n",
    "    # {\"result\": \"versicolor\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "#### Key Concepts\n",
    "#### Key API Elements in This Section\n",
    "#### Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray RLLib\n",
    "RLlib is the industry-standard reinforcement larning Python frameowrk built on Ray. Designed for quick interation and a fast path to production, it includes 25+ latest algorithms that are all implemented to run at scale and in multi-agent mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    " \n",
    "tune.run(PPOTrainer, config={\n",
    "   \"env\": \"CartPole-v0\",\n",
    "   \"framework\": \"torch\",\n",
    "   \"log_level\": \"INFO\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "#### Key Concepts\n",
    "#### Key API Elements in This Section\n",
    "#### Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Three: Ray Ecosystem\n",
    "---\n",
    "Up until now, we've covered Ray Core, the low-level, distributed computing framework, as well as Ray AIR, the set of high-level native libraries which include Ray Data, Train, Tune, Serve, and RLlib. In addition, Ray integrates with a growing ecosystem of the most popular Python and machine learning libraries and frameworks that you may already be familiar with.\n",
    "\n",
    "Instead of trying to create new standards, Ray allows you to scale existing workloads by unifying tools in a common interface. This interface enables you to run tasks in a distributed way, a property most of the respective backends don't have, or not to the same extent.\n",
    "\n",
    "For example, Ray Datasets is backed by Arrow and comes with many integrations to other frameworks, such as Spark and Dask. Ray Train and RLlib are backed by the full power of Tensorflow and PyTorch. Ray Tune supports algorithms from practically every noteable HPO tool available, including Hyperopt, Optuna, Nevergrad, Ax, SigOpt, and many others. Ray Serve can be used with frameworks such as FastAPI, gradio, and Streamlit. With ample opportunity to extend current projects as well as integrate more backends in the future, a key strength of Ray is this robust design pattern for integration which you can read more about [here](https://www.anyscale.com/blog/ray-distributed-library-patterns).\n",
    "\n",
    "For a complete list of integrations with links, go to the [Ray Ecosystem](https://docs.ray.io/en/latest/ray-overview/ray-libraries.html#) page in the docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: HuggingFace\n",
    "### Example: XGBoost\n",
    "### Example: Distributed Scikit-Learn / Joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "#### Key Concepts\n",
    "#### Key API Elements in This Section\n",
    "#### Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Four: Ray Clusters\n",
    "A Ray cluster consists of nodes that are connected to each other via a network. You can program against the so-called driver, the program root, which lives on the head node. The driver can run jobs, that is a collection of tasks, that are run on the nodes in the cluster. Specifically, the individual tasks of a job are run on worker processes on worker nodes.\n",
    "\n",
    "Ray Cluster\n",
    "\n",
    "A Ray cluster consists of a single head node and any number of connected worker nodes. The number of worker nodes may be autoscaled with application demand as specified by your Ray cluster configuration. The head node runs the autoscaler. Users can submit jobs for execution on the Ray cluster, or can interactively use the cluster by connecting to the head node and running `ray.init`.\n",
    "\n",
    "Head Node\n",
    "\n",
    "Every Ray cluster has one node which is designated as the head node of the cluster. The head node is identical to other worker nodes, except that it also runs singleton processes responsible for cluster management such as the autoscaler and the Ray driver processes which run Ray jobs. Ray may schedule tasks and actor on the head node just like any other worker node, unless configured otherwise.\n",
    "\n",
    "Worker Node\n",
    "\n",
    "Worker nodes do not run any head node management processes, and serve only to run user code in Ray tasks and actors. They participate in distributed scheduling, as well as the storage and distribution of Ray objects in cluster memory.\n",
    "\n",
    "Autoscaling\n",
    "\n",
    "The Ray autoscaler is a process that runs on the head node (or as a sidecar container in the head pod if using Kubernetes). When the resource demands of the Ray workload exceed the current capacity of the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "#### Key Concepts\n",
    "#### Key API Elements in This Section\n",
    "#### Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "---\n",
    "If you would like to practice your new skills further with some in-depth examples beyond the embedded coding excercises, take a look at this list of suggested problems:\n",
    "- [Distribute a Classical Algorithm with Ray](https://github.com/ray-project/hackathon5-algo)\n",
    "    - In this excercise, go to the GitHub repo linked above for details on choosing a classic algorithm implemented in Python, editing the implementation to parallelize the work with Ray, and compare your results against the sequential implementation.\n",
    "\n",
    "\n",
    "# Next Steps\n",
    "---\n",
    "🎉 Congratulations! You have completed your first tutorial on an Introduction to Ray! We discussed the three layers of Ray: Core, AIR, and the Ecosystem. Each library in Ray AIR (Data, Train, Tune, Serve, RLLib) and the ecosystem of integrated libraries runs on Ray Core's distributed execution engine, and with Ray Clusters, you can deploy your workloads on AWS, GCP, Azure, or on Kubernetes.\n",
    "\n",
    "From here, you can learn and get more involved with our active community of developers and researchers by checking out the following resources:\n",
    "- 📖 [Ray's \"Getting Started\" Guides](https://docs.ray.io/en/latest/ray-overview/index.html): A collection of QuickStart Guides for every library including installation walkthrough, examples, blogs, talks, and more!\n",
    "- 💻 [Official Ray Website](https://www.ray.io/): Browse the ecosystem and use this site as a hub to get the information that you need to get going and building with Ray.\n",
    "- 💬 [Join the Community on Slack](https://forms.gle/9TSdDYUgxYs8SA9e8): Find friends to discuss your new learnings in our Slack space.\n",
    "- 📣 [Use the Discussion Board](https://discuss.ray.io/): Ask questions, follow topics, and view announcements on this community forum.\n",
    "- 🙋‍♀️ [Join a Meetup Group](https://www.meetup.com/Bay-Area-Ray-Meetup/): Tune in on meet-ups to listen to compelling talks, get to know other users, and meet the team behind Ray.\n",
    "- 🪲 [Open an Issue](https://github.com/ray-project/ray/issues/new/choose): Ray is constantly evolving to improve developer experience. Submit feature requests, bug-reports, and get help via GitHub issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "567405a8058597909526349386224fe35dd047505a91307e44ed44be00113429"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
