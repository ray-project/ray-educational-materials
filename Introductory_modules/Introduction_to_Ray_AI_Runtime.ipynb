{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Ray AI Runtime (AIR)\n",
    "\n",
    "<img src=\"../_static/assets/Generic/ray_logo.png\" width=\"20%\" loading=\"lazy\">\n",
    "\n",
    "## About this notebook\n",
    "\n",
    "### Is this module right for you?\n",
    "\n",
    "In this module, you will learn how to use Ray AIR to build an end-to-end machine learning application. It covers the entire process from data loading to training and hyperparameter tuning to prediction and serving. Along the way, each section will introduce you to key components of Ray AIR and provide hands-on coding exercises to demonstrate usage.\n",
    "\n",
    "The ideal learner will have the following attributes:\n",
    "\n",
    "* A basic understanding of the Ray project.\n",
    "* An interest in learning about Ray AIR, the unified API for scalable ML applications.\n",
    "* An existing ML application or workload that requires a simple scaling solution.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "For this notebook, you should satisfy the following requirements:\n",
    "\n",
    "* Practical Python and machine learning experience.\n",
    "* Basic familiarity with Ray, equivalent to completing this module:\n",
    "    * [Overview of Ray](https://github.com/ray-project/ray-educational-materials/blob/main/Introductory_modules/Overview_of_Ray.ipynb)\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "* Understand the high-level ML libraries that make up Ray AIR.\n",
    "    * Data\n",
    "    * Train\n",
    "    * Tune\n",
    "    * Serve\n",
    "    * RLlib (not covered in-depth in this module)\n",
    "* Use Ray AIR as a unified toolkit to write an end-to-end ML applications.\n",
    "* Extend mini-examples presented here to scale individual workloads in Python with Ray AIR.\n",
    "* Identify the problems and challenges that Ray AIR attempts to solve.\n",
    "\n",
    "### What will you do?\n",
    "\n",
    "Through hands-on exercises, you will practice the key concepts from each stage of an example ML workflow.\n",
    "\n",
    "|ML workflow stage|Ray AIR key concept|\n",
    "|:--|:--|\n",
    "|Data loading and preprocessing|`Preprocessor` to load and transform data|\n",
    "|Model training|`Trainer` for supported ML frameworks (Keras, Pytorch and more)|\n",
    "|Hyperparameter tuning|`Tuner` for hyperparameter search|\n",
    "|Batch prediction|`BatchPredictor` to load model from best checkpoint for batch inference|\n",
    "|Model serving|`PredictorDeployment` for online inference|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Overview of Ray AI Runtime (AIR)\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong><a href=\"https://docs.ray.io/en/latest/ray-air/getting-started.html\" target=\"_blank\">Ray AI Runtime (AIR)</a></strong> is an open-source, Python-based, domain-specific library that equips ML engineers, data scientists, and researchers with a scalable and unified toolkit for ML applications.\n",
    "</div>\n",
    "\n",
    "Built on top of Ray Core, Ray AIR inherits all the performance and scalability benefits offered by Core while providing a convenient abstraction layer for machine learning. Ray AIR's Python-first native libraries allow ML practitioners to distribute individual workloads, end-to-end applications, and build custom use cases in a unified framework.\n",
    "\n",
    "### Machine learning workflow with Ray AIR\n",
    "\n",
    "Ray AIR wraps five native Ray libraries that scale a specific stage of the ML workflow. In addition, Ray AIR brings together an ever-growing ecosystem of integrations with popular machine learning frameworks to create a common interface for development.\n",
    "\n",
    "|<img src=\"../_static/assets/Introduction_to_Ray_AIR/e2e_air.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Ray AIR enables end-to-end ML development and provides multiple options for integrating with other tools and libraries form the MLOps ecosystem.|\n",
    "\n",
    "1. [**Ray Data**](https://docs.ray.io/en/latest/data/dataset.html)  \n",
    "Scalable, framework-agnostic data loading and transformation across training, tuning, and prediction.\n",
    "\n",
    "2. [**Ray Train**](https://docs.ray.io/en/latest/train/train.html)  \n",
    "Distributed multi-node and multi-core model training with fault tolerance that integrates with popular machine learning training libraries.\n",
    "\n",
    "3. [**Ray Tune**](https://docs.ray.io/en/latest/tune/index.html)  \n",
    "Scales hyperparameter tuning to optimize model performance.\n",
    "\n",
    "4. [**Ray Serve**](https://docs.ray.io/en/latest/serve/index.html)  \n",
    "Deploys a model or ensemble of models for online inference.\n",
    "\n",
    "5. [**Ray RLlib**](https://docs.ray.io/en/latest/rllib/index.html)  \n",
    "Scales reinforcement learning workloads that integrate with other Ray AIR libraries; not covered in-depth in this module"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: End-to-end workflow with Ray AI Runtime\n",
    "\n",
    "To illustrate Ray AIR's capabilities, you will implement an end-to-end machine learning application that predicts big tips using New York City taxi data. Each section will introduce the relevant Ray AIR library or component and demonstrating its functionality through code examples.\n",
    "\n",
    "|Ray AIR Component|NYC Taxi Use Case|\n",
    "|:--|:--|\n",
    "|Ray Data|Use `Preprocessor` to load and transform input data.|\n",
    "|Ray Train|Use `Trainer` to scale XGBoost model training.|\n",
    "|Ray Tune|Use `Tuner` for hyperparameter search.|\n",
    "|Ray AIR Predictor|Use `BatchPredictor` to load model from best checkpoint for batch inference; part of Ray Train.|\n",
    "|Ray Serve|Use `PredictorDeployment` for online inference.|\n",
    "\n",
    "For this classification task, you will apply a simple [XGBoost](https://xgboost.readthedocs.io/en/stable/) (a gradient boosted trees framework) model to the June 2021 [New York City Taxi & Limousine Commission's Trip Record Data](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page). This dataset contains over 2 million samples of yellow cab rides, and the goal is to predict whether a trip will result in a tip greater than 20% or not.\n",
    "\n",
    "**Dataset features**\n",
    "* `passenger_count`\n",
    "    * Float (whole number) representing number of passengers.\n",
    "* `trip_distance` \n",
    "    * Float representing trip distance in miles.\n",
    "* `fare_amount`\n",
    "    * Float representing total price including tax, tip, fees, etc.\n",
    "* `trip_duration`\n",
    "    * Integer representing seconds elapsed.\n",
    "* `hour`\n",
    "    * Hour that the trip started.\n",
    "    * Integer in the range `[0, 23]`\n",
    "* `day_of_week`\n",
    "    * Integer in the range `[1, 7]`.\n",
    "* `is_big_tip`\n",
    "    * Whether the tip amount was greater than 20%.\n",
    "    * Boolean `[True, False]`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Ray Data\n",
    "---\n",
    "\n",
    "First, you need to load in the taxi data and transform the raw input into cleaned features that will be passed to the XGBoost model.\n",
    "\n",
    "|<img src=\"../_static/assets/Introduction_to_Ray_AIR/data_highlight.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Ray AIR wraps Ray Data to provide distributed data ingestion and transformation during training, tuning, and inference.|\n",
    "\n",
    "### Introduction to Ray Datasets\n",
    "\n",
    "Backed by PyArrow, [Ray Datasets](https://docs.ray.io/en/latest/data/user-guide.html) parallelize the loading and transforming of data and provide a standard way to pass references to data across Ray libraries and applications. Datasets are not intended to replace more general data processing systems. Instead, it serves as a last-mile bridge from ETL pipeline outputs to distributed applications and libraries in Ray.\n",
    "\n",
    "#### Key features\n",
    "\n",
    "- **Flexibility**\n",
    "\n",
    "    Datasets are compatible with a variety of file formats, data sources, and distributed frameworks. They work seamlessly with library integrations like Dask on Ray and can be passed between Ray tasks and actors without copying data.\n",
    "\n",
    "- **Performance for ML Workloads**\n",
    "\n",
    "    Datasets offer important features like accelerator support, pipelining, and global random shuffles that accelerate ML training and inference workloads. They also support basic distributed data transformations such as map, filter, sort, groupby, and repartition.\n",
    "\n",
    "- **Persistent Preprocessor**\n",
    "\n",
    "    The `Preprocessor` primitive captures and stores the transformations applied to convert inputs into features. It is applied during training, tuning, batch prediction, and serving to keep the preprocessing consistent across the pipeline.\n",
    "    \n",
    "- **Built on Ray Core**\n",
    "\n",
    "    Datasets inherits scalability to hundreds of nodes, efficient memory usage, object spilling, and failure handling from Ray Core. Because Datasets are just lists of object references, they can be passed between tasks and actors without needing to make a copy of the data, which is crucial for making data-intensive applications and libraries scalable.\n",
    "\n",
    "|<img src=\"../_static/assets/Introduction_to_Ray_AIR/data_code.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|A general pattern for creating a `Dataset`, configuring a `Preprocessor`, and passing these into the `Trainer` for consistent data handling throughout the pipeline.|\n",
    "\n",
    "With this generic structure in mind, you will see how it is applied with the tip prediction task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Ray runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ray.is_initialized:\n",
    "    ray.shutdown()\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start a Ray cluster (see: installation [instructions](https://docs.ray.io/en/latest/ray-overview/installation.html)) so that Ray can utilize all the cores available to you as workers. \n",
    "\n",
    "- check `ray.is_initialized` to ensure that you start with a fresh cluster\n",
    "- use `ray.init()` to initialize a Ray context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Ray Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Parquet file to Ray Dataset.\n",
    "dataset = ray.data.read_parquet(\n",
    "    \"s3://anyscale-training-data/intro-to-ray-air/nyc_taxi_2021.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation subsets.\n",
    "train_dataset, valid_dataset = dataset.train_test_split(test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split datasets into blocks for parallel preprocessing.\n",
    "# `num_blocks` should be lower than number of cores in the cluster.\n",
    "train_dataset = train_dataset.repartition(num_blocks=5)\n",
    "valid_dataset = valid_dataset.repartition(num_blocks=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding exercise**\n",
    "\n",
    "There are many [`Dataset` API elements](https://docs.ray.io/en/latest/data/api/dataset.html#) available for common transformations and operations. Try completing the following tasks:\n",
    "\n",
    "1. Inspect [the schema](https://docs.ray.io/en/latest/data/api/dataset.html#inspecting-metadata) of the underlying Parquet metadata.\n",
    "2. [Count](https://docs.ray.io/en/latest/data/api/dataset.html#inspecting-metadata) the number of rows in the training and validation datasets.\n",
    "3. [Show](https://docs.ray.io/en/latest/data/api/dataset.html#consuming-datasets) the first five samples from either dataset.\n",
    "4. Calculate the average `fare_amount` [grouped by](https://docs.ray.io/en/latest/data/api/dataset.html#grouped-and-global-aggregations) `passenger_count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAMPLE IMPLEMENTATION ###\n",
    "\n",
    "print(f\"Schema of training dataset: \\n {train_dataset.schema()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAMPLE IMPLEMENTATION ###\n",
    "\n",
    "print(f\"Number of samples in training dataset: \\n {train_dataset.count()}\")\n",
    "print(f\"Number of samples in validation dataset: \\n {valid_dataset.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAMPLE IMPLEMENTATION ###\n",
    "\n",
    "train_dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAMPLE IMPLEMENTATION ###\n",
    "\n",
    "train_dataset.groupby(\"passenger_count\").mean(\"fare_amount\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the dataset\n",
    "To transform our raw data into features, you will define a `Preprocessor`. [Ray AIR's `Preprocessor`](https://docs.ray.io/en/latest/ray-air/package-ref.html#preprocessor) captures the data transformation you apply and persists:\n",
    "\n",
    "- **During training**\n",
    "\n",
    "    `Preprocessor` is passed into a `Trainer` to `fit` and `transform` input `Datasets`.\n",
    "- **During tuning**\n",
    "\n",
    "    Each `Trial` will create its own copy of the `Preprocessor`, and the fitting and transformation logic will occur once per `Trial`.\n",
    "- **During checkpointing**\n",
    "\n",
    "    The `Preprocessor` is saved in the `Checkpoint` if it was passed into the `Trainer`.\n",
    "- **During predicting**\n",
    "\n",
    "    If the `Checkpoint` contains a `Preprocessor`, then it will be used to call `transform_batch` on input batches prior to performing inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.preprocessors import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a preprocessor to normalize the columns by their range.\n",
    "preprocessor = MinMaxScaler(columns=[\"trip_distance\", \"trip_duration\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding exercise**\n",
    "\n",
    "Ray AIR provides several [preprocessors out of the box](https://docs.ray.io/en/latest/ray-air/preprocessors.html#) and also supports the implementation of [custom preprocessors](https://docs.ray.io/en/latest/ray-air/preprocessors.html#implementing-custom-preprocessors). Later on, you can compare model performance between the given preprocessor and your custom configuration.\n",
    "\n",
    "Select a [built-in](https://docs.ray.io/en/latest/ray-air/preprocessors.html#types-of-preprocessors) `Preprocessor` and use `fit_transform()` to [apply it](https://docs.ray.io/en/latest/ray-air/package-ref.html#preprocessor) to the dataset. Visualize the results (perhaps using the [integration with pandas](https://docs.ray.io/en/latest/data/api/input_output.html#ray.data.from_pandas) to generate a histogram view)\n",
    "\n",
    "Note: You may want to create a sample dataset to transform, as the original data and preprocessor will be passed to the `Trainer` in the next step for transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAMPLE IMPLEMENTATION ###\n",
    "\n",
    "from ray.data.preprocessors import *\n",
    "\n",
    "sample_data = ray.data.read_parquet(\n",
    "    \"s3://anyscale-training-data/intro-to-ray-air/nyc_taxi_2021.parquet\"\n",
    ")\n",
    "\n",
    "# create new preprocessor\n",
    "sample_preprocessor = PowerTransformer(\n",
    "    columns=[\"trip_distance\", \"trip_duration\"], power=0.5\n",
    ")\n",
    "\n",
    "# apply the transformation\n",
    "transformed_data = sample_preprocessor.fit_transform(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAMPLE IMPLEMENTATION ###\n",
    "\n",
    "# visualize original data\n",
    "sample_df = sample_data.to_pandas(limit=2704905)\n",
    "sample_df.hist(\"trip_distance\")\n",
    "sample_df.hist(\"trip_duration\")\n",
    "\n",
    "# visualize transformed data\n",
    "transformed_df = transformed_data.to_pandas(limit=2704905)\n",
    "transformed_df.hist(\"trip_distance\")\n",
    "transformed_df.hist(\"trip_duration\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the positively-skewed distributions for `trip_distance` and `trip_duration`. For these numerical features, you can choose an appropriate AIR `Preprocessor` depending on your data's properties:\n",
    "\n",
    "- `PowerTransformer`  \n",
    "Your data isn't normal, but you need it to be.\n",
    "- `Normalizer`  \n",
    "You need unit norm rows.\n",
    "- `MinMaxScaler`  \n",
    "You are unsure of the distribution of your data.\n",
    "\n",
    "Feature scaling can often improve performance during training, and testing choice of `Preprocessor` is worth investigating when you have a small number of features that are not already unit normalized."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "#### Key concepts\n",
    "\n",
    "`Dataset`\n",
    "\n",
    "The standard way to load and exchange data in Ray AIR. In AIR, Datasets are used extensively for data loading and transformation. They are meant as a last-mile bridge from ETL pipeline outputs to distributed applications and libraries in Ray.\n",
    "\n",
    "`Preprocessor`\n",
    "\n",
    "Preprocessors are primitives that transform input data into features. They operate on Datasets, making them scalable and compatible with a variety of datasources and dataframe libraries.\n",
    "\n",
    "Preprocessors persist through various stages of the pipeline:\n",
    "\n",
    "- During training to fit and transform input data\n",
    "- In each trial of hyperparameter tuning\n",
    "- Within a checkpoint\n",
    "- On input batches for inference\n",
    "\n",
    "AIR comes with a collection of built-in preprocessors, and you can also define your own with simple templates (see the [user guide](https://docs.ray.io/en/latest/ray-air/preprocessors.html) for more information)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Ray Train\n",
    "***\n",
    "\n",
    "Taking the dataset and preprocessor from the previous step, you can set up a simple XGBoost model to classify taxi rides based on predicted tip amounts.\n",
    "\n",
    "|<img src=\"../_static/assets/Introduction_to_Ray_AIR/train_highlight.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Ray AIR wraps Ray Train to provide distributed model training.|\n",
    "\n",
    "### Introduction to Ray Train\n",
    "\n",
    "#### Common use cases\n",
    "\n",
    "ML pracitioners tend to run into a few common problems with training models that prompt them to consider distributed solutions:\n",
    "\n",
    "1. training time is [too long](https://www.anyscale.com/blog/how-anastasia-implements-ray-and-anyscale-to-speed-up-ml-processes-9x) to be practical\n",
    "2. the [data is too large](https://www.anyscale.com/blog/how-ray-and-anyscale-make-it-easy-to-do-massive-scale-machine-learning-on) to fit on one machine\n",
    "3. [training many models](https://www.anyscale.com/blog/training-one-million-machine-learning-models-in-record-time-with-ray) sequentially doesn't utilize resources efficiently\n",
    "4. the [model itself is too large](https://www.uber.com/blog/horovod-ray/) to fit on a single machine\n",
    "\n",
    "[Ray Train](https://docs.ray.io/en/latest/ray-air/trainer.html) addresses these issues by improving performance through distributed multi-node training and is able to leverage Ray Data to distribute preprocessing and data ingestion.\n",
    "\n",
    "#### Integration with Ray ecosystem\n",
    "\n",
    "`Trainers` can plug into:\n",
    "\n",
    "- Ray Data: to enable scalable data loading and preprocessing with Ray `Datasets` and `Preprocessors`\n",
    "- Ray Tune: to compose with `Tuners` for distributed hyperparameter tuning\n",
    "- Ray AIR Predictor: as a checkpointed trained model to be applied during inference\n",
    "- Popular ML training frameworks like:\n",
    "    - [PyTorch](https://docs.ray.io/en/latest/ray-air/package-ref.html#pytorch)\n",
    "    - [Tensorflow](https://docs.ray.io/en/latest/ray-air/package-ref.html#tensorflow)\n",
    "    - [Horovod](https://docs.ray.io/en/latest/ray-air/package-ref.html#horovod)\n",
    "    - [XGBoost](https://docs.ray.io/en/latest/ray-air/package-ref.html#xgboost)\n",
    "    - [HuggingFace Transformers](https://docs.ray.io/en/latest/ray-air/package-ref.html#huggingface)\n",
    "    - [Scikit-Learn](https://docs.ray.io/en/latest/ray-air/package-ref.html#scikit-learn)\n",
    "    - and more\n",
    "\n",
    "#### Useful features\n",
    "\n",
    "* callbacks for early stopping\n",
    "* checkpointing\n",
    "* integration with Tensorboard, Weights & Biases, and MLflow for observability\n",
    "* export mechanisms for models\n",
    "\n",
    "|<img src=\"../_static/assets/Introduction_to_Ray_AIR/train_code.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Define the `Trainer` object and then fit it to the training dataset. This snippet uses a `TorchTrainer`, however, this may be swapped out with any [integration](https://docs.ray.io/en/latest/ray-air/package-ref.html#trainer-and-predictor-integrations) or custom-defined `Trainer`.|\n",
    "\n",
    "In the next section, define and fit an XGBoost Trainer to fit the NYC taxi data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define AIR `Trainer`\n",
    "\n",
    "Ray AIR provides a variety of built-in [`Trainers`](https://docs.ray.io/en/latest/ray-air/trainer.html) (PyTorch, Tensorflow, HuggingFace, etc.). In the example below, you will use a Ray `XGBoostTrainer` which [offers support](https://docs.ray.io/en/latest/train/gbdt.html) for XGBoost models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.air.config import ScalingConfig\n",
    "from ray.train.xgboost import XGBoostTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = XGBoostTrainer(\n",
    "    label_column=\"is_big_tip\",\n",
    "    num_boost_round=50,\n",
    "    scaling_config=ScalingConfig(\n",
    "        num_workers=5,\n",
    "        use_gpu=False,\n",
    "    ),\n",
    "    params={\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": [\"logloss\", \"error\"],\n",
    "        \"tree_method\": \"approx\",\n",
    "    },\n",
    "    datasets={\"train\": train_dataset, \"valid\": valid_dataset},\n",
    "    preprocessor=preprocessor,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct a `Trainer`, you provide:\n",
    "\n",
    "- a `ScalingConfig` which [specifies](https://docs.ray.io/en/releases-2.0.0rc0/ray-air/config-scaling.html) how many parallel training workers and what type of resources (CPUs/GPUs) to use per worker during training; supports seamless scaling across heterogeneous hardware.\n",
    "- a dictionary of training and validation sets\n",
    "- the `Preprocessor` used to transform the `Datasets`\n",
    "\n",
    "Optionally, you can choose to add `resume_from_checkpoint` which allows you to continue training from a [saved checkpoint](https://docs.ray.io/en/latest/ray-air/package-ref.html#ray.air.checkpoint.Checkpoint) should the run be interrupted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To invoke training, call `.fit()`. Trainer objects produce a `Result` object which gives you access to metrics, checkpoints, and errors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding exercise**\n",
    "\n",
    "You can check out the training results from the `Result` object with the following calls:\n",
    "\n",
    "```python\n",
    "# returns last saved checkpoint\n",
    "result.checkpoint\n",
    "\n",
    "# returns the `n` best saved checkpoints as configured in `RunConfig.CheckpointConfig`\n",
    "result.best_checkpoints\n",
    "\n",
    "# returns the final metrics as reported\n",
    "result.metrics\n",
    "\n",
    "# returns an Exception if training failed\n",
    "result.error\n",
    "```\n",
    "\n",
    "Inspect your training result below. What is the reported accuracy for the training and validation runs?\n",
    "\n",
    "Note: `result.error` contains the binary classification error rate in this case calculated as `#(wrong cases)/#(all cases)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAMPLE IMPLEMENTATION ###\n",
    "\n",
    "print(f\"Result metrics: \\n {result.metrics} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAMPLE IMPLEMENTATION ###\n",
    "\n",
    "print(f\"Training accuracy: {1 - result.metrics['train-error']:.4f}\")\n",
    "print(f\"Validation accuracy: {1 - result.metrics['valid-error']:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "#### Key concepts\n",
    "\n",
    "`Trainer`\n",
    "\n",
    "Trainers are wrapper classes around third-party training frameworks such as XGBoost, Pytorch, and Tensorflow. They are built to help integrate with Ray Actors (for distribution), Ray Datasets, and Ray Tune."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Ray Tune\n",
    "***\n",
    "\n",
    "Now that you have a baseline XGBoost model trained, you can try to improve performance by running hyperparameter tuning experiments.\n",
    "\n",
    "|<img src=\"../_static/assets/Introduction_to_Ray_AIR/tune_highlight.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Ray AIR wraps Ray Tune to provide distributed hyperparameter optimization.|\n",
    "\n",
    "### Introduction to Ray Tune\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong><a href=\"https://en.wikipedia.org/wiki/Hyperparameter_optimization\" target=\"_blank\">Hyperparameter tuning, or optimization, (HPO)</a></strong> is the process of choosing optimal hyperparameters for a machine learning model. Hyperparameters, in contrast to weights learned by the model, are parameters that you set to influence training.\n",
    "</div>\n",
    "\n",
    "\n",
    "Setting up and executing hyperparameter optimization (HPO) can be expensive in terms of compute resources and runtime with several complexities including:\n",
    "\n",
    "- **Vast Search Space**\n",
    "\n",
    "    Your model could have several hyperparameters, each with different data types, ranges, and possible correlations.\n",
    "    Sampling good candidates from high-dimensional spaces is difficult.\n",
    "- **Search Algorithms**\n",
    "\n",
    "    Choosing hyperparameters strategically requires testing complex search algorithms to achieve good results.\n",
    "- **Long Runtime**\n",
    "\n",
    "    Even if you distribute tuning, training complex models in themselves can take a long time to complete per run, so it's best to have an efficiency at every stage in the pipeline.\n",
    "- **Resource Allocation**\n",
    "\n",
    "    You must have enough compute resources available to during each trial as to not slow down search because of scheduling mismatches.\n",
    "- **User Experience**\n",
    "\n",
    "    Observability tooling for developers like stopping bad runs early, saving intermediate results, restarting from checkpoints, or pausing/resuming runs makes HPO easier.\n",
    "\n",
    "Ray Tune is a distributed HPO library that addresses all of these topics above to provide a simplified interface for running trials and integrates with popular frameworks such as HyperOpt and Optuna.\n",
    "\n",
    "|<img src=\"../_static/assets/Introduction_to_Ray_AIR/tune_code.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|General pattern for using AIR `Tuners` which involves taking in a `Trainer`, defining a search space, establishing a search algorithm, scheduling trials, and analyzing results.|\n",
    "\n",
    "See how to interact with Ray Tune to make some improvements to our big tip classifier in the next section."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use AIR `Tuner` for hyperparameter search\n",
    "\n",
    "As an aside, Ray Tune will advance a default checkpointing system for hyperparameter tuning. For particularly large models, it may be advisable to set up a `CheckpointConfig` which defines a [checkpointing strategy](https://docs.ray.io/en/latest/ray-air/package-ref.html#ray.air.config.CheckpointConfig). In particular, you can toggle `num_to_keep` to avoid saving any fruitless trials to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from ray.tune.tuner import Tuner, TuneConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_space = {\n",
    "    \"params\": {\n",
    "        \"eta\": tune.uniform(0.2, 0.4),\n",
    "        \"max_depth\": tune.randint(1, 6),\n",
    "        \"min_child_weight\": tune.uniform(0.8, 1.0),\n",
    "    }\n",
    "}\n",
    "\n",
    "tuner = Tuner(\n",
    "    trainer,\n",
    "    param_space=param_space,\n",
    "    tune_config=TuneConfig(num_samples=3, metric=\"train-logloss\", mode=\"min\"),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First define a search space with a few hyperparameters to tune:\n",
    "\n",
    "- `eta` is the learning rate\n",
    "- `max_depth` specifies how deep each tree is (default=6). A higher value leads to a more complex model.\n",
    "- `min_child_weight` defines the minimum sum of weights of all observations in a child; used to control overfitting\n",
    "\n",
    "To set up an AIR `Tuner`, you must specify:\n",
    "\n",
    "- `Trainer`: the training loop from before; support for heterogeneous hardware built-in with each Trainer's `ScalingConfig`\n",
    "- `search space`: a set of hyperparameters you wish to tune\n",
    "- `search_algorithm`: to optimize parameter search\n",
    "- `scheduler`: (optional) to stop searches early and speed up experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute hyperparameter search and analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_grid = tuner.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can execute tuning on `num_samples=10` trials. After tuning, you can query the `ResultGrid` object to see metrics, results, and checkpoints of each trial."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding exercise**\n",
    "\n",
    "You can probe the `ResultGrid` [for metrics](https://docs.ray.io/en/latest/tune/api_docs/result_grid.html) using these calls:\n",
    "\n",
    "```python\n",
    "\n",
    "# checks if there have been errors\n",
    "result_grid.errors\n",
    "\n",
    "# gets the best result\n",
    "best_result = result_grid.get_best_result()\n",
    "\n",
    "# gets the best checkpoint\n",
    "best_checkpoint = best_result.checkpoint\n",
    "\n",
    "# gets the best metrics\n",
    "best_metrics = best_result.metrics\n",
    "\n",
    "```\n",
    "\n",
    "Inspect your tuning results. What is the best result from these experiments? Are they better than the baseline model in the training step in the previous section?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAMPLE IMPLEMENTATION ###\n",
    "\n",
    "best_result = result_grid.get_best_result()\n",
    "print(f\"Best result: \\n {best_result} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAMPLE IMPLEMENTATION ###\n",
    "\n",
    "print(f\"Training accuracy: {1 - best_result.metrics['train-error']:.4f}\")\n",
    "print(f\"Validation accuracy: {1 - best_result.metrics['valid-error']:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "#### Key concepts\n",
    "\n",
    "`Tuner`\n",
    "\n",
    "Provides an interface that works with AIR `Trainers` to perform distributed hyperparameter tuning. You define a set of hyperparameters you wish to tune in a search space, specify a search algorithm, and the `Tuner` returns its results in a `ResultGrid` that contains metrics, results, and checkpoints for each `trial`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Ray AIR Predictors\n",
    "***\n",
    "\n",
    "[Ray AIR Predictors](https://docs.ray.io/en/latest/ray-air/predictors.html) load models from [checkpoints](https://docs.ray.io/en/latest/ray-air/key-concepts.html#air-checkpoints-doc) generated during training or tuning to perform distributed inference.\n",
    "\n",
    "`BatchPredictor` is a utility for large scale [batch inference](https://docs.ray.io/en/latest/ray-air/predictors.html#batch-prediction) and takes in:\n",
    "1. `Checkpoint` - a saved model from training or tuning\n",
    "    - `Preprocessor` (optional) - defined earlier to transform input data for training can be reapplied to incoming batches\n",
    "2. `Predictor` - loads model from checkpoint to perform inference\n",
    "\n",
    "|<img src=\"../_static/assets/Introduction_to_Ray_AIR/batchpredict_code.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|`Checkpoint` and `Predictor` is passed into each instance of `BatchPredictor`|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use AIR `BatchPredictor` for Batch Prediction\n",
    "\n",
    "Previously, you have trained and tuned the XGBoost model on data from June 2021. You will now take the best checkpoint from tuning and perform offline, or batch, inference on taxi tip data from June 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.batch_predictor import BatchPredictor\n",
    "from ray.train.xgboost import XGBoostPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ray.data.read_parquet(\n",
    "    \"s3://anyscale-training-data/intro-to-ray-air/nyc_taxi_2022.parquet\"\n",
    ").drop_columns(\"is_big_tip\")\n",
    "\n",
    "test_dataset = test_dataset.repartition(num_blocks=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create `BatchPredictor` from best trial of HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result = result_grid.get_best_result()\n",
    "\n",
    "batch_predictor = BatchPredictor.from_checkpoint(\n",
    "    checkpoint=best_result.checkpoint, predictor_cls=XGBoostPredictor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `BatchPredictor` by passing in the best result from the hyperparameter tuning step along with an XGBoost specific `Predictor` to handle the distributed inference step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probabilities = batch_predictor.predict(test_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run batch inference on the testing dataset by calling `predict()`. Prediction scales seamlessly across heterogeneous hardware if specified during instantiation of the `Trainer` object via the `ScalingConfig`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding exercise**\n",
    "\n",
    "Now that you have the predictions generated from the testing set, how did the model perform? \n",
    "\n",
    "Inspect the predictions outputted by `BatchPredictor` by using `predicted_probabilities.show()`.\n",
    "\n",
    "For more practice working with Ray `Datasets`, see if you can compare the results from `predicted_probabilities` with the ground truth labels made available in the raw June 2022 Parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAMPLE IMPLEMENTATION ###\n",
    "predicted_probabilities.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "#### Key concepts\n",
    "\n",
    "`Checkpoints`\n",
    "\n",
    "Store the full state of the model periodically, so that partially trained models are available and can be used to resume training from an intermediate point, instead of starting from scratch; also allows for the best model to be saved for batch inference later on.\n",
    "\n",
    "`BatchPredictor`\n",
    "\n",
    "Loads the best model from a checkpoint to perform batch inference on large-scales or online inference."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Ray Serve\n",
    "***\n",
    "\n",
    "Finally, you may want to serve this taxi tip prediction application to end users, hopefully with a low latency to be maximally useful to drivers on the job. This poses a challenge since machine learning models are compute intensive, and ideally, this model wouldn't be served in isolation, but rather adjacent to business logic or even other ML models.\n",
    "\n",
    "|<img src=\"../_static/assets/Introduction_to_Ray_AIR/serve_highlight.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Ray AIR wraps Ray Serve to provide distributed model serving.|\n",
    "\n",
    "### Introduction to Ray Serve\n",
    "\n",
    "Ray Serve is a scalable compute layer for serving machine learning models that enables serving individual models or creating composite model pipelines where you can independently deploy, update, and scale individual components. \n",
    "\n",
    "Serve isn't tied to a specific machine learning library, but rather treats models as ordinary Python code. \n",
    "\n",
    "Additionally, it allows you to flexibly combine normal Python business logic alongside machine learning models. This makes it possible to build online inference services completely end-to-end: \n",
    "\n",
    "- validate user input\n",
    "- query a database\n",
    "- perform inference scalably across multiple ML models\n",
    "- combine, filter, and validate the output all in the process of handling a single inference request\n",
    "\n",
    "|<img src=\"../_static/assets/Introduction_to_Ray_AIR/serve_code.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Pattern for deploying a `Predictor` from a `Checkpoint` wth Ray Serve.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `PredictorDeployment` for Online Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the XGBoost model from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import serve\n",
    "from ray.serve import PredictorDeployment\n",
    "from ray.serve.http_adapters import pandas_read_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.run(\n",
    "    PredictorDeployment.options(\n",
    "        name=\"XGBoostService\", num_replicas=2, route_prefix=\"/rayair\"\n",
    "    ).bind(XGBoostPredictor, best_result.checkpoint, http_adapter=pandas_read_json)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy the best model as an inference service by using Ray Serve and the `PredictorDeployment` class. After deploying the service, you can send requests to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send some test traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = test_dataset.take(1)\n",
    "sample_input = dict(sample_input[0])\n",
    "\n",
    "output = requests.post(\"http://localhost:8000/rayair\", json=[sample_input]).json()\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send a request through HTTP by using the `PredictorDeployment` to deploy checkpoints trained in Ray AIR as live endpoints."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding exercise**\n",
    "\n",
    "You've just served a prediction for a single sample input from an item in the test dataset.\n",
    "\n",
    "Try tweaking a custom input in the cell below to send a personalized request. Modify the code below `### YOUR CODE HERE ###` to send your request and print the output.\n",
    "\n",
    "Note: There is much more to deployments that falls outside of the scope of this introductory module. For more, check out the [user guide](https://docs.ray.io/en/latest/ray-air/examples/serving_guide.html) for Ray Serve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play with these inputs to receive predictions\n",
    "passenger_count = 1\n",
    "trip_distance = 10  # in miles\n",
    "fare_amount = 45\n",
    "trip_duration = 1200  # in seconds\n",
    "hour = 18\n",
    "day_of_week = 6\n",
    "\n",
    "sample_input = {\n",
    "    \"passenger_count\": passenger_count,\n",
    "    \"trip_distance\": trip_distance,\n",
    "    \"fare_amount\": fare_amount,\n",
    "    \"trip_duration\": trip_duration,\n",
    "    \"hour\": hour,\n",
    "    \"day_of_week\": day_of_week,\n",
    "}\n",
    "\n",
    "### YOUR CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAMPLE IMPLEMENTATION ###\n",
    "\n",
    "# play with these inputs to receive predictions\n",
    "passenger_count = 1\n",
    "trip_distance = 10  # in miles\n",
    "fare_amount = 45\n",
    "trip_duration = 1200  # in seconds\n",
    "hour = 18\n",
    "day_of_week = 6\n",
    "\n",
    "sample_input = {\n",
    "    \"passenger_count\": passenger_count,\n",
    "    \"trip_distance\": trip_distance,\n",
    "    \"fare_amount\": fare_amount,\n",
    "    \"trip_duration\": trip_duration,\n",
    "    \"hour\": hour,\n",
    "    \"day_of_week\": day_of_week,\n",
    "}\n",
    "\n",
    "output = requests.post(\"http://localhost:8000/rayair\", json=[sample_input]).json()\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "#### Key concepts\n",
    "\n",
    "`Deployments`\n",
    "\n",
    "A managed group of Ray actors that can be addressed together and will handle requests load-balanced across them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shutdown Ray runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disconnect the worker, and terminate processes started by `ray.init()`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Summary\n",
    "***\n",
    "You've now just created a Ray Dataset, preprocessed some features, built a model with XGBoost, searched a hyperparameter space for the best configuration, loaded the best model from a checkpoint to perform batch inference, and served that model for online inference. \n",
    "\n",
    "Through this end-to-end example, you explored how to use Ray AIR to distribute an entire ML pipeline.\n",
    "\n",
    "### Key concepts\n",
    "\n",
    "- `Datasets`\n",
    "\n",
    "    The standard way to load and exchange data in Ray AIR. In AIR, Datasets are used extensively for data loading and transformation. Meant as the last-mile bridge from ETL pipeline outputs to distributed applications and libraries in Ray.\n",
    "\n",
    "- `Preprocessors`\n",
    "\n",
    "    Preprocessors are primitives that can be used to transform input data into features. Preprocessors operate on Datasets, which makes them scalable and compatible with a variety of datasources and dataframe libraries.\n",
    "\n",
    "- `Trainers`\n",
    "\n",
    "    Trainers are wrapper classes around third-party training frameworks such as XGBoost, Pytorch, and Tensorflow. They are built to help integrate with Ray Actors (for distribution), Ray Datasets, and Ray Tune.\n",
    "\n",
    "- `Tuner`\n",
    "\n",
    "    Provides an interface that works with AIR Trainers to perform distributed hyperparameter tuning. You define a set of hyperparameters you wish to tune in a search space, specify a search algorithm, and the Tuner returns its results in a ResultGrid that contains metrics, results, and checkpoints for each trial.\n",
    "\n",
    "- `Checkpoints`\n",
    "\n",
    "    Store the full state of the model periodically, so that partially trained models are available and can be used to resume training from an intermediate point, instead of starting from scratch; also allows for the best model to be saved for batch inference later on.\n",
    "\n",
    "- `BatchPredictor`\n",
    "\n",
    "    Loads the best model from a checkpoint to perform batch inference on large-scales or online inference.\n",
    "\n",
    "- `Deployments`\n",
    "\n",
    "    A managed group of Ray actors that can be addressed together and will handle requests load-balanced across them.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect with the Ray community\n",
    "\n",
    "You can learn and get more involved with the Ray community of developers and researchers:\n",
    "\n",
    "* [Ray documentation](https://docs.ray.io/en/latest)\n",
    "* [Official Ray Website](https://www.ray.io/): Browse the ecosystem and use this site as a hub to get the information that you need to get going and building with Ray.\n",
    "* [Join the Community on Slack](https://forms.gle/9TSdDYUgxYs8SA9e8): Find friends to discuss your new learnings in our Slack space.\n",
    "* [Use the Discussion Board](https://discuss.ray.io/): Ask questions, follow topics, and view announcements on this community forum.\n",
    "* [Join a Meetup Group](https://www.meetup.com/Bay-Area-Ray-Meetup/): Tune in on meet-ups to listen to compelling talks, get to know other users, and meet the team behind Ray.\n",
    "* [Open an Issue](https://github.com/ray-project/ray/issues/new/choose): Ray is constantly evolving to improve developer experience. Submit feature requests, bug-reports, and get help via GitHub issues.\n",
    "* [Become a Ray contributor](https://docs.ray.io/en/latest/ray-contribute/getting-involved.html): We welcome community contributions to improve our documentation and Ray framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../_static/assets/Generic/ray_logo.png\" width=\"20%\" loading=\"lazy\">"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:22) [Clang 13.0.1 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "567405a8058597909526349386224fe35dd047505a91307e44ed44be00113429"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
