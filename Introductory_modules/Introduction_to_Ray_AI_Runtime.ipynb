{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Ray AI Runtime (AIR)\n",
    "\n",
    "<img src=\"../_static/assets/Generic/ray_logo.png\" width=\"20%\" loading=\"lazy\">\n",
    "\n",
    "## About this notebook\n",
    "\n",
    "### Is it right for you?\n",
    "\n",
    "This notebook is an example-based introduction to the Ray AI Runtime (AIR).\n",
    "\n",
    "You will go through an end-to-end example that covers data loading, training, hyper-parameter tuning, predicting and serving. Along the way you will learn about Ray AIR's specialized libraries that collectively form a unified API for scalable ML applications. It is right for you if:\n",
    "\n",
    "* You have basic familiarity with Ray project.\n",
    "* You want to learn about Ray AIR: the unified API for scalable ML applications.\n",
    "* You have an existing ML application or workload and look for tools that will let you scale it easily.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "For this notebook you should have:\n",
    "\n",
    "* Practical Python and machine learning experience.\n",
    "* Basic familiarity with Ray equivalent to completing this training module:\n",
    "    * [Overview of Ray](https://github.com/ray-project/ray-educational-materials/blob/main/Introductory_modules/Overview_of_Ray.ipynb)\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "Upon completion of this notebook, you will know about:\n",
    "\n",
    "* High-level ML libraries that compose Ray AIR: Data, Train, Tune, and Serve.\n",
    "* How to use Ray AIR as a unified toolkit to write an end-to-end ML application in Python as well as scale individual jobs.\n",
    "* Problems and challenges that Ray AIR attempts to solve.\n",
    "\n",
    "### What will you do?\n",
    "\n",
    "You will run and analyze an end-to-end example that covers all Ray AIR libraries. Through hands-on exercises, you will practice the key concepts from each stage of the example ML workflow:\n",
    "\n",
    "|ML workflow stage|Ray AIR key concept|\n",
    "|:--|:--|\n",
    "|data loading and preprocessing|`Preprocessor` to load and transform data|\n",
    "|model training|`Trainer` for supported ML frameworks (Keras, Pytorch and more)|\n",
    "|hyper-parameter tuning|`Tuner` for hyperparameter search|\n",
    "|batch prediction at scale|`BatchPredictor` to load model from best checkpoint for batch inference|\n",
    "|model serving|`PredictorDeployment` for online inference|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Overview of Ray AI Runtime (AIR)\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong><a href=\"https://docs.ray.io/en/latest/ray-air/getting-started.html\" target=\"_blank\">Ray AI Runtime (AIR)</a></strong> is an open-source, Python, domain specific library that equips ML engineers, data scientists, and researchers with a scalable and unified toolkit for ML applications.\n",
    "</div>\n",
    "\n",
    "Built on top of Ray Core, Ray AIR inherits all the performance and scalability benefits offered by Core while exposing a convenient abstraction layer for machine learning. Ray AIR's Python-first native libraries allow ML practitioners to distribute individual workloads, end-to-end applications, and build custom use cases in a unified framework.\n",
    "\n",
    "### Machine learning workflow with Ray AIR\n",
    "\n",
    "Ray AIR wraps five native Ray libraries that scale a specific stage of the ML workflow. In addition, Ray AIR brings together an ever-growing ecosystem of integrations with other popular existing machine learning frameworks to create a common interface for development.\n",
    "\n",
    "|<img src=\"../_static/assets/Introduction_to_Ray_AIR/e2e_air.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Ray AIR enables end-to-end ML development and provides multiple options to integrate with other tools and libraries form the MLOps ecosystem.|\n",
    "\n",
    "1. [Ray Data](https://docs.ray.io/en/latest/data/dataset.html): scalable, framework-agnostic loading and transforming raw data\n",
    "2. [Ray Train](https://docs.ray.io/en/latest/train/train.html): distributed multi-node and multi-core model training with fault tolerance that integrates with your favorite training libraries\n",
    "3. [Ray Tune](https://docs.ray.io/en/latest/tune/index.html): scales experiment execution and hyper-parameter tuning to optimize model performance\n",
    "4. [Ray Serve](https://docs.ray.io/en/latest/serve/index.html): deploys your model for online or batch inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: End to end workflow with Ray AI Runtime\n",
    "\n",
    "### Overview\n",
    "\n",
    "To illustrate Ray AIR's capabilities, you will implement an end-to-end example - predicting big tips with New York City Taxi data. Each section will introduce the Ray AIR library before demonstrating its functionality with code examples.\n",
    "\n",
    "|Ray AIR Component|Example Use Case|\n",
    "|:--|:--|\n",
    "|Ray Data|use `Preprocessor` to load and transform input data|\n",
    "|Ray Train|use `Trainer` to scale XGBoost model training|\n",
    "|Ray Tune|use `Tuner` for hyperparameter search|\n",
    "|Ray AIR Predictor*|use `BatchPredictor` to load model from best checkpoint for batch inference|\n",
    "|Ray Serve|use `PredictorDeployment` for online inference|\n",
    "\n",
    "#### Data\n",
    "\n",
    "For this example, you will use the June 2021 [New York City Taxi & Limousine Commission's Trip Record Data](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page) which contains over 2 million samples to predict whether a trip may result in a tip over 20%.\n",
    "\n",
    "**Key features**\n",
    "- `passenger_count`\n",
    "- `trip_distance` (in miles)\n",
    "- `fare_amount` (including tax, tip, fees, etc.)\n",
    "- `trip_duration` (in seconds)\n",
    "- `hour` (hour that the trip started)\n",
    "- `day_of_week`\n",
    "- `is_big_tip` (whether the tip amount was greater than 20%)\n",
    "\n",
    "#### Model\n",
    "\n",
    "[XGBoost](https://xgboost.readthedocs.io/en/stable/) is a gradient boosted decision trees library, and you will set up a simple version for this classification task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Ray Data\n",
    "---\n",
    "\n",
    "First up, you will load in the taxi dataset and transform its raw input into features that will be given to our machine learning model.\n",
    "\n",
    "![Data Highlight](../_static/assets/Introduction_to_Ray_AIR/data_highlight.png)\n",
    "\n",
    "### Introduction to Ray Datasets\n",
    "\n",
    "Backed by PyArrow, [Ray Datasets](https://docs.ray.io/en/latest/data/user-guide.html) parallelize loading and transforming data, and provide a standard way to pass references to data across Ray libraries and applications.\n",
    "\n",
    "**Key features**\n",
    "\n",
    "- **Flexibility**\n",
    "\n",
    "    Compatible with a variety of file formats, data sources, and distributed frameworks, Datasets work seamlessly with library integrations like Dask on Ray and can be passed between Ray tasks and actors without copying data.\n",
    "\n",
    "- **Performance for ML Workloads**\n",
    "\n",
    "    Datasets offers important features like accelerator support, pipelining, and global random shuffles that accelerate ML training and inference workloads along with basic distributed data transformations such as map, filter, sort, groupby, and repartition.\n",
    "\n",
    "- **Persistent Preprocessor**\n",
    "\n",
    "    The `Preprocessor` primitive explicitly captures and stores the transformations applied to convert inputs into features and is applied at both training and serving to keep the processing consistent across the pipeline.\n",
    "    \n",
    "- **Built on Ray Core**\n",
    "\n",
    "    Inherits scalability to hundreds of nodes, efficient memory usage due to memory across processes on the same node, and object spilling and recovery to handle failures. Because Datasets are just lists of object references, they can be passed between tasks and actors without needing to make a copy of the data, which is crucial for making data-intensive applications and libraries scalable.\n",
    "\n",
    "|<img src=\"../_static/assets/Introduction_to_Ray_AIR/data_code.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|A general pattern for creating a `Dataset`, configuring a `Preprocessor`, and passing these into the `Trainer` for consistent data handling throughout the pipeline.|\n",
    "\n",
    "Let's take this generic structure and see how it plays out with our tip prediction task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Ray runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "if ray.is_initialized:\n",
    "    ray.shutdown()\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start a Ray cluster (check out these [instructions](https://docs.ray.io/en/latest/ray-overview/installation.html) if you haven't installed) so that Ray can utilize all the cores available to you as workers. \n",
    "\n",
    "- check `ray.is_initialized` to ensure that you start with a fresh cluster\n",
    "- use `ray.init()` to initialize a Ray context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Ray Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read Parquet file to Ray Dataset\n",
    "dataset = ray.data.read_parquet(\n",
    "    \"s3://anyscale-training-data/intro-to-ray-air/nyc_taxi_2021.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and validation subsets\n",
    "train_dataset, valid_dataset = dataset.train_test_split(test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split datasets into blocks for parallel preprocessing\n",
    "# num_blocks should be lower than number of cores in the cluster\n",
    "train_dataset = train_dataset.repartition(num_blocks=5)\n",
    "valid_dataset = valid_dataset.repartition(num_blocks=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding exercise**\n",
    "\n",
    "There exist many [`Dataset` API elements](https://docs.ray.io/en/latest/data/api/dataset.html#) available for common transformations and operations. Using the above as a reference:\n",
    "1. Inspect the schema from the underlying Parquet metadata.\n",
    "2. Count how many rows are in the training and validation datasets.\n",
    "3. Inspect the first five samples of either dataset.\n",
    "4. What is the average `fare_amount` grouped by `passenger_count`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAMPLE IMPLEMENTATION ###\n",
    "\n",
    "print(f\"Schema of training dataset: \\n {train_dataset.schema()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAMPLE IMPLEMENTATION ###\n",
    "\n",
    "print(f\"Number of samples in training dataset: \\n {train_dataset.count()}\")\n",
    "print(f\"Number of samples in validation dataset: \\n {valid_dataset.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAMPLE IMPLEMENTATION ###\n",
    "\n",
    "train_dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAMPLE IMPLEMENTATION ###\n",
    "\n",
    "train_dataset.groupby(\"passenger_count\").mean(\"fare_amount\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the dataset\n",
    "To transform our raw data into features, you will define a `Preprocessor`. Ray AIR's `Preprocessor` captures the data transformation you apply and persists:\n",
    "\n",
    "- **During Training**\n",
    "\n",
    "    `Preprocessor` is passed into a `Trainer` to `fit` and `transform` input `Dataset`s.\n",
    "- **During Tuning**\n",
    "\n",
    "    Each `Trial` will create its own copy of the `Preprocessor` and the fitting and transformation logic will occur once per `Trial`\n",
    "- **During Checkpointing**\n",
    "\n",
    "    The `Preprocessor` is saved in the `Checkpoint` if it was passed into the `Trainer`\n",
    "- **During Predicting**\n",
    "\n",
    "    If the `Checkpoint` contains a `Preprocessor`, then it will be used to call `transform_batch` on input batches prior to performing inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.preprocessors import MinMaxScaler\n",
    "\n",
    "preprocessor = MinMaxScaler(columns=[\"trip_distance\", \"trip_duration\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You define a `MinMaxScaler` preprocessor that will normalize the `trip_distance` and `trip_duration` columns by their range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding exercise**\n",
    "\n",
    "Ray AIR provides several [preprocessors out of the box](https://docs.ray.io/en/latest/ray-air/preprocessors.html#) as well as support for implementing custom preprocessors. \n",
    "\n",
    "For this exercise, visualize the distribution for each of the features in our dataset, read through the \"Which preprocessor should you use?\" section of the linked user guide above, and determine whether `MinMaxScaler` applied to `trip_distance` and `trip_duration` is sufficient.\n",
    "\n",
    "Later on, you can compare model performance between the given preprocessor and your custom configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAMPLE IMPLEMENTATION ###\n",
    "\n",
    "from ray.data.preprocessors import *\n",
    "\n",
    "pd_df = train_dataset.to_pandas(limit=1893433)\n",
    "pd_df.hist(\"trip_distance\")\n",
    "pd_df.hist(\"trip_duration\")\n",
    "\n",
    "sample_preprocessor = PowerTransformer(\n",
    "    columns=[\"trip_distance\", \"trip_duration\"], power=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the positively-skewed distributions for `trip_distance` and `trip_duration`. For these numerical features, you can choose an appropriate AIR `Preprocessor` depending on your data's properties:\n",
    "\n",
    "- `PowerTransformer`: your data isn't normal, but you need it to be\n",
    "- `Normalizer`: you need unit norm rows\n",
    "- `MinMaxScaler`: you aren't sure what your data looks like\n",
    "\n",
    "Feature scaling can offer a performance boost during training, and testing choice of `Preprocessor` is worth investigating when you have few features which are not already unit normalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "#### Key concepts\n",
    "\n",
    "`Dataset`\n",
    "\n",
    "The standard way to load and exchange data in Ray AIR. In AIR, Datasets are used extensively for data loading, preprocessing, and batch inference.\n",
    "\n",
    "`Preprocessors`\n",
    "\n",
    "Preprocessors are primitives that can be used to transform input data into features. Preprocessors operate on Datasets, which makes them scalable and compatible with a variety of datasources and dataframe libraries.\n",
    "\n",
    "Preprocessors persist:\n",
    "\n",
    "- during training to fit and transform input data\n",
    "- in each trial of hyperparameter tuning\n",
    "- within a checkpoint\n",
    "- on input batches for inference\n",
    "\n",
    "AIR comes with a collection of built-in preprocessors, and you can also define your own with simple templates which you can read more about in the [user guide](https://docs.ray.io/en/latest/ray-air/preprocessors.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Ray Train\n",
    "***\n",
    "\n",
    "Following data preprocessing, you can define the model for binary classification of big tip rides.\n",
    "\n",
    "![Train Highlight](../_static/assets/Introduction_to_Ray_AIR/train_highlight.png)\n",
    "\n",
    "### Introduction to Ray Train\n",
    "\n",
    "ML pracitioners tend to run into a few common problems with training models that prompt them to consider distributed solutions:\n",
    "\n",
    "1. training time is too long to be practical\n",
    "2. the data is too large to fit on one machine\n",
    "3. training many models sequentially doesn't utilize resources efficiently\n",
    "4. the model itself is too large to fit on a single machine\n",
    "\n",
    "[Ray Train](https://docs.ray.io/en/latest/ray-air/trainer.html) addresses these issues by cutting down runtime through distributed multi-node training with fault tolerance and leveraging Ray Data to distribute preprocessing and data ingestion.\n",
    "\n",
    "Fully integrated into the Ray AIR ecosystem, `Trainer`s can plug into:\n",
    "\n",
    "- Ray Data: to enable scalable data loading and preprocessing\n",
    "- Ray Tune: for distributed hyperparameter tuning\n",
    "- Ray AIR Predictor: as a checkpointed trained model to be applied during inference\n",
    "- Popular ML training frameworks like:\n",
    "    - PyTorch\n",
    "    - Tensorflow\n",
    "    - Horovod\n",
    "    - XGBoost\n",
    "    - HuggingFace Transformers\n",
    "    - Scikit-Learn\n",
    "    - and more\n",
    "\n",
    "**Key features**\n",
    "\n",
    "- callbacks for early stopping\n",
    "- checkpointing\n",
    "- integration with Tensorboard, Weights & Biases, and MLflow for observability\n",
    "- export mechanisms for models\n",
    "\n",
    "|<img src=\"../_static/assets/Introduction_to_Ray_AIR/train_code.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Training comes in two major parts: defining the `Trainer` object and then fitting it to the training dataset. In this code snippet, you use a `TorchTrainer`, however, this may be swapped out with any [integrations](https://docs.ray.io/en/latest/ray-air/package-ref.html#trainer-and-predictor-integrations).|\n",
    "\n",
    "Let's put these concepts in practice by applying it to our taxi problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define AIR `Trainer`\n",
    "\n",
    "Ray AIR provides a variety of [`Trainer`s](https://docs.ray.io/en/latest/ray-air/trainer.html) (PyTorch, Tensorflow, HuggingFace, etc.). In the example below, you will use an `XGBoostTrainer` to perform binary classification on these NYC Taxi rides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.air.config import ScalingConfig\n",
    "from ray.train.xgboost import XGBoostTrainer\n",
    "\n",
    "trainer = XGBoostTrainer(\n",
    "    label_column=\"is_big_tip\",\n",
    "    num_boost_round=50,\n",
    "    scaling_config=ScalingConfig(\n",
    "        num_workers=5,\n",
    "        use_gpu=False,\n",
    "    ),\n",
    "    params={\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": [\"logloss\", \"error\"],\n",
    "        \"tree_method\": \"approx\",\n",
    "    },\n",
    "    datasets={\"train\": train_dataset, \"valid\": valid_dataset},\n",
    "    preprocessor=preprocessor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct a `Trainer`, you provide:\n",
    "\n",
    "- a `ScalingConfig` which specifies how many parallel training workers and what type of resources (CPUs/GPUs) to use per worker during training\n",
    "- a dictionary of training and validation sets\n",
    "- the `Preprocessor` used to transform the `Dataset`s\n",
    "\n",
    "Optionally, you can choose to add `resume_from_checkpoint` which allows you to continue training from a saved checkpoint should the run be interrupted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To invoke training, call `.fit()`. Trainer objects produce a `Result` object which gives you access to metrics, checkpoints, and errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding exercise**\n",
    "\n",
    "You can check out the training results from the `Result` object with the following calls:\n",
    "\n",
    "```python\n",
    "# returns last saved checkpoint\n",
    "result.checkpoint\n",
    "\n",
    "# returns the `n` best saved checkpoints as configured in `RunConfig.CheckpointConfig`\n",
    "result.best_checkpoints\n",
    "\n",
    "# returns the final metrics as reported\n",
    "result.metrics\n",
    "\n",
    "# returns the contain an Exception if training failed\n",
    "result.error\n",
    "```\n",
    "\n",
    "Inspect your training result below. What is the reported accuracy for the training and validation runs? \n",
    "\n",
    "Note: `error` is the binary classification error rate in this case calculated as `#(wrong cases)/#(all cases)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAMPLE IMPLEMENTATION ###\n",
    "\n",
    "print(f\"Result metrics: \\n {result.metrics} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAMPLE IMPLEMENTATION ###\n",
    "\n",
    "print(f\"Training accuracy: {1 - result.metrics['train-error']:.4f}\")\n",
    "print(f\"Validation accuracy: {1 - result.metrics['valid-error']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "#### Key concepts\n",
    "\n",
    "`Trainer`\n",
    "\n",
    "Trainers are wrapper classes around third-party training frameworks such as XGBoost, Pytorch, and Tensorflow. They are built to help integrate with core Ray Actors (for distribution), Ray Datasets, and Ray Tune."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Ray Tune\n",
    "***\n",
    "\n",
    "Now that you have a baseline XGBoost model trained, you may want to improve performance by running hyperparameter tuning experiments.\n",
    "\n",
    "![Tune Highlight](../_static/assets/Introduction_to_Ray_AIR/tune_highlight.png)\n",
    "\n",
    "### Introduction to Ray Tune\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong><a href=\"https://en.wikipedia.org/wiki/Hyperparameter_optimization\" target=\"_blank\">Hyperparameter tuning or optimization</a></strong> is the process of choosing optimal hyperparameters for a machine learning model. Hyperparameters, in contrast to weights learned by the model, are parameters that you set to influence training.\n",
    "</div>\n",
    "\n",
    "\n",
    "Setting up and executing hyperparameter optimization (HPO) can be expensive in terms of compute resources and runtime with several complexities including:\n",
    "\n",
    "- **Vast Search Space**\n",
    "\n",
    "    Your model could have several hyperparameters, each with different data types, ranges, and possible correlations.\n",
    "    Sampling good candidates from high-dimensional spaces is difficult.\n",
    "- **Search Algorithms**\n",
    "\n",
    "    Choosing hyperparameters strategically requires testing complex search algorithms to achieve good results.\n",
    "- **Long Runtime**\n",
    "\n",
    "    Even if you distribute tuning, training complex models in themselves can take a long time to complete per run, so it's best to have an efficiency at every stage in the pipeline.\n",
    "- **Resource Allocation**\n",
    "\n",
    "    You must have enough compute resources available to during each trial as to not slow down search because of scheduling mismatches.\n",
    "- **User Experience**\n",
    "\n",
    "    Observability tooling for developers like stopping bad runs early, saving intermediate results, restarting from checkpoints, or pausing/resuming runs makes HPO easier.\n",
    "\n",
    "Ray Tune is a distributed HPO library that addresses all of these topics above to provide a simplified interface for running trials and integrates with popular frameworks such as HyperOpt and Optuna.\n",
    "\n",
    "|<img src=\"../_static/assets/Introduction_to_Ray_AIR/tune_code.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|General pattern for using AIR `Tuner`s which involves taking in a trainable, defining a search space, establishing a search algorithm, scheduling trials, and analyzing results.|\n",
    "\n",
    "Let's see how to interact with Ray Tune to make some improvements to our big tip classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use AIR `Tuner` for hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from ray.tune.tuner import Tuner, TuneConfig\n",
    "\n",
    "param_space = {\n",
    "    \"params\": {\n",
    "        \"eta\": tune.uniform(0.2, 0.4),\n",
    "        \"max_depth\": tune.randint(1, 6),\n",
    "        \"min_child_weight\": tune.uniform(0.8, 1.0),\n",
    "    }\n",
    "}\n",
    "\n",
    "tuner = Tuner(\n",
    "    trainer,\n",
    "    param_space=param_space,\n",
    "    tune_config=TuneConfig(num_samples=10, metric=\"train-logloss\", mode=\"min\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First define a search space with a few hyperparameters to tune:\n",
    "\n",
    "- `eta` is the learning rate\n",
    "- `max_depth` specifies how deep each tree is (default=6). A higher value leads to a more complex model.\n",
    "- `min_child_weight` defines the minimum sum of weights of all observations in a child; used to control overfitting\n",
    "\n",
    "To set up an AIR `Tuner`, you must specify:\n",
    "\n",
    "- `Trainer`: the training loop\n",
    "- `search space`: a set of hyperparameters you wish to tune\n",
    "- `search_algorithm`: to optimize parameter search\n",
    "- `scheduler`: (optional) to stop searches early and speed up experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute hyperparameter search and analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_grid = tuner.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can execute tuning on `num_samples=10` trials. After tuning, you can query the `ResultGrid` object to see metrics, results, and checkpoints of each trial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding exercise**\n",
    "\n",
    "You can probe the `ResultGrid` for metrics using these calls:\n",
    "\n",
    "```python\n",
    "\n",
    "# checks if there have been errors\n",
    "result_grid.errors\n",
    "\n",
    "# gets the best result\n",
    "best_result = result_grid.get_best_result()\n",
    "\n",
    "# gets the best checkpoint\n",
    "best_checkpoint = best_result.checkpoint\n",
    "\n",
    "# gets the best metrics\n",
    "best_metrics = best_result.metrics\n",
    "\n",
    "```\n",
    "\n",
    "Inspect your tunings results, what is the best result from these experiments? Are they better than the baseline model in the training step in the previous section?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAMPLE IMPLEMENTATION ###\n",
    "\n",
    "best_result = result_grid.get_best_result()\n",
    "print(f\"Best result: \\n {best_result} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAMPLE IMPLEMENTATION ###\n",
    "\n",
    "print(f\"Training accuracy: {1 - best_result.metrics['train-error']:.4f}\")\n",
    "print(f\"Validation accuracy: {1 - best_result.metrics['valid-error']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "#### Key concepts\n",
    "\n",
    "`Tuner`\n",
    "\n",
    "Provides an interface that works with AIR `Trainer`s to perform distributed hyperparameter tuning. You define a set of hyperparameters you wish to tune in a search space, specify a search algorithm, and the `Tuner` returns its results in a `ResultGrid` that contains metrics, results, and checkpoints for each `trial`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Ray AIR Predictors\n",
    "***\n",
    "\n",
    "Ray AIR Predictors load models from [checkpoints](https://docs.ray.io/en/latest/ray-air/key-concepts.html#air-checkpoints-doc) generated during training or tuning to perform distributed inference.\n",
    "\n",
    "`BatchPredictor` is a utility for large scale batch inference and takes in:\n",
    "1. `Checkpoint` - a saved model from training or tuning\n",
    "    - `Preprocessor` (optional) - defined earlier to transform input data for training can be reapplied to incoming batches\n",
    "2. `Predictor` - loads model from checkpoint to perform inference\n",
    "\n",
    "|<img src=\"../_static/assets/Introduction_to_Ray_AIR/batchpredict_code.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|`Checkpoint` and `Predictor` is passed into each instance of `BatchPredictor`|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use AIR `BatchPredictor` for Batch Prediction\n",
    "\n",
    "Previously, you have trained and tuned our XGBoost model on data from June 2021. You will now take the best checkpoint from tuning and perform offline or batch inference on taxi tip data from June 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.batch_predictor import BatchPredictor\n",
    "from ray.train.xgboost import XGBoostPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ray.data.read_parquet(\n",
    "    \"s3://anyscale-training-data/intro-to-ray-air/nyc_taxi_2022.parquet\"\n",
    ").drop_columns(\"is_big_tip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create `BatchPredictor` from best trial of HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_predictor = BatchPredictor.from_checkpoint(\n",
    "    best_result.checkpoint, XGBoostPredictor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `BatchPredictor` by passing in the best result from the hyperparameter tuning step along with an XGBoost specific `Predictor` to handle the distributed inference step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probabilities = batch_predictor.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run batch inference on the testing dataset by calling `predict()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding exercise**\n",
    "\n",
    "Now that you have the predictions generated from the testing set, how did the model perform? Compare the predictions outputted by `BatchPredictor` with the ground truth labels available in the raw data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAMPLE IMPLEMENTATION ###\n",
    "\n",
    "print(\"PREDICTED PROBABILITIES\")\n",
    "predicted_probabilities.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "#### Key concepts\n",
    "\n",
    "`Checkpoints`\n",
    "\n",
    "Store the full state of the model periodically, so that partially trained models are available and can be used to resume training from an intermediate point, instead of starting from scratch; also allows for the best model to be saved for batch inference later on.\n",
    "\n",
    "`BatchPredictor`\n",
    "\n",
    "Loads the best model from a checkpoint to perform batch inference on large-scales or online inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Ray Serve\n",
    "***\n",
    "\n",
    "Finally, you may want to serve this taxi tip prediction application to end users, hopefully with a low latency to be maximally useful to drivers on the job. This poses a challenge since machine learning models are compute intensive and ideally, this model wouldn't be served in isolation, but rather adjacent to business logic or even other ML models.\n",
    "\n",
    "![Serve Highlight](../_static/assets/Introduction_to_Ray_AIR/serve_highlight.png)\n",
    "\n",
    "### Introduction to Ray Serve\n",
    "\n",
    "Ray Serve is a scalable compute layer for serving machine learning models that enables serving individual models or creating composite model pipelines where you can independently deploy, update, and scale individual components. \n",
    "\n",
    "Serve isn't tied to a specific machine learning library, but rather treats models as ordinary Python code. \n",
    "\n",
    "Additionally, it allows you to flexibly combine normal Python business logic alongside machine learning models. This makes it possible to build online inference services completely end-to-end: \n",
    "\n",
    "- validate user input\n",
    "- query a database\n",
    "- perform inference scalably across multiple ML models\n",
    "- combine, filter, and validate the output all in the process of handling a single inference request\n",
    "\n",
    "|<img src=\"../_static/assets/Introduction_to_Ray_AIR/serve_code.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Pattern for deploying a `Predictor` from a `Checkpoint` wth Ray Serve.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `PredictorDeployment` for Online Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import serve\n",
    "from ray.serve import PredictorDeployment\n",
    "from ray.serve.http_adapters import pandas_read_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.run(\n",
    "    PredictorDeployment.options(\n",
    "        name=\"XGBoostService\", num_replicas=2, route_prefix=\"/rayair\"\n",
    "    ).bind(XGBoostPredictor, best_result.checkpoint, http_adapter=pandas_read_json)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy the best model as an inference service by using Ray Serve and the `PredictorDeployment` class. After deploying the service, you can send requests to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = test_dataset.take(1)\n",
    "sample_input = dict(sample_input[0])\n",
    "\n",
    "output = requests.post(\"http://localhost:8000/rayair\", json=[sample_input]).json()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try sending a request through HTTP. You can use the `PredictorDeployment` to deploy checkpoints trained in Ray AIR as live endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding exercise**\n",
    "\n",
    "You've just served a prediction for a single sample input from our test dataset. Predictors are able to accept array, dataframe, and custom inputs (that can be transformed to array or dataframe). You can also configure micro-batching to enhance performance.\n",
    "\n",
    "Try reading through the [user guide](https://docs.ray.io/en/latest/ray-air/examples/serving_guide.html) for predictors to accept incoming data for more than one sample and run the prediction again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAMPLE IMPLEMENTATION ###\n",
    "\n",
    "n = 5\n",
    "\n",
    "for i in range(n):\n",
    "    sample_input = test_dataset.take(i)\n",
    "    sample_input = dict(sample_input[0])\n",
    "\n",
    "    output = requests.post(\"http://localhost:8000/rayair\", json=[sample_input]).json()\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "#### Key concepts\n",
    "\n",
    "`Deployments`\n",
    "\n",
    "You can think of this as a managed group of Ray actors that can be addressed together and will handle requests load-balanced across them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shutdown Ray runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disconnect the worker, and terminate processes started by `ray.init()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Summary\n",
    "***\n",
    "You've now just created a Ray Dataset, preprocessed some features, built a model with XGBoost, searched a hyperparameter space for the best configuration, loaded the best model from a checkpoint to perform batch inference, and served that model for online inference. \n",
    "\n",
    "Through this end-to-end example, you explored how to use Ray AIR to distribute an entire ML pipeline.\n",
    "\n",
    "### Key concepts\n",
    "\n",
    "- `Datasets`\n",
    "\n",
    "    The standard way to load and exchange data in Ray AIR. In AIR, Datasets are used extensively for data loading, preprocessing, and batch inference.\n",
    "\n",
    "- `Preprocessors`\n",
    "\n",
    "    Preprocessors are primitives that can be used to transform input data into features. Preprocessors operate on Datasets, which makes them scalable and compatible with a variety of datasources and dataframe libraries.\n",
    "\n",
    "- `Trainers`\n",
    "\n",
    "    Trainers are wrapper classes around third-party training frameworks such as XGBoost, Pytorch, and Tensorflow. They are built to help integrate with core Ray Actors (for distribution), Ray Datasets, and Ray Tune.\n",
    "\n",
    "- `Tuner`\n",
    "\n",
    "    Provides an interface that works with AIR Trainers to perform distributed hyperparameter tuning. You define a set of hyperparameters you wish to tune in a search space, specify a search algorithm, and the Tuner returns its results in a ResultGrid that contains metrics, results, and checkpoints for each trial.\n",
    "\n",
    "- `Checkpoints`\n",
    "\n",
    "    Store the full state of the model periodically, so that partially trained models are available and can be used to resume training from an intermediate point, instead of starting from scratch; also allows for the best model to be saved for batch inference later on.\n",
    "\n",
    "- `BatchPredictor`\n",
    "\n",
    "    Loads the best model from a checkpoint to perform batch inference on large-scales or online inference.\n",
    "\n",
    "- `Deployments`\n",
    "\n",
    "    You can think of this as a managed group of Ray actors that can be addressed together and will handle requests load-balanced across them.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect with the Ray community\n",
    "\n",
    "You can learn and get more involved with the Ray community of developers and researchers:\n",
    "\n",
    "* [Ray documentation](https://docs.ray.io/en/latest)\n",
    "* [Official Ray Website](https://www.ray.io/): Browse the ecosystem and use this site as a hub to get the information that you need to get going and building with Ray.\n",
    "* [Join the Community on Slack](https://forms.gle/9TSdDYUgxYs8SA9e8): Find friends to discuss your new learnings in our Slack space.\n",
    "* [Use the Discussion Board](https://discuss.ray.io/): Ask questions, follow topics, and view announcements on this community forum.\n",
    "* [Join a Meetup Group](https://www.meetup.com/Bay-Area-Ray-Meetup/): Tune in on meet-ups to listen to compelling talks, get to know other users, and meet the team behind Ray.\n",
    "* [Open an Issue](https://github.com/ray-project/ray/issues/new/choose): Ray is constantly evolving to improve developer experience. Submit feature requests, bug-reports, and get help via GitHub issues.\n",
    "* [Become a Ray contributor](https://docs.ray.io/en/latest/ray-contribute/getting-involved.html): We welcome community contributions to improve our documentation and Ray framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../_static/assets/Generic/ray_logo.png\" width=\"20%\" loading=\"lazy\">"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "567405a8058597909526349386224fe35dd047505a91307e44ed44be00113429"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
