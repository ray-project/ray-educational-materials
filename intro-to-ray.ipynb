{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Ray\n",
    "---\n",
    "(*Suggested Time To Complete: 30 minutes*)\n",
    "\n",
    "Welcome! [detailed description and introduction goes here]\n",
    "\n",
    "<!-- - Simple Primitives: flexibly compose distributed applications with tasks, actors, and objects in native Python code.\n",
    "- Multi-cloud: run the same Ray code on any cloud --AWS, GCP, Azure -- or even on-prem.\n",
    "- Dynamic Scaling: Ray Core can automatically scale (using Ray Autoscaler) up or down to smoothly handle changing compute load.\n",
    "- Massive Scalability: Ray Core can easily scale to thousands of cores, and is getting more scalable with every release.\n",
    "- Open Community / Ecosystem: With a vibrant dedicated community and rich ecosystem of integrations, fixes, and best practices are easy to find.\n",
    "- Laptop -> Cluster With Ease: With Ray Client, going from laptop to cluster is as easy as changing 1 variable -->\n",
    "\n",
    "**Why Ray?**\n",
    "\n",
    "[discuss: scaling is a necessity, scaling is hard; make distributed computing easy and simple for everyone]\n",
    "- Machine Learning is pervasive\n",
    "- Distributed computing is a necessity\n",
    "- Python is the default language for DS/ML\n",
    "\n",
    "- Simple Primitives: flexibly compose distributed applications with tasks, actors, and objects in native Python code.\n",
    "- Dynamic Scaling: Ray Core can automatically scale (using Ray Autoscaler) up or down to smoothly handle changing compute load.\n",
    "- Massive Scalability: Ray Core can easily scale to thousands of cores, and is getting more scalable with every release.\n",
    "- Open Community / Ecosystem: With a vibrant dedicated community and rich ecosystem of integrations, fixes, and best practices are easy to find.\n",
    "- Laptop -> Cluster With Ease: With Ray Client, going from laptop to cluster is as easy as changing 1 variable\n",
    "- Multi-cloud: Run the same Ray code on any cloud -- AWS, GCP, Azure -- or even on-premise.\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "[insert pre-reqs or target audience for this notebook]\n",
    "\n",
    "**Notebook Outline**\n",
    "\n",
    "- Introduction to Ray\n",
    "- Part One: Ray Core\n",
    "- Part Two: Ray AIR\n",
    "    - Ray Data\n",
    "    - Ray Train\n",
    "    - Ray Tune\n",
    "    - Ray Serve\n",
    "    - Ray RLlib\n",
    "- Part Three: Ray Ecosystem\n",
    "- Part Four: Ray Clusters\n",
    "- Homework\n",
    "- Next Steps\n",
    "\n",
    "**Learning Goals**\n",
    "\n",
    "[after this notebook, you will be able to...]\n",
    "\n",
    "Emmy's Notes:\n",
    "\n",
    "- Add more detailed description of what Ray is, the motivation behind Ray, what we will cover in this notebook, and the learning goals.\n",
    "- Add Map of Ray image at the top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part One: Ray Core\n",
    "---\n",
    "Ray Core is a low-level, distributed computing framework for Python with a concise core API, and you can think of it as the foundation that Ray's data science libraries (Ray AIR) and third-party integrations (Ray Ecosystem) are built on. This simple and general-purpose Python library enables every developer to easily build scalable, distributed systems that run on your laptop, cluster, cloud or Kubernetes.\n",
    "\n",
    "Ray sets up and manages clusters of computers so that you can run distributed tasks on them. A Ray cluster consists of nodes that are connected to each other via a network. You can program against the so-called driver, the program root, which lives on the head node. The driver can run jobs, that is a collection of tasks, that are run on the nodes in the cluster. Specifically, the individual tasks of a job are run on worker processes on worker nodes.\n",
    "\n",
    "**Most Popular Use Cases**\n",
    "\n",
    "- Using Ray for Highly Parallel Tasks: **description, get actual most popular use case feedback from eng**\n",
    "- Parallel Model Selection: description\n",
    "\n",
    "**Emmy's TO DOs:**\n",
    "- Add image below describing Ray Core's relationship to everything else to come"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 💾 Code Example\n",
    "In this example, we will use Ray Core to implement a distributed version of the classic algorithm quick sort. Quick sort selects an element from the array as a pivot, partitions the array into two sub-arrays (according to whether they are less than or greater than the pivot), then sorts them recursively. We'll go through the mechanics in depth in the code sample, and you can view an animation of quick sort in action below in Figure **#TBD.**\n",
    "\n",
    "Quick sort is an example of a divide and conquer algorithm, a strategy of solving a large problem by recursively breaking the problem down into smaller sub-problems. By nature, this strategy lends itself well to a parallel implementation because the execution of each operation independently solves smaller instances of the same problem which is then merged into the final solution.\n",
    "\n",
    "Below, we will take a tour of the sequential implementation before looking at how to distribute it with Ray. At the end, we will compare the code differences between the two to further highlight how concise Ray Core's API is.\n",
    "\n",
    "**[INSERT GIF OF QUICKSORT, not now though, it would make the version history so beefy]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential Implementation of Quick Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(collection):\n",
    "    # Use the last element as the first pivot\n",
    "    pivot = collection.pop()\n",
    "    greater, lesser = [], []\n",
    "    for element in collection:\n",
    "        if element > pivot:\n",
    "            greater.append(element)\n",
    "        else:\n",
    "            lesser.append(element)\n",
    "    return lesser, pivot, greater\n",
    "\n",
    "def quick_sort(collection):\n",
    "    lesser, pivot, greater = partition(collection)\n",
    "    lesser = quick_sort(lesser)\n",
    "    greater = quick_sort(greater)\n",
    "    return lesser + [pivot] + greater\n",
    "\n",
    "def quick_sort_sequential(collection):\n",
    "    lesser, pivot, greater = partition(collection)\n",
    "    lesser = quick_sort_sequential(lesser)\n",
    "    greater = quick_sort_sequential(greater)\n",
    "    return lesser + [pivot] + greater"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Ray Cluster to Begin Distributed Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "if ray.is_initialized:\n",
    "    ray.shutdown()\n",
    "cluster_info = ray.init(num_cpus=8)\n",
    "cluster_info.address_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the Same Partition and Quick Sort Functions From Above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(collection):\n",
    "    # Use the last element as the first pivot\n",
    "    pivot = collection.pop()\n",
    "    greater, lesser = [], []\n",
    "    for element in collection:\n",
    "        if element > pivot:\n",
    "            greater.append(element)\n",
    "        else:\n",
    "            lesser.append(element)\n",
    "    return lesser, pivot, greater\n",
    "\n",
    "def quick_sort(collection):\n",
    "    lesser, pivot, greater = partition(collection)\n",
    "    lesser = quick_sort(lesser)\n",
    "    greater = quick_sort(greater)\n",
    "    return lesser + [pivot] + greater"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Decorator and Append `.remote()` to Tasks to Parallelize with Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def quick_sort_distributed(collection):\n",
    "    lesser, pivot, greater = partition(collection)\n",
    "    lesser = quick_sort_distributed.remote(lesser)\n",
    "    greater = quick_sort_distributed.remote(greater)\n",
    "    return ray.get(lesser) + [pivot] + ray.get(greater)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Difference\n",
    "\n",
    "Here's a summary of the changes we made to go from sequential to distributed.\n",
    "\n",
    "```diff\n",
    "+ import ray\n",
    "\n",
    "def partition(collection):\n",
    "    # Use the last element as the first pivot\n",
    "    pivot = collection.pop()\n",
    "    greater, lesser = [], []\n",
    "    for element in collection:\n",
    "        if element > pivot:\n",
    "            greater.append(element)\n",
    "        else:\n",
    "            lesser.append(element)\n",
    "    return lesser, pivot, greater\n",
    "\n",
    "def quick_sort(collection):\n",
    "    lesser, pivot, greater = partition(collection)\n",
    "    lesser = quick_sort(lesser)\n",
    "    greater = quick_sort(greater)\n",
    "    return lesser + [pivot] + greater\n",
    "\n",
    "+ @ray.remote\n",
    "- def quick_sort_sequential(collection):\n",
    "def quick_sort_distributed(collection):\n",
    "    lesser, pivot, greater = partition(collection)\n",
    "-   lesser = quick_sort_distributed(lesser)\n",
    "+    lesser = quick_sort_distributed.remote(lesser)\n",
    "-   greater = quick_sort_sequential(greater)\n",
    "+    greater = quick_sort_distributed.remote(greater)\n",
    "-   return less + [pivot] + greater\n",
    "+    return ray.get(lesser) + [pivot] + ray.get(greater)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Excercise: [this]\n",
    "Try out this thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "1. Introduced to Ray Core and Most Popular Workloads\n",
    "2. Sequential Implementation of Quicksort\n",
    "3. Distributed Implementation of Quicksort\n",
    "\n",
    "#### Key Concepts\n",
    "#### Key API Elements in This Section\n",
    "#### Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Part Two: Ray AIR\n",
    "---\n",
    "Ray AI Runtime (AIR) is an open-source toolkit for building ML applications. It provides libraries for distributed data process, model training, tuning, reinforcement learning, model serving, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Data\n",
    "Ray Datasets are the standard way to load and exchange data in Ray libraries and applications. They provide basic distributed data transformations such as map, filter, and repartition, and are compatible with a variety of file formats, data sources, and distributed frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataset of Python objects.\n",
    "ds = ray.data.range(10000)\n",
    "# -> Dataset(num_blocks=200, num_rows=10000, schema=<class 'int'>)\n",
    "\n",
    "ds.take(5)\n",
    "# -> [0, 1, 2, 3, 4]\n",
    "\n",
    "ds.count()\n",
    "# -> 10000\n",
    "\n",
    "# Create a Dataset of Arrow records.\n",
    "ds = ray.data.from_items([{\"col1\": i, \"col2\": str(i)} for i in range(10000)])\n",
    "# -> Dataset(num_blocks=200, num_rows=10000, schema={col1: int64, col2: string})\n",
    "\n",
    "ds.show(5)\n",
    "# -> {'col1': 0, 'col2': '0'}\n",
    "# -> {'col1': 1, 'col2': '1'}\n",
    "# -> {'col1': 2, 'col2': '2'}\n",
    "# -> {'col1': 3, 'col2': '3'}\n",
    "# -> {'col1': 4, 'col2': '4'}\n",
    "\n",
    "ds.schema()\n",
    "# -> col1: int64\n",
    "# -> col2: string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "#### Key Concepts\n",
    "#### Key API Elements in This Section\n",
    "#### Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Train\n",
    "Ray Train is a lightweight library for distributed deep learning that allows you to easily supercharge your distributed PyTorch and TensorFlow training on Ray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.train as train\n",
    "from ray.train import Trainer\n",
    "import torch\n",
    "\n",
    "def train_func():\n",
    "    # Setup model.\n",
    "    model = torch.nn.Linear(1, 1)\n",
    "    model = train.torch.prepare_model(model)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "    # Setup data.\n",
    "    input = torch.randn(1000, 1)\n",
    "    labels = input * 2\n",
    "    dataset = torch.utils.data.TensorDataset(input, labels)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=32)\n",
    "    dataloader = train.torch.prepare_data_loader(dataloader)\n",
    "\n",
    "    # Train.\n",
    "    for _ in range(5):\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return model.state_dict()\n",
    "\n",
    "trainer = Trainer(backend=\"torch\", num_workers=4)\n",
    "trainer.start()\n",
    "results = trainer.run(train_func)\n",
    "trainer.shutdown()\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "#### Key Concepts\n",
    "#### Key API Elements in This Section\n",
    "#### Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Tune\n",
    "Ray Tune is a Python library for fast hyperparameter tuning at scale. Easily distribute your trial runs to quickly find the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    " \n",
    "def objective(step, alpha, beta):\n",
    "   return (0.1 + alpha * step / 100)**(-1) + beta * 0.1\n",
    " \n",
    "def training_function(config):\n",
    "   # Hyperparameters\n",
    "   alpha, beta = config[\"alpha\"], config[\"beta\"]\n",
    "   for step in range(10):\n",
    "       # Iterative training function - can be any arbitrary training procedure.\n",
    "       intermediate_score = objective(step, alpha, beta)\n",
    "       # Feed the score back back to Tune.\n",
    "       tune.report(mean_loss=intermediate_score)\n",
    " \n",
    "analysis = tune.run(\n",
    "   training_function,\n",
    "   config={\n",
    "       \"alpha\": tune.grid_search([0.001, 0.01, 0.1]),\n",
    "       \"beta\": tune.choice([1, 2, 3])\n",
    "   })\n",
    " \n",
    "print(\"Best config: \", analysis.get_best_config(\n",
    "   metric=\"mean_loss\", mode=\"min\"))\n",
    " \n",
    "# Get a dataframe for analyzing trial results.\n",
    "df = analysis.results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "#### Key Concepts\n",
    "#### Key API Elements in This Section\n",
    "#### Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Serve\n",
    "Ray Serve lets you serve machine learning models in real-time or batch using a simple Python API. Serve individual models or create composite model pipelines, where you can independently deploy, update, and scale individual components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from ray import serve\n",
    "\n",
    "@serve.deployment(route_prefix=\"/iris\")\n",
    "class BoostingModel:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.label_list = iris_dataset[\"target_names\"].tolist()\n",
    "\n",
    "    async def __call__(self, request):\n",
    "        payload = await request.json()\n",
    "        print(f\"Received flask request with data {payload}\")\n",
    "\n",
    "        prediction = self.model.predict([payload[\"vector\"]])[0]\n",
    "        human_name = self.label_list[prediction]\n",
    "        return {\"result\": human_name}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Train model.\n",
    "    iris_dataset = load_iris()\n",
    "    model = GradientBoostingClassifier()\n",
    "    model.fit(iris_dataset[\"data\"], iris_dataset[\"target\"])\n",
    "\n",
    "    # Deploy model\n",
    "    serve.run(BoostingModel.bind(model))\n",
    "\n",
    "    # Query model\n",
    "    sample_request_input = {\"vector\": [1.2, 1.0, 1.1, 0.9]}\n",
    "    response = requests.get(\"http://localhost:8000/iris\", json=sample_request_input)\n",
    "    print(response.text)\n",
    "    \n",
    "    # prints\n",
    "    # Result:\n",
    "    # {\"result\": \"versicolor\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "#### Key Concepts\n",
    "#### Key API Elements in This Section\n",
    "#### Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray RLLib\n",
    "RLlib is the industry-standard reinforcement larning Python frameowrk built on Ray. Designed for quick interation and a fast path to production, it includes 25+ latest algorithms that are all implemented to run at scale and in multi-agent mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    " \n",
    "tune.run(PPOTrainer, config={\n",
    "   \"env\": \"CartPole-v0\",\n",
    "   \"framework\": \"torch\",\n",
    "   \"log_level\": \"INFO\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "#### Key Concepts\n",
    "#### Key API Elements in This Section\n",
    "#### Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Three: Ray Ecosystem\n",
    "---\n",
    "Up until now, we've covered Ray Core, the low-level, distributed computing framework, as well as Ray AIR, the set of high-level native libraries which include Ray Data, Train, Tune, Serve, and RLlib. In addition, Ray integrates with a growing ecosystem of the most popular Python and machine learning libraries and frameworks that you may already be familiar with.\n",
    "\n",
    "Instead of trying to create new standards, Ray allows you to scale existing workloads by unifying tools in a common interface. This interface enables you to run tasks in a distributed way, a property most of the respective backends don't have, or not to the same extent.\n",
    "\n",
    "For example, Ray Datasets is backed by Arrow and comes with many integrations to other frameworks, such as Spark and Dask. Ray Train and RLlib are backed by the full power of Tensorflow and PyTorch. Ray Tune supports algorithms from practically every noteable HPO tool available, including Hyperopt, Optuna, Nevergrad, Ax, SigOpt, and many others. Ray Serve can be used with frameworks such as FastAPI, gradio, and Streamlit. With ample opportunity to extend current projects as well as integrate more backends in the future, a key strength of Ray is this robust design pattern for integration which you can read more about [here](https://www.anyscale.com/blog/ray-distributed-library-patterns).\n",
    "\n",
    "For a complete list of integrations with links, go to the [Ray Ecosystem](https://docs.ray.io/en/latest/ray-overview/ray-libraries.html#) page in the docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: HuggingFace\n",
    "### Example: XGBoost\n",
    "### Example: Distributed Scikit-Learn / Joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "#### Key Concepts\n",
    "#### Key API Elements in This Section\n",
    "#### Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Four: Ray Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "#### Key Concepts\n",
    "#### Key API Elements in This Section\n",
    "#### Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "---\n",
    "If you would like to practice your new skills further with some in-depth examples beyond the embedded coding excercises, take a look at this list of suggested problems:\n",
    "- [Distribute a Classical Algorithm with Ray](https://github.com/ray-project/hackathon5-algo)\n",
    "    - In this excercise, go to the GitHub repo linked above for details on choosing a classic algorithm implemented in Python, editing the implementation to parallelize the work with Ray, and compare your results against the sequential implementation.\n",
    "\n",
    "\n",
    "# Next Steps\n",
    "---\n",
    "🎉 Congratulations! You have completed your first tutorial on an Introduction to Ray! We discussed the three layers of Ray: Core, AIR, and the Ecosystem. Each library in Ray AIR (Data, Train, Tune, Serve, RLLib) and the ecosystem of integrated libraries runs on Ray Core's distributed execution engine, and with Ray Clusters, you can deploy your workloads on AWS, GCP, Azure, or on Kubernetes.\n",
    "\n",
    "From here, you can learn and get more involved with our active community of developers and researchers by checking out the following resources:\n",
    "- 📖 [Ray's \"Getting Started\" Guides](https://docs.ray.io/en/latest/ray-overview/index.html): A collection of QuickStart Guides for every library including installation walkthrough, examples, blogs, talks, and more!\n",
    "- 💻 [Official Ray Website](https://www.ray.io/): Browse the ecosystem and use this site as a hub to get the information that you need to get going and building with Ray.\n",
    "- 💬 [Join the Community on Slack](https://forms.gle/9TSdDYUgxYs8SA9e8): Find friends to discuss your new learnings in our Slack space.\n",
    "- 📣 [Use the Discussion Board](https://discuss.ray.io/): Ask questions, follow topics, and view announcements on this community forum.\n",
    "- 🙋‍♀️ [Join a Meetup Group](https://www.meetup.com/Bay-Area-Ray-Meetup/): Tune in on meet-ups to listen to compelling talks, get to know other users, and meet the team behind Ray.\n",
    "- 🪲 [Open an Issue](https://github.com/ray-project/ray/issues/new/choose): Ray is constantly evolving to improve developer experience. Submit feature requests, bug-reports, and get help via GitHub issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "567405a8058597909526349386224fe35dd047505a91307e44ed44be00113429"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
