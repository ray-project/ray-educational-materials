{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Ray\n",
    "---\n",
    "(*Suggested Time To Complete: 40 minutes*)\n",
    "\n",
    "Welcome, we're glad to have you along! This module serves as an interactive introduction to Ray, a flexible distributed computing framework built for Python with data science and machine learning practitioners in mind. Before we jump into the structure of this tutorial, let us first unpack the context of where we are coming from along with the motivation for learning Ray.\n",
    "\n",
    "![Main Map](intro-to-ray-images/Main_Map.png)\n",
    "\n",
    "*Figure 1*\n",
    "\n",
    "**Context**\n",
    "\n",
    "When we think about our daily interactions with artificial intelligence (AI), products such as recommendation systems, photo editing software, and auto-captioning on videos come to mind. In addition to these, user-facing experiences, enterprise-level use cases like reducing downtime in manufacturing, order-fulfillment optimization, and maximizing power generation in wind farms contribute to ever-growing integration of AI with the way we live. Furthermore, today's AI applications require enormous amounts of data to be trained on and machine learning models tend to grow over time. They have become so complex and infrastructure intensive that developers have no option but to distribute execution across multiple machines. However, distributing computing is hard. It requires specialized knowledge about orchestrating clusters of computers together to efficiently schedule tasks and must provide features like fault tolerance when a component fails, high availability to minimize service interruption, and autoscaling to reduce waste.\n",
    "\n",
    "As a data scientist, machine learning pracitioner, developer, or engineer, your contribution may center on building data processing pipelines, training complicated models, running efficient hyperparameter experiments, creating simulations of agents, and/or serving your application to users. In each case, you need to choose a distributed system to support each task, but you don't want to learn a different programming language or toss out your existing toolbox. This is where Ray comes in.\n",
    "\n",
    "**What is Ray?**\n",
    "\n",
    "Ray is an open source, distributed execution framework that allows you to scale AI and machine learning workloads. Our goal is to keep things simple (which is enabled by a concise core API) so that you can parallelize Python programs on your laptop, cluster, cloud, or even on-premise with minimal code changes. Ray automatically handles all aspects of distributed execution including orchestration, scheduling, fault tolerance, and auto scaling so that you can scale your apps without becoming a distributed systems expert. With a rich ecosystem of distributed machine learning libraries and integrations with many important data science tools, Ray lowers the effort needed to scale compute intensive workloads.\n",
    "\n",
    "**Notebook Outline**\n",
    "\n",
    "We will discuss the four major **layers** that comprise Ray, namely its core engine, high-level libraries, ecosystem of integrations, and cluster deployment support (see Figure 1). In each notebook section, we will cover a layer of Ray through a basic example, and to foster active learning, you will encounter short coding excercises and discussion questions throughout the notebook to reinforce knowledge through practice. This tutorial is designed to be both narrative as well as modular, so feel free to work through in order or skip to the relevant material for you. Below, you'll find a table of contents.\n",
    "\n",
    "**Introduction to Ray**\n",
    "- Part One: Ray Core\n",
    "- Part Two: Ray AIR\n",
    "    - Ray Data\n",
    "    - Ray Train\n",
    "    - Ray Tune\n",
    "    - Ray Serve\n",
    "    - Ray RLlib\n",
    "- Part Three: Ray Ecosystem\n",
    "- Part Four: Ray Clusters\n",
    "- Homework\n",
    "- Next Steps\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "To gain the most from this notebook, it helps if you have a working knowledge of Python as well as previous experience with machine learning. The ideal learner has minimal familiarity with Ray and is interested in leveraging Ray's simple distributed computing framework to scale AI and Python workloads.\n",
    "\n",
    "**Learning Goals**\n",
    "\n",
    "Upon completion of this module, you will have a intuition for:\n",
    "1. a general overview and introduction to the layers of Ray\n",
    "2. popular workloads best suited for each layer\n",
    "3. how to navigate next steps to start scaling your own workloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part One: Ray Core\n",
    "---\n",
    "\n",
    "![Ray Core](intro-to-ray-images/Ray_Core.png)\n",
    "\n",
    "*Figure 2*\n",
    "\n",
    "Ray Core is a low-level, distributed computing framework for Python with a concise core API, and you can think of it as the foundation that Ray's data science libraries (Ray AIR) and third-party integrations (Ray Ecosystem) are built on. This simple and general-purpose Python library enables every developer to easily build scalable, distributed systems that run on your laptop, cluster, cloud or Kubernetes.\n",
    "\n",
    "A key strength lies in Ray Core's simple primitives: Tasks, Actors, and Objects.\n",
    "\n",
    "**Tasks:** Ray enables you to designate functions to be executed asychronously on separate Python workers. These asynchronous Ray functions, *tasks*, can specify their resource requirements in terms of CPUs, GPUs, and custom resources which are used by the cluster scheduler to distribute tasks for parallelized execution.\n",
    "\n",
    "**Actors:** What tasks are to functions, actors are to classes. An actor is a stateful worker and methods of the actor are scheduled on that specific worker and can access and mutate the state of that worker.\n",
    "\n",
    "**Objects:** In Ray, tasks and actors create and compute on objects, and we refer to these objects as *remote objects* because they can be stored anywhere in a Ray cluster. We use *object references* to refer to them, and they are cached in Ray's distributed shared memory *object store*.\n",
    "\n",
    "Ray sets up and manages clusters of computers so that you can run distributed tasks on them. For more information about how Ray clusters work in detail, jump down to Part Four of this notebook or [check out the documentation](https://docs.ray.io/en/latest/cluster/key-concepts.html#cluster-head-node). For now, the best way to understand Ray Core is to motivate it with some popular use cases and then try our hand at parallelizing a program outselves.\n",
    "\n",
    "**Most Popular Use Cases**\n",
    "\n",
    "- Using Ray for Highly Parallel Tasks: **description, get actual most popular use case feedback from eng**\n",
    "- Parallel Model Selection: description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üè° Coding Excercise: Housing Prices with `sklearn` üìà\n",
    "***\n",
    "Let's take a look at how we can apply Ray Core's flexible and simple API to scale a bare bones version of a common ML task: cross-validation.\n",
    "\n",
    "Here, we have a dataset of [California Housing](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html) with 20,640 samples and features including `[longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, median_hous_value, ocean_proximity]`. Given that we want to use a linear regression model, we want to assess how the results will generalize to an unseen independent dataset, say, on new housing data coming in this year. To do this, we would try cross-validation which is a model validation technique that resamples different portions of the data to train and test a model on different iterations. After we conduct these trials, we can average the error to get an estimate of the model's predictive performance.\n",
    "\n",
    "However, training the same model multiple times on different subsets of a dataset can take a really long time, especially if you're working with a much more complex model and larger dataset. In this example, we will first implement the sequential approach, then improve it by distributing training with Ray Core, and finally compare the code differences to highlight how minimal the changes are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Relevant Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "num_trials = 1000\n",
    "X, y = fetch_california_housing(return_X_y=True, as_frame=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train 1000 Models Sequentially\n",
    "Here, we will define an `sklearn` function that randomly splits our housing dataset into testing and training subsets (in the style of Monte Carlo Cross-Validation, where subsets are generated without replacement and has non-unique subsets from round to round). `sequential()` then fits a model, generates predictions, and returns the R-squared score (closer to 1 = better performance, closer to 0 = worse performance).\n",
    "\n",
    "Then, we'll train 1000 models on these random splits, one after another, and finally print out the average of the rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5995951366616112\n",
      "CPU times: user 17.8 s, sys: 11.5 s, total: 29.3 s\n",
      "Wall time: 8.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def sequential():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    model = LinearRegression()\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    r2 = metrics.r2_score(y_test, predictions)\n",
    "\n",
    "    return r2\n",
    "\n",
    "errors_seq = []\n",
    "\n",
    "for i in range (num_trials):\n",
    "    errors_seq.append(sequential())\n",
    "\n",
    "print(sum(errors_seq) / num_trials)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Sequential -> Parallel\n",
    "\n",
    "We just trained 1000 linear regression models in a series and averaged their R-squared values in about ~8 seconds. Let's now leverage Ray to train these models in parallel and see a runtime improvement. When we say *sequential*, this means that each process happens one after the other in a series as opposed to a *parallel* implementation where multiple operations can happen all at once.\n",
    "\n",
    "With just a few code changes, we will modify our existing Python program to distribute it among n number of workers. Of course, this is a lightweight example, but it's illustrative of the kind of user experience you get with Ray Core's lean API.\n",
    "\n",
    "Notice here that we need to use four API calls:\n",
    "\n",
    "1. `ray.init()` - initialize a Ray context\n",
    "2. `@ray.remote` - function or class decorator specifying that the function will be executed as a task or the class as an actor\n",
    "3. `.remote()` - postfix to every remote function, remote class declaration, or invocation of a remote class method; specifies an asynchronous operation\n",
    "4. `ray.get()` - return an object or list of objects from the object ID\n",
    "\n",
    "You may notice that instead of storing the result of `train.remote()` directly into a list of `errors`, we instead store it in a list called `obj_ids`. Once you run a Ray remote function, it will immediately return an `ObjectID`. This `ObjectID` is a *promise* of future work, meaning that the actual task of the function is delegated in the background to a worker. In order to access the expected output, you need to call `ray.get()` on the `ObjectID`\n",
    "\n",
    "![Housing Diff](intro-to-ray-images/Housing_Diff.png)\n",
    "\n",
    "*Figure 3*\n",
    "\n",
    "And with just a few lines of difference, we're able to parallelize quick sort without having to concern ourselves with orchestration, fault tolerance, autoscaling, or anything else that requires specialized knowledge of distributed systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train 1000 Models in Parallel with Ray\n",
    "To start, we'll import Ray (check out our [installation instructions](https://docs.ray.io/en/latest/ray-overview/installation.html)) and start a Ray cluster on our local machine that can utilize all the cores available on your computer as workers. We use `ray.is_initialized` to allow us to make sure that we only have one Ray cluster active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-11 14:32:45,066\tINFO worker.py:1509 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8266 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.10.6</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 2.0.0</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n",
       "    <td style=\"text-align: left\"><b><a href=\"http://127.0.0.1:8266\" target=\"_blank\">http://127.0.0.1:8266</a></b></td>\n",
       "</tr>\n",
       "\n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='127.0.0.1:8266', python_version='3.10.6', ray_version='2.0.0', ray_commit='cba26cc83f6b5b8a2ff166594a65cb74c0ec8740', address_info={'node_ip_address': '127.0.0.1', 'raylet_ip_address': '127.0.0.1', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-10-11_14-32-43_095356_64287/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-10-11_14-32-43_095356_64287/sockets/raylet', 'webui_url': '127.0.0.1:8266', 'session_dir': '/tmp/ray/session_2022-10-11_14-32-43_095356_64287', 'metrics_export_port': 63046, 'gcs_address': '127.0.0.1:60351', 'address': '127.0.0.1:60351', 'dashboard_agent_listen_port': 52365, 'node_id': '59bba0adf6e2134100e35763894f105b9499c58b69eed8b903cfa57c'})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "if ray.is_initialized:\n",
    "    ray.shutdown()\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As illustrated above in Figure 3, we will add the decorator `@ray.remote` to our function `distributed()` to specify that it is a remote task to be run remotely. Then we call that function as `distributed.remote()` in the `for` loop to append to our list of object ids. Finally, we fetch the result outside of the loop to access the final error list (as to not *block* the launching of remote tasks asynchronously) and print out the average R-squared value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5895728400213048\n",
      "CPU times: user 192 ms, sys: 75.4 ms, total: 268 ms\n",
      "Wall time: 1.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "@ray.remote\n",
    "def distributed():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    model = LinearRegression()\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    r2 = metrics.r2_score(y_test, predictions)\n",
    "\n",
    "    return r2\n",
    "\n",
    "obj_ids = []\n",
    "\n",
    "for i in range (num_trials):\n",
    "    obj_ids.append(distributed.remote())\n",
    "\n",
    "errors_dist = ray.get(obj_ids)\n",
    "\n",
    "print(sum(errors_dist) / num_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now you've done it! You have distributed the training of 1000 models in a very through round of cross-validation on our California Housing dataset. Compare the runtime for each method of training 1000 models. Is this what you expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üíæ Optional Excercise: Quick Sort ‚ö°Ô∏è\n",
    "***\n",
    "If you would like to test your knowledge of the Ray Core concepts we covered in the housing example, try your hand at building a distributed version of the classic algorithm, quick sort.\n",
    "\n",
    "At a glance, quick sort selects an element from the array as a pivot, partitions the array into two sub-arrays (according to whether they are less than or greater than the pivot), then sorts them recursively. You can preview an animation of quick sort in action below in Figure 4.\n",
    "\n",
    "Quick sort is an example of a divide and conquer algorithm, a strategy of solving a large problem by recursively breaking the problem down into smaller sub-problems. By nature, this strategy lends itself well to a parallel implementation because the execution of each operation independently solves smaller instances of the same problem which is then merged into the final solution.\n",
    "\n",
    "Let's walkthrough the mechanics a little deeper in the code sample that follows.\n",
    "\n",
    "![Quick Sort](https://upload.wikimedia.org/wikipedia/commons/6/6a/Sorting_quicksort_anim.gif)\n",
    "\n",
    "*Figure 4*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential Implementation of Quick Sort\n",
    "Here is the classic version that you may be familiar with below.\n",
    "\n",
    "1. Given an array, `partition` uses the last element as the pivot and creates two subarrays of elements less than the pivot and elements greater than the pivot (`greater` will always be empty in this case).\n",
    "2. A call to `quick_sort_sequential` partitions the array into two subarrays and the pivot element, then calls itself by passing in the two newly created subarrays, and finally merges the sorted answer.\n",
    "3. We use the `if len(collection) <= 500000` condition because it will be useful later in the distributed version to avoid tiny tasks. We'll discuss it further in subsequent tutorials, but if you are curious, find more information [here](https://docs.ray.io/en/latest/ray-core/tips-for-first-time.html#tip-2-avoid-tiny-tasks).\n",
    "\n",
    "Notice that each merge depends on the recursive call below it to return before completing its task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(collection):\n",
    "    pivot = collection.pop()\n",
    "    greater, lesser = [], []\n",
    "    for element in collection:\n",
    "        if element > pivot:\n",
    "            greater.append(element)\n",
    "        else:\n",
    "            lesser.append(element)\n",
    "    return lesser, pivot, greater\n",
    "\n",
    "def quick_sort_sequential(collection):\n",
    "    if len(collection) <= 500000:\n",
    "        return sorted(collection)\n",
    "    lesser, pivot, greater = partition(collection)\n",
    "    lesser = quick_sort_sequential(lesser)\n",
    "    greater = quick_sort_sequential(greater)\n",
    "    return lesser + [pivot] + greater"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Turn: Distributed Implementation of Quick Sort\n",
    "\n",
    "In the space below, try to apply the key concepts in the section before to create a distributed implementation of quick sort. We'll start you out with some base of code to modify, but it's up to you finish it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "if ray.is_initialized:\n",
    "    ray.shutdown()\n",
    "\n",
    "### MODIFY THIS CODE BELOW ###\n",
    "def quick_sort_distributed(collection):\n",
    "    if len(collection) <= 500000:\n",
    "        return sorted(collection)\n",
    "    lesser, pivot, greater = partition(collection)\n",
    "    lesser = quick_sort_sequential(lesser)\n",
    "    greater = quick_sort_sequential(greater)\n",
    "    return lesser + [pivot] + greater"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare Both Versions on a Large Array\n",
    "\n",
    "Let's now compare these two implementations by timing each and inspecting the results. We'll create a large random array and time both methods to see how long each takes. Note that we use `ray.put()` to store the array in the object store as to not pass the same large object as an argument repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from numpy import random\n",
    "\n",
    "size = 10_000_000\n",
    "\n",
    "unsorted = random.randint(1000000, size=(size)).tolist()\n",
    "\n",
    "s = time.time()\n",
    "quick_sort_sequential(unsorted)\n",
    "print(f\"Sequential Duration: {(time.time() - s):.3f}\")\n",
    "\n",
    "s = time.time()\n",
    "unsorted_obj = ray.put(unsorted)\n",
    "ray.get(quick_sort_distributed.remote(unsorted_obj))\n",
    "print(f\"Distributed Duration: {(time.time() - s):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "1. Introduced to Ray Core and Most Popular Workloads\n",
    "2. Sequential -> Distributed Training of 1000 Models\n",
    "3. Optional: Distributed Implementation of Quick Sort\n",
    "\n",
    "#### [Key Concepts](https://docs.ray.io/en/latest/ray-core/key-concepts.html)\n",
    "- Tasks\n",
    "- Actors\n",
    "- Objects\n",
    "\n",
    "#### [Key API Elements in This Section](https://docs.ray.io/en/latest/ray-core/package-ref.html#python-api)\n",
    "- `ray.init()`\n",
    "- `@ray.remote`\n",
    "- `.remote()`\n",
    "- `ray.get()`\n",
    "- `ray.put()`\n",
    "\n",
    "#### Next\n",
    "Now that we've covered the core engine, let's go up one layer of abstraction to look at a suite of data science libraries build on top of Ray Core to target specific machine learning workloads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Part Two: Ray AI Runtime (AIR)\n",
    "---\n",
    "![Ray AIR](intro-to-ray-images/Ray_AIR.png)\n",
    "*Figure 5*\n",
    "\n",
    "Ray AI Runtime (AIR) is a unified set of libraries for distributed data processing, model training, tuning, reinforcement learning, and model serving, all in Python. Before we lay out each library and their unique jobs to be done, let's take a moment to motivate Ray AIR by taking a high-level view of the data science and machine learning workflow.\n",
    "\n",
    "Developing a machine learning system is an iterative and often cyclical process that roughly touches on the following stages:\n",
    "1. Business Needs: work with stakeholders to identify business requirements and align on which metric to optimize\n",
    "2. Data Collection: source and collect data and labels\n",
    "3. Feature Engineering: turn raw data into usable material\n",
    "4. Model Training: the learning part of machine learning\n",
    "5. Model Evaluation: try to improve upon your baseline model with hyperparameter tuning or more feature engineering or even a more relevant set of data\n",
    "6. Deployment: deploy your solution to production and/or serve your model to the end user\n",
    "\n",
    "Each of the five main native libraries that Ray AIR wraps tackles a specific piece of the ML specific tasks outlined above and because this abstraction layer is built on top of Ray Core, it is distributed by nature.\n",
    "\n",
    "1. Ray Data: load and manipulate data\n",
    "2. Ray Train: scales model training for popular ML frameworks\n",
    "3. Ray Tune: scales experiment execution and hyperparameter tuning\n",
    "4. Ray Serve: scales model serving\n",
    "5. Ray RLlib: for distributed reinforment learning workloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Data\n",
    "Ray Datasets are the standard way to load and exchange data in Ray libraries and applications. They provide basic distributed data transformations such as map, filter, and repartition, and are compatible with a variety of file formats, data sources, and distributed frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataset of Python objects.\n",
    "ds = ray.data.range(10000)\n",
    "# -> Dataset(num_blocks=200, num_rows=10000, schema=<class 'int'>)\n",
    "\n",
    "ds.take(5)\n",
    "# -> [0, 1, 2, 3, 4]\n",
    "\n",
    "ds.count()\n",
    "# -> 10000\n",
    "\n",
    "# Create a Dataset of Arrow records.\n",
    "ds = ray.data.from_items([{\"col1\": i, \"col2\": str(i)} for i in range(10000)])\n",
    "# -> Dataset(num_blocks=200, num_rows=10000, schema={col1: int64, col2: string})\n",
    "\n",
    "ds.show(5)\n",
    "# -> {'col1': 0, 'col2': '0'}\n",
    "# -> {'col1': 1, 'col2': '1'}\n",
    "# -> {'col1': 2, 'col2': '2'}\n",
    "# -> {'col1': 3, 'col2': '3'}\n",
    "# -> {'col1': 4, 'col2': '4'}\n",
    "\n",
    "ds.schema()\n",
    "# -> col1: int64\n",
    "# -> col2: string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "#### Key Concepts\n",
    "#### Key API Elements in This Section\n",
    "#### Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Train\n",
    "Ray Train is a lightweight library for distributed deep learning that allows you to easily supercharge your distributed PyTorch and TensorFlow training on Ray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.train as train\n",
    "from ray.train import Trainer\n",
    "import torch\n",
    "\n",
    "def train_func():\n",
    "    # Setup model.\n",
    "    model = torch.nn.Linear(1, 1)\n",
    "    model = train.torch.prepare_model(model)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "    # Setup data.\n",
    "    input = torch.randn(1000, 1)\n",
    "    labels = input * 2\n",
    "    dataset = torch.utils.data.TensorDataset(input, labels)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=32)\n",
    "    dataloader = train.torch.prepare_data_loader(dataloader)\n",
    "\n",
    "    # Train.\n",
    "    for _ in range(5):\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return model.state_dict()\n",
    "\n",
    "trainer = Trainer(backend=\"torch\", num_workers=4)\n",
    "trainer.start()\n",
    "results = trainer.run(train_func)\n",
    "trainer.shutdown()\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "#### Key Concepts\n",
    "#### Key API Elements in This Section\n",
    "#### Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Tune\n",
    "Ray Tune is a Python library for fast hyperparameter tuning at scale. Easily distribute your trial runs to quickly find the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    " \n",
    "def objective(step, alpha, beta):\n",
    "   return (0.1 + alpha * step / 100)**(-1) + beta * 0.1\n",
    " \n",
    "def training_function(config):\n",
    "   # Hyperparameters\n",
    "   alpha, beta = config[\"alpha\"], config[\"beta\"]\n",
    "   for step in range(10):\n",
    "       # Iterative training function - can be any arbitrary training procedure.\n",
    "       intermediate_score = objective(step, alpha, beta)\n",
    "       # Feed the score back back to Tune.\n",
    "       tune.report(mean_loss=intermediate_score)\n",
    " \n",
    "analysis = tune.run(\n",
    "   training_function,\n",
    "   config={\n",
    "       \"alpha\": tune.grid_search([0.001, 0.01, 0.1]),\n",
    "       \"beta\": tune.choice([1, 2, 3])\n",
    "   })\n",
    " \n",
    "print(\"Best config: \", analysis.get_best_config(\n",
    "   metric=\"mean_loss\", mode=\"min\"))\n",
    " \n",
    "# Get a dataframe for analyzing trial results.\n",
    "df = analysis.results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "#### Key Concepts\n",
    "#### Key API Elements in This Section\n",
    "#### Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Serve\n",
    "Ray Serve lets you serve machine learning models in real-time or batch using a simple Python API. Serve individual models or create composite model pipelines, where you can independently deploy, update, and scale individual components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from ray import serve\n",
    "\n",
    "@serve.deployment(route_prefix=\"/iris\")\n",
    "class BoostingModel:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.label_list = iris_dataset[\"target_names\"].tolist()\n",
    "\n",
    "    async def __call__(self, request):\n",
    "        payload = await request.json()\n",
    "        print(f\"Received flask request with data {payload}\")\n",
    "\n",
    "        prediction = self.model.predict([payload[\"vector\"]])[0]\n",
    "        human_name = self.label_list[prediction]\n",
    "        return {\"result\": human_name}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Train model.\n",
    "    iris_dataset = load_iris()\n",
    "    model = GradientBoostingClassifier()\n",
    "    model.fit(iris_dataset[\"data\"], iris_dataset[\"target\"])\n",
    "\n",
    "    # Deploy model\n",
    "    serve.run(BoostingModel.bind(model))\n",
    "\n",
    "    # Query model\n",
    "    sample_request_input = {\"vector\": [1.2, 1.0, 1.1, 0.9]}\n",
    "    response = requests.get(\"http://localhost:8000/iris\", json=sample_request_input)\n",
    "    print(response.text)\n",
    "    \n",
    "    # prints\n",
    "    # Result:\n",
    "    # {\"result\": \"versicolor\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "#### Key Concepts\n",
    "#### Key API Elements in This Section\n",
    "#### Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray RLLib\n",
    "RLlib is the industry-standard reinforcement larning Python frameowrk built on Ray. Designed for quick interation and a fast path to production, it includes 25+ latest algorithms that are all implemented to run at scale and in multi-agent mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    " \n",
    "tune.run(PPOTrainer, config={\n",
    "   \"env\": \"CartPole-v0\",\n",
    "   \"framework\": \"torch\",\n",
    "   \"log_level\": \"INFO\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "#### Key Concepts\n",
    "#### Key API Elements in This Section\n",
    "#### Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Three: Ray Ecosystem\n",
    "---\n",
    "Up until now, we've covered Ray Core, the low-level, distributed computing framework, as well as Ray AIR, the set of high-level native libraries which include Ray Data, Train, Tune, Serve, and RLlib. In addition, Ray integrates with a growing ecosystem of the most popular Python and machine learning libraries and frameworks that you may already be familiar with.\n",
    "\n",
    "Instead of trying to create new standards, Ray allows you to scale existing workloads by unifying tools in a common interface. This interface enables you to run tasks in a distributed way, a property most of the respective backends don't have, or not to the same extent.\n",
    "\n",
    "For example, Ray Datasets is backed by Arrow and comes with many integrations to other frameworks, such as Spark and Dask. Ray Train and RLlib are backed by the full power of Tensorflow and PyTorch. Ray Tune supports algorithms from practically every noteable HPO tool available, including Hyperopt, Optuna, Nevergrad, Ax, SigOpt, and many others. Ray Serve can be used with frameworks such as FastAPI, gradio, and Streamlit. With ample opportunity to extend current projects as well as integrate more backends in the future, a key strength of Ray is this robust design pattern for integration which you can read more about [here](https://www.anyscale.com/blog/ray-distributed-library-patterns).\n",
    "\n",
    "For a complete list of integrations with links, go to the [Ray Ecosystem](https://docs.ray.io/en/latest/ray-overview/ray-libraries.html#) page in the docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: HuggingFace\n",
    "### Example: XGBoost\n",
    "### Example: Distributed Scikit-Learn / Joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "#### Key Concepts\n",
    "#### Key API Elements in This Section\n",
    "#### Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Four: Ray Clusters\n",
    "A Ray cluster consists of nodes that are connected to each other via a network. You can program against the so-called driver, the program root, which lives on the head node. The driver can run jobs, that is a collection of tasks, that are run on the nodes in the cluster. Specifically, the individual tasks of a job are run on worker processes on worker nodes.\n",
    "\n",
    "Ray Cluster\n",
    "\n",
    "A Ray cluster consists of a single head node and any number of connected worker nodes. The number of worker nodes may be autoscaled with application demand as specified by your Ray cluster configuration. The head node runs the autoscaler. Users can submit jobs for execution on the Ray cluster, or can interactively use the cluster by connecting to the head node and running `ray.init`.\n",
    "\n",
    "Head Node\n",
    "\n",
    "Every Ray cluster has one node which is designated as the head node of the cluster. The head node is identical to other worker nodes, except that it also runs singleton processes responsible for cluster management such as the autoscaler and the Ray driver processes which run Ray jobs. Ray may schedule tasks and actor on the head node just like any other worker node, unless configured otherwise.\n",
    "\n",
    "Worker Node\n",
    "\n",
    "Worker nodes do not run any head node management processes, and serve only to run user code in Ray tasks and actors. They participate in distributed scheduling, as well as the storage and distribution of Ray objects in cluster memory.\n",
    "\n",
    "Autoscaling\n",
    "\n",
    "The Ray autoscaler is a process that runs on the head node (or as a sidecar container in the head pod if using Kubernetes). When the resource demands of the Ray workload exceed the current capacity of the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "#### Key Concepts\n",
    "#### Key API Elements in This Section\n",
    "#### Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "---\n",
    "If you would like to practice your new skills further with some in-depth examples beyond the embedded coding excercises, take a look at this list of suggested problems:\n",
    "- [Distribute a Classical Algorithm with Ray](https://github.com/ray-project/hackathon5-algo)\n",
    "    - In this excercise, go to the GitHub repo linked above for details on choosing a classic algorithm implemented in Python, editing the implementation to parallelize the work with Ray, and compare your results against the sequential implementation.\n",
    "\n",
    "\n",
    "# Next Steps\n",
    "---\n",
    "üéâ Congratulations! You have completed your first tutorial on an Introduction to Ray! We discussed the three layers of Ray: Core, AIR, and the Ecosystem. Each library in Ray AIR (Data, Train, Tune, Serve, RLLib) and the ecosystem of integrated libraries runs on Ray Core's distributed execution engine, and with Ray Clusters, you can deploy your workloads on AWS, GCP, Azure, or on Kubernetes.\n",
    "\n",
    "From here, you can learn and get more involved with our active community of developers and researchers by checking out the following resources:\n",
    "- üìñ [Ray's \"Getting Started\" Guides](https://docs.ray.io/en/latest/ray-overview/index.html): A collection of QuickStart Guides for every library including installation walkthrough, examples, blogs, talks, and more!\n",
    "- üíª [Official Ray Website](https://www.ray.io/): Browse the ecosystem and use this site as a hub to get the information that you need to get going and building with Ray.\n",
    "- üí¨ [Join the Community on Slack](https://forms.gle/9TSdDYUgxYs8SA9e8): Find friends to discuss your new learnings in our Slack space.\n",
    "- üì£ [Use the Discussion Board](https://discuss.ray.io/): Ask questions, follow topics, and view announcements on this community forum.\n",
    "- üôã‚Äç‚ôÄÔ∏è [Join a Meetup Group](https://www.meetup.com/Bay-Area-Ray-Meetup/): Tune in on meet-ups to listen to compelling talks, get to know other users, and meet the team behind Ray.\n",
    "- ü™≤ [Open an Issue](https://github.com/ray-project/ray/issues/new/choose): Ray is constantly evolving to improve developer experience. Submit feature requests, bug-reports, and get help via GitHub issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "567405a8058597909526349386224fe35dd047505a91307e44ed44be00113429"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
